{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages \ud83e\udd17 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. BERTopic supports guided , (semi-) supervised , and dynamic topic modeling. It even supports visualizations similar to LDAvis! Corresponding medium posts can be found here and here . Installation \u00b6 Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] Quick Start \u00b6 We start by extracting topics from the well-known 20 newsgroups dataset containing English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages. Overview \u00b6 BERTopic has quite a number of functions that quickly can become overwhelming. To alleviate this issue, you will find an overview of all methods and a short description of its purpose. Common \u00b6 Below, you will find an overview of common functions in BERTopic. Method Code Fit the model .fit(docs) Fit the model and predict documents .fit_transform(docs) Predict new documents .transform([new_doc]) Access single topic .get_topic(topic=12) Access all topics .get_topics() Get topic freq .get_topic_freq() Get all topic information .get_topic_info() Get representative docs per topic .get_representative_docs() Update topic representation .update_topics(docs, n_gram_range=(1, 3)) Generate topic labels .generate_topic_labels() Set topic labels .set_topic_labels(my_custom_labels) Merge topics .merge_topics(docs, topics_to_merge) Reduce nr of topics .reduce_topics(docs, nr_topics=30) Find topics .find_topics(\"vehicle\") Save model .save(\"my_model\") Load model BERTopic.load(\"my_model\") Get parameters .get_params() Attributes \u00b6 After having trained your BERTopic model, a number of attributes are saved within your model. These attributes, in part, refer to how model information is stored on an estimator during fitting. The attributes that you see below all end in _ and are public attributes that can be used to access model information. Attribute Description topics_ The topics that are generated for each document after training or updating the topic model. probabilities_ The probabilities that are generated for each document if HDBSCAN is used. topic_sizes_ The size of each topic topic_mapper_ A class for tracking topics and their mappings anytime they are merged/reduced. topic_representations_ The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ The topic-term matrix as calculated through c-TF-IDF. topic_labels_ The default labels for each topic. custom_labels_ Custom labels for each topic as generated through .set_topic_labels . topic_embeddings_ The embeddings for each topic if embedding_model was used. representative_docs_ The representative documents for each topic if HDBSCAN is used. Variations \u00b6 There are many different use cases in which topic modeling can be used. As such, a number of variations of BERTopic have been developed such that one package can be used across across many use cases. Method Code (semi-) Supervised Topic Modeling .fit(docs, y=y) Topic Modeling per Class .topics_per_class(docs, classes) Dynamic Topic Modeling .topics_over_time(docs, timestamps) Hierarchical Topic Modeling .hierarchical_topics(docs) Guided Topic Modeling BERTopic(seed_topic_list=seed_topic_list) Visualizations \u00b6 Evaluating topic models can be rather difficult due to the somewhat subjective nature of evaluation. Visualizing different aspects of the topic model helps in understanding the model and makes it easier to tweak the model to your liking. Method Code Visualize Topics .visualize_topics() Visualize Documents .visualize_documents() Visualize Document Hierarchy .visualize_hierarchical_documents() Visualize Topic Hierarchy .visualize_hierarchy() Visualize Topic Tree .get_topic_tree(hierarchical_topics) Visualize Topic Terms .visualize_barchart() Visualize Topic Similarity .visualize_heatmap() Visualize Term Score Decline .visualize_term_rank() Visualize Topic Probability Distribution .visualize_distribution(probs[0]) Visualize Topics over Time .visualize_topics_over_time(topics_over_time) Visualize Topics per Class .visualize_topics_per_class(topics_per_class) Citation \u00b6 To cite the BERTopic paper , please use the following bibtex reference: @article{grootendorst2022bertopic, title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, author={Grootendorst, Maarten}, journal={arXiv preprint arXiv:2203.05794}, year={2022} }","title":"Home"},{"location":"index.html#bertopic","text":"BERTopic is a topic modeling technique that leverages \ud83e\udd17 transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. BERTopic supports guided , (semi-) supervised , and dynamic topic modeling. It even supports visualizations similar to LDAvis! Corresponding medium posts can be found here and here .","title":"BERTopic"},{"location":"index.html#installation","text":"Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ]","title":"Installation"},{"location":"index.html#quick-start","text":"We start by extracting topics from the well-known 20 newsgroups dataset containing English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics and their probabilities, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] NOTE : Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages.","title":"Quick Start"},{"location":"index.html#overview","text":"BERTopic has quite a number of functions that quickly can become overwhelming. To alleviate this issue, you will find an overview of all methods and a short description of its purpose.","title":"Overview"},{"location":"index.html#common","text":"Below, you will find an overview of common functions in BERTopic. Method Code Fit the model .fit(docs) Fit the model and predict documents .fit_transform(docs) Predict new documents .transform([new_doc]) Access single topic .get_topic(topic=12) Access all topics .get_topics() Get topic freq .get_topic_freq() Get all topic information .get_topic_info() Get representative docs per topic .get_representative_docs() Update topic representation .update_topics(docs, n_gram_range=(1, 3)) Generate topic labels .generate_topic_labels() Set topic labels .set_topic_labels(my_custom_labels) Merge topics .merge_topics(docs, topics_to_merge) Reduce nr of topics .reduce_topics(docs, nr_topics=30) Find topics .find_topics(\"vehicle\") Save model .save(\"my_model\") Load model BERTopic.load(\"my_model\") Get parameters .get_params()","title":"Common"},{"location":"index.html#attributes","text":"After having trained your BERTopic model, a number of attributes are saved within your model. These attributes, in part, refer to how model information is stored on an estimator during fitting. The attributes that you see below all end in _ and are public attributes that can be used to access model information. Attribute Description topics_ The topics that are generated for each document after training or updating the topic model. probabilities_ The probabilities that are generated for each document if HDBSCAN is used. topic_sizes_ The size of each topic topic_mapper_ A class for tracking topics and their mappings anytime they are merged/reduced. topic_representations_ The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ The topic-term matrix as calculated through c-TF-IDF. topic_labels_ The default labels for each topic. custom_labels_ Custom labels for each topic as generated through .set_topic_labels . topic_embeddings_ The embeddings for each topic if embedding_model was used. representative_docs_ The representative documents for each topic if HDBSCAN is used.","title":"Attributes"},{"location":"index.html#variations","text":"There are many different use cases in which topic modeling can be used. As such, a number of variations of BERTopic have been developed such that one package can be used across across many use cases. Method Code (semi-) Supervised Topic Modeling .fit(docs, y=y) Topic Modeling per Class .topics_per_class(docs, classes) Dynamic Topic Modeling .topics_over_time(docs, timestamps) Hierarchical Topic Modeling .hierarchical_topics(docs) Guided Topic Modeling BERTopic(seed_topic_list=seed_topic_list)","title":"Variations"},{"location":"index.html#visualizations","text":"Evaluating topic models can be rather difficult due to the somewhat subjective nature of evaluation. Visualizing different aspects of the topic model helps in understanding the model and makes it easier to tweak the model to your liking. Method Code Visualize Topics .visualize_topics() Visualize Documents .visualize_documents() Visualize Document Hierarchy .visualize_hierarchical_documents() Visualize Topic Hierarchy .visualize_hierarchy() Visualize Topic Tree .get_topic_tree(hierarchical_topics) Visualize Topic Terms .visualize_barchart() Visualize Topic Similarity .visualize_heatmap() Visualize Term Score Decline .visualize_term_rank() Visualize Topic Probability Distribution .visualize_distribution(probs[0]) Visualize Topics over Time .visualize_topics_over_time(topics_over_time) Visualize Topics per Class .visualize_topics_per_class(topics_per_class)","title":"Visualizations"},{"location":"index.html#citation","text":"To cite the BERTopic paper , please use the following bibtex reference: @article{grootendorst2022bertopic, title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure}, author={Grootendorst, Maarten}, journal={arXiv preprint arXiv:2203.05794}, year={2022} }","title":"Citation"},{"location":"changelog.html","text":"Changelog \u00b6 Version 0.12.0 \u00b6 Release date: 5 September, 2022 Highlights : Perform online/incremental topic modeling with .partial_fit Expose c-TF-IDF model for customization with bertopic.vectorizers.ClassTfidfTransformer The parameters bm25_weighting and reduce_frequent_words were added to potentially improve representations: Expose attributes for easier access to internal data Major changes to the Algorithm page of the documentation, which now contains three overviews of the algorithm: Visualize Overview Code Overview Detailed Overview Added an example of combining BERTopic with KeyBERT Added many tests with the intention of making development a bit more stable Fixes : Fixed iteratively merging topics ( #632 and ( #648 ) Fixed 0th topic not showing up in visualizations ( #667 ) Fixed lowercasing not being optional ( #682 ) Fixed spelling ( #664 and ( #673 ) Fixed 0th topic not shown in .get_topic_info by @oxymor0n in #660 Fixed spelling by @domenicrosati in #674 Add custom labels and title options to barchart @leloykun in #694 Online/incremental topic modeling : Online topic modeling (sometimes called \"incremental topic modeling\") is the ability to learn incrementally from a mini-batch of instances. Essentially, it is a way to update your topic model with data on which it was not trained on before. In Scikit-Learn, this technique is often modeled through a .partial_fit function, which is also used in BERTopic. At a minimum, the cluster model needs to support a .partial_fit function in order to use this feature. The default HDBSCAN model will not work as it does not support online updating. from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents all_docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] doc_chunks = [ all_docs [ i : i + 1000 ] for i in range ( 0 , len ( all_docs ), 1000 )] # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs ) Only the topics for the most recent batch of documents are tracked. If you want to be using online topic modeling, not for a streaming setting but merely for low-memory use cases, then it is advised to also update the .topics_ attribute as variations such as hierarchical topic modeling will not work afterward: # Incrementally fit the topic model by training on 1000 documents at a time and track the topics in each iteration topics = [] for docs in doc_chunks : topic_model . partial_fit ( docs ) topics . extend ( topic_model . topics_ ) topic_model . topics_ = topics c-TF-IDF : Explicitly define, use, and adjust the ClassTfidfTransformer with new parameters, bm25_weighting and reduce_frequent_words , to potentially improve the topic representation: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( bm25_weighting = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model ) Attributes : After having fitted your BERTopic instance, you can use the following attributes to have quick access to certain information, such as the topic assignment for each document in topic_model.topics_ . Attribute Type Description topics_ List[int] The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ List[float] The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When calculate_probabilities=True , then it is the probabilities of all topics per document. topic_sizes_ Mapping[int, int] The size of each topic. topic_mapper_ TopicMapper A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ Mapping[int, Tuple[int, float]] The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ csr_matrix The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run .vectorizer_model.get_feature_names() or .vectorizer_model.get_feature_names_out() topic_labels_ Mapping[int, str] The default labels for each topic. custom_labels_ List[str] Custom labels for each topic as generated through .set_topic_labels . topic_embeddings_ np.ndarray The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ Mapping[int, str] The representative documents for each topic if HDBSCAN is used. Version 0.11.0 \u00b6 Release date: 11 July, 2022 Highlights : Perform hierarchical topic modeling with .hierarchical_topics hierarchical_topics = topic_model . hierarchical_topics ( docs , topics ) Visualize hierarchical topic representations with .visualize_hierarchy topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) Extract a text-based hierarchical topic representation with .get_topic_tree tree = topic_model . get_topic_tree ( hierarchical_topics ) Visualize 2D documents with .visualize_documents() # Use input embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # or use 2D reduced embeddings through a method of your own (e.g., PCA, t-SNE, UMAP, etc.) reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Visualize 2D hierarchical documents with .visualize_hierarchical_documents() # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already which speed things up quite a bit: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Create custom labels to the topics throughout most visualizations # Generate topic labels topic_labels = topic_model . generate_topic_labels ( nr_words = 3 , topic_prefix = False , word_length = 10 , separator = \", \" ) # Set them internally in BERTopic topic_model . set_topic_labels ( topics_labels ) Manually merge topics with .merge_topics() # Merge topics 1, 2, and 3 topics_to_merge = [ 1 , 2 , 3 ] topic_model . merge_topics ( docs , topics , topics_to_merge ) # Merge topics 1 and 2, and separately merge topics 3 and 4 topics_to_merge = [[ 1 , 2 ], [ 3 , 4 ]] topic_model . merge_topics ( docs , topics , topics_to_merge ) Added example for finding similar topics between two models in the tips & tricks page Add multi-modal example in the tips & tricks page Added native Hugging Face transformers support Fixes : Fix support for k-Means in .visualize_heatmap ( #532 ) Fix missing topic 0 in .visualize_topics ( #533 ) Fix inconsistencies in .get_topic_info ( #572 ) and ( #581 ) Add optimal_ordering parameter to .visualize_hierarchy by @rafaelvalero in #390 Fix RuntimeError when used as sklearn estimator by @simonfelding in #448 Fix typo in visualization documentation by @dwhdai in #475 Fix typo in docstrings by @xwwwwww in #549 Support higher Flair versions Version 0.10.0 \u00b6 Release date: 30 April, 2022 Highlights : Use any dimensionality reduction technique instead of UMAP: from bertopic import BERTopic from sklearn.decomposition import PCA dim_model = PCA ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model ) Use any clustering technique instead of HDBSCAN: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model ) Documentation : Add a CountVectorizer page with tips and tricks on how to create topic representations that fit your use case Added pages on how to use other dimensionality reduction and clustering algorithms Additional instructions on how to reduce outliers in the FAQ: import numpy as np probability_threshold = 0.01 new_topics = [ np . argmax ( prob ) if max ( prob ) >= probability_threshold else - 1 for prob in probs ] Fixes : Fixed None being returned for probabilities when transforming unseen documents Replaced all instances of arg: with Arguments: for consistency Before saving a fitted BERTopic instance, we remove the stopwords in the fitted CountVectorizer model as it can get quite large due to the number of words that end in stopwords if min_df is set to a value larger than 1 Set \"hdbscan>=0.8.28\" to prevent numpy issues Although this was already fixed by the new release of HDBSCAN, it is technically still possible to install 0.8.27 with BERTopic which leads to these numpy issues Update gensim dependency to >=4.0.0 ( #371 ) Fix topic 0 not appearing in visualizations ( #472 ) Fix ( #506 ) Fix ( #429 ) Fix typo in DTM documentation by @hp0404 in #386 Version 0.9.4 \u00b6 Release date: 14 December, 2021 A number of fixes, documentation updates, and small features: Expose diversity parameter Use BERTopic(diversity=0.1) to change how diverse the words in a topic representation are (ranges from 0 to 1) Improve stability of topic reduction by only computing the cosine similarity within c-TF-IDF and not the topic embeddings Added property to c-TF-IDF that all IDF values should be positive ( #351 ) Improve stability of .visualize_barchart() and .visualize_hierarchy() Major documentation overhaul (mkdocs, tutorials, FAQ, images, etc. ) ( #330 ) Drop python 3.6 ( #333 ) Relax plotly dependency ( #88 ) Additional logging for .transform ( #356 ) Version 0.9.3 \u00b6 Release date: 17 October, 2021 Fix #282 As it turns out the old implementation of topic mapping was still found in the transform function Fix #285 Fix getting all representative docs Fix #288 A recent issue with the package pyyaml that can be found in Google Colab Version 0.9.2 \u00b6 Release date: 12 October, 2021 A release focused on algorithmic optimization and fixing several issues: Highlights : Update the non-multilingual paraphrase- models to the all- models due to improved performance Reduce necessary RAM in c-TF-IDF top 30 word extraction Fixes : Fix topic mapping When reducing the number of topics, these need to be mapped to the correct input/output which had some issues in the previous version A new class was created as a way to track these mappings regardless of how many times they were executed In other words, you can iteratively reduce the number of topics after training the model without the need to continuously train the model Fix typo in embeddings page ( #200 ) Fix link in README ( #233 ) Fix documentation .visualize_term_rank() ( #253 ) Fix getting correct representative docs ( #258 ) Update memory FAQ with HDBSCAN pr Version 0.9.1 \u00b6 Release date: 1 September, 2021 A release focused on fixing several issues: Fixes : Fix TypeError when auto-reducing topics ( #210 ) Fix mapping representative docs when reducing topics ( #208 ) Fix visualization issues with probabilities ( #205 ) Fix missing normalize_frequency param in plots ( #213 ) Version 0.9.0 \u00b6 Release date: 9 August, 2021 Highlights : Implemented a Guided BERTopic -> Use seeds to steer the Topic Modeling Get the most representative documents per topic: topic_model.get_representative_docs(topic=1) This allows users to see which documents are good representations of a topic and better understand the topics that were created Added normalize_frequency parameter to visualize_topics_per_class and visualize_topics_over_time in order to better compare the relative topic frequencies between topics Return flat probabilities as default, only calculate the probabilities of all topics per document if calculate_probabilities is True Added several FAQs Fixes : Fix loading pre-trained BERTopic model Fix mapping of probabilities Fix #190 Guided BERTopic : Guided BERTopic works in two ways: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. seed_topic_list = [[ \"company\" , \"billion\" , \"quarter\" , \"shrs\" , \"earnings\" ], [ \"acquisition\" , \"procurement\" , \"merge\" ], [ \"exchange\" , \"currency\" , \"trading\" , \"rate\" , \"euro\" ], [ \"grain\" , \"wheat\" , \"corn\" ], [ \"coffee\" , \"cocoa\" ], [ \"natural\" , \"gas\" , \"oil\" , \"fuel\" , \"products\" , \"petrol\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) Version 0.8.1 \u00b6 Release date: 8 June, 2021 Highlights : Improved models: For English documents the default is now: \"paraphrase-MiniLM-L6-v2\" For Non-English or multi-lingual documents the default is now: \"paraphrase-multilingual-MiniLM-L12-v2\" Both models show not only great performance but are much faster! Add interactive visualizations to the plotting API documentation For better performance, please use the following models: English: \"paraphrase-mpnet-base-v2\" Non-English or multi-lingual: \"paraphrase-multilingual-mpnet-base-v2\" Fixes : Improved unit testing for more stability Set transformers version for Flair Version 0.8.0 \u00b6 Release date: 31 May, 2021 Highlights : Additional visualizations: Topic Hierarchy: topic_model.visualize_hierarchy() Topic Similarity Heatmap: topic_model.visualize_heatmap() Topic Representation Barchart: topic_model.visualize_barchart() Term Score Decline: topic_model.visualize_term_rank() Created bertopic.plotting library to easily extend visualizations Improved automatic topic reduction by using HDBSCAN to detect similar topics Sort topic ids by their frequency. -1 is the outlier class and contains typically the most documents. After that 0 is the largest topic, 1 the second largest, etc. Fixes : Fix typo #113 , #117 Fix #121 by removing these two lines Fix mapping of topics after reduction (it now excludes 0) ( #103 ) Version 0.7.0 \u00b6 Release date: 26 April, 2021 The two main features are (semi-)supervised topic modeling and several backends to use instead of Flair and SentenceTransformers! Highlights : (semi-)supervised topic modeling by leveraging supervised options in UMAP model.fit(docs, y=target_classes) Backends: Added Spacy, Gensim, USE (TFHub) Use a different backend for document embeddings and word embeddings Create your own backends with bertopic.backend.BaseEmbedder Click here for an overview of all new backends Calculate and visualize topics per class Calculate: topics_per_class = topic_model.topics_per_class(docs, topics, classes) Visualize: topic_model.visualize_topics_per_class(topics_per_class) Several tutorials were updated and added: Name Link Topic Modeling with BERTopic (Custom) Embedding Models in BERTopic Advanced Customization in BERTopic (semi-)Supervised Topic Modeling with BERTopic Dynamic Topic Modeling with Trump's Tweets Fixes : Fixed issues with Torch req Prevent saving term frequency matrix in CTFIDF class Fixed DTM not working when reducing topics ( #96 ) Moved visualization dependencies to base BERTopic pip install bertopic[visualization] becomes pip install bertopic Allow precomputed embeddings in bertopic.find_topics() ( #79 ): model = BERTopic ( embedding_model = my_embedding_model ) model . fit ( docs , my_precomputed_embeddings ) model . find_topics ( search_term ) Version 0.6.0 \u00b6 Release date: 1 March, 2021 Highlights : DTM: Added a basic dynamic topic modeling technique based on the global c-TF-IDF representation model.topics_over_time(docs, timestamps, global_tuning=True) DTM: Option to evolve topics based on t-1 c-TF-IDF representation which results in evolving topics over time Only uses topics at t-1 and skips evolution if there is a gap model.topics_over_time(docs, timestamps, evolution_tuning=True) DTM: Function to visualize topics over time model.visualize_topics_over_time(topics_over_time) DTM: Add binning of timestamps model.topics_over_time(docs, timestamps, nr_bins=10) Add function get general information about topics (id, frequency, name, etc.) get_topic_info() Improved stability of c-TF-IDF by taking the average number of words across all topics instead of the number of documents Fixes : _map_probabilities() does not take into account that there is no probability of the outlier class and the probabilities are mutated instead of copied (#63, #64) Version 0.5.0 \u00b6 Release date: 8 Februari, 2021 Highlights : Add Flair to allow for more (custom) token/document embeddings, including \ud83e\udd17 transformers Option to use custom UMAP, HDBSCAN, and CountVectorizer Added low_memory parameter to reduce memory during computation Improved verbosity (shows progress bar) Return the figure of visualize_topics() Expose all parameters with a single function: get_params() Fixes : To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used. Set calculate_probabilities to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on. Use the newest version of sentence-transformers as it speeds ups encoding significantly Version 0.4.2 \u00b6 Release date: 10 Januari, 2021 Fixes : Selecting embedding_model did not work when language was also used. This led to the user needing to set language to None before being able to use embedding_model . Fixed by using embedding_model when language is used (as a default parameter). Version 0.4.1 \u00b6 Release date: 07 Januari, 2021 Fixes : Simple fix by lowering the languages variable to match the lowered input language. Version 0.4.0 \u00b6 Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities Version 0.3.2 \u00b6 Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary. Version 0.3.1 \u00b6 Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking. Version 0.3.0 \u00b6 Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document Version 0.2.2 \u00b6 Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords Version 0.2.1 \u00b6 Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets. Version 0.2.0 \u00b6 Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests Version 0.1.2 \u00b6 Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init. Version 0.1.1 \u00b6 Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation Version 0.1.0 \u00b6 Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Changelog"},{"location":"changelog.html#changelog","text":"","title":"Changelog"},{"location":"changelog.html#version-0120","text":"Release date: 5 September, 2022 Highlights : Perform online/incremental topic modeling with .partial_fit Expose c-TF-IDF model for customization with bertopic.vectorizers.ClassTfidfTransformer The parameters bm25_weighting and reduce_frequent_words were added to potentially improve representations: Expose attributes for easier access to internal data Major changes to the Algorithm page of the documentation, which now contains three overviews of the algorithm: Visualize Overview Code Overview Detailed Overview Added an example of combining BERTopic with KeyBERT Added many tests with the intention of making development a bit more stable Fixes : Fixed iteratively merging topics ( #632 and ( #648 ) Fixed 0th topic not showing up in visualizations ( #667 ) Fixed lowercasing not being optional ( #682 ) Fixed spelling ( #664 and ( #673 ) Fixed 0th topic not shown in .get_topic_info by @oxymor0n in #660 Fixed spelling by @domenicrosati in #674 Add custom labels and title options to barchart @leloykun in #694 Online/incremental topic modeling : Online topic modeling (sometimes called \"incremental topic modeling\") is the ability to learn incrementally from a mini-batch of instances. Essentially, it is a way to update your topic model with data on which it was not trained on before. In Scikit-Learn, this technique is often modeled through a .partial_fit function, which is also used in BERTopic. At a minimum, the cluster model needs to support a .partial_fit function in order to use this feature. The default HDBSCAN model will not work as it does not support online updating. from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents all_docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] doc_chunks = [ all_docs [ i : i + 1000 ] for i in range ( 0 , len ( all_docs ), 1000 )] # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs ) Only the topics for the most recent batch of documents are tracked. If you want to be using online topic modeling, not for a streaming setting but merely for low-memory use cases, then it is advised to also update the .topics_ attribute as variations such as hierarchical topic modeling will not work afterward: # Incrementally fit the topic model by training on 1000 documents at a time and track the topics in each iteration topics = [] for docs in doc_chunks : topic_model . partial_fit ( docs ) topics . extend ( topic_model . topics_ ) topic_model . topics_ = topics c-TF-IDF : Explicitly define, use, and adjust the ClassTfidfTransformer with new parameters, bm25_weighting and reduce_frequent_words , to potentially improve the topic representation: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( bm25_weighting = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model ) Attributes : After having fitted your BERTopic instance, you can use the following attributes to have quick access to certain information, such as the topic assignment for each document in topic_model.topics_ . Attribute Type Description topics_ List[int] The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ List[float] The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When calculate_probabilities=True , then it is the probabilities of all topics per document. topic_sizes_ Mapping[int, int] The size of each topic. topic_mapper_ TopicMapper A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ Mapping[int, Tuple[int, float]] The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ csr_matrix The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run .vectorizer_model.get_feature_names() or .vectorizer_model.get_feature_names_out() topic_labels_ Mapping[int, str] The default labels for each topic. custom_labels_ List[str] Custom labels for each topic as generated through .set_topic_labels . topic_embeddings_ np.ndarray The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ Mapping[int, str] The representative documents for each topic if HDBSCAN is used.","title":"Version 0.12.0"},{"location":"changelog.html#version-0110","text":"Release date: 11 July, 2022 Highlights : Perform hierarchical topic modeling with .hierarchical_topics hierarchical_topics = topic_model . hierarchical_topics ( docs , topics ) Visualize hierarchical topic representations with .visualize_hierarchy topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) Extract a text-based hierarchical topic representation with .get_topic_tree tree = topic_model . get_topic_tree ( hierarchical_topics ) Visualize 2D documents with .visualize_documents() # Use input embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # or use 2D reduced embeddings through a method of your own (e.g., PCA, t-SNE, UMAP, etc.) reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Visualize 2D hierarchical documents with .visualize_hierarchical_documents() # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already which speed things up quite a bit: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Create custom labels to the topics throughout most visualizations # Generate topic labels topic_labels = topic_model . generate_topic_labels ( nr_words = 3 , topic_prefix = False , word_length = 10 , separator = \", \" ) # Set them internally in BERTopic topic_model . set_topic_labels ( topics_labels ) Manually merge topics with .merge_topics() # Merge topics 1, 2, and 3 topics_to_merge = [ 1 , 2 , 3 ] topic_model . merge_topics ( docs , topics , topics_to_merge ) # Merge topics 1 and 2, and separately merge topics 3 and 4 topics_to_merge = [[ 1 , 2 ], [ 3 , 4 ]] topic_model . merge_topics ( docs , topics , topics_to_merge ) Added example for finding similar topics between two models in the tips & tricks page Add multi-modal example in the tips & tricks page Added native Hugging Face transformers support Fixes : Fix support for k-Means in .visualize_heatmap ( #532 ) Fix missing topic 0 in .visualize_topics ( #533 ) Fix inconsistencies in .get_topic_info ( #572 ) and ( #581 ) Add optimal_ordering parameter to .visualize_hierarchy by @rafaelvalero in #390 Fix RuntimeError when used as sklearn estimator by @simonfelding in #448 Fix typo in visualization documentation by @dwhdai in #475 Fix typo in docstrings by @xwwwwww in #549 Support higher Flair versions","title":"Version 0.11.0"},{"location":"changelog.html#version-0100","text":"Release date: 30 April, 2022 Highlights : Use any dimensionality reduction technique instead of UMAP: from bertopic import BERTopic from sklearn.decomposition import PCA dim_model = PCA ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model ) Use any clustering technique instead of HDBSCAN: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model ) Documentation : Add a CountVectorizer page with tips and tricks on how to create topic representations that fit your use case Added pages on how to use other dimensionality reduction and clustering algorithms Additional instructions on how to reduce outliers in the FAQ: import numpy as np probability_threshold = 0.01 new_topics = [ np . argmax ( prob ) if max ( prob ) >= probability_threshold else - 1 for prob in probs ] Fixes : Fixed None being returned for probabilities when transforming unseen documents Replaced all instances of arg: with Arguments: for consistency Before saving a fitted BERTopic instance, we remove the stopwords in the fitted CountVectorizer model as it can get quite large due to the number of words that end in stopwords if min_df is set to a value larger than 1 Set \"hdbscan>=0.8.28\" to prevent numpy issues Although this was already fixed by the new release of HDBSCAN, it is technically still possible to install 0.8.27 with BERTopic which leads to these numpy issues Update gensim dependency to >=4.0.0 ( #371 ) Fix topic 0 not appearing in visualizations ( #472 ) Fix ( #506 ) Fix ( #429 ) Fix typo in DTM documentation by @hp0404 in #386","title":"Version 0.10.0"},{"location":"changelog.html#version-094","text":"Release date: 14 December, 2021 A number of fixes, documentation updates, and small features: Expose diversity parameter Use BERTopic(diversity=0.1) to change how diverse the words in a topic representation are (ranges from 0 to 1) Improve stability of topic reduction by only computing the cosine similarity within c-TF-IDF and not the topic embeddings Added property to c-TF-IDF that all IDF values should be positive ( #351 ) Improve stability of .visualize_barchart() and .visualize_hierarchy() Major documentation overhaul (mkdocs, tutorials, FAQ, images, etc. ) ( #330 ) Drop python 3.6 ( #333 ) Relax plotly dependency ( #88 ) Additional logging for .transform ( #356 )","title":"Version 0.9.4"},{"location":"changelog.html#version-093","text":"Release date: 17 October, 2021 Fix #282 As it turns out the old implementation of topic mapping was still found in the transform function Fix #285 Fix getting all representative docs Fix #288 A recent issue with the package pyyaml that can be found in Google Colab","title":"Version 0.9.3"},{"location":"changelog.html#version-092","text":"Release date: 12 October, 2021 A release focused on algorithmic optimization and fixing several issues: Highlights : Update the non-multilingual paraphrase- models to the all- models due to improved performance Reduce necessary RAM in c-TF-IDF top 30 word extraction Fixes : Fix topic mapping When reducing the number of topics, these need to be mapped to the correct input/output which had some issues in the previous version A new class was created as a way to track these mappings regardless of how many times they were executed In other words, you can iteratively reduce the number of topics after training the model without the need to continuously train the model Fix typo in embeddings page ( #200 ) Fix link in README ( #233 ) Fix documentation .visualize_term_rank() ( #253 ) Fix getting correct representative docs ( #258 ) Update memory FAQ with HDBSCAN pr","title":"Version 0.9.2"},{"location":"changelog.html#version-091","text":"Release date: 1 September, 2021 A release focused on fixing several issues: Fixes : Fix TypeError when auto-reducing topics ( #210 ) Fix mapping representative docs when reducing topics ( #208 ) Fix visualization issues with probabilities ( #205 ) Fix missing normalize_frequency param in plots ( #213 )","title":"Version 0.9.1"},{"location":"changelog.html#version-090","text":"Release date: 9 August, 2021 Highlights : Implemented a Guided BERTopic -> Use seeds to steer the Topic Modeling Get the most representative documents per topic: topic_model.get_representative_docs(topic=1) This allows users to see which documents are good representations of a topic and better understand the topics that were created Added normalize_frequency parameter to visualize_topics_per_class and visualize_topics_over_time in order to better compare the relative topic frequencies between topics Return flat probabilities as default, only calculate the probabilities of all topics per document if calculate_probabilities is True Added several FAQs Fixes : Fix loading pre-trained BERTopic model Fix mapping of probabilities Fix #190 Guided BERTopic : Guided BERTopic works in two ways: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. seed_topic_list = [[ \"company\" , \"billion\" , \"quarter\" , \"shrs\" , \"earnings\" ], [ \"acquisition\" , \"procurement\" , \"merge\" ], [ \"exchange\" , \"currency\" , \"trading\" , \"rate\" , \"euro\" ], [ \"grain\" , \"wheat\" , \"corn\" ], [ \"coffee\" , \"cocoa\" ], [ \"natural\" , \"gas\" , \"oil\" , \"fuel\" , \"products\" , \"petrol\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs )","title":"Version 0.9.0"},{"location":"changelog.html#version-081","text":"Release date: 8 June, 2021 Highlights : Improved models: For English documents the default is now: \"paraphrase-MiniLM-L6-v2\" For Non-English or multi-lingual documents the default is now: \"paraphrase-multilingual-MiniLM-L12-v2\" Both models show not only great performance but are much faster! Add interactive visualizations to the plotting API documentation For better performance, please use the following models: English: \"paraphrase-mpnet-base-v2\" Non-English or multi-lingual: \"paraphrase-multilingual-mpnet-base-v2\" Fixes : Improved unit testing for more stability Set transformers version for Flair","title":"Version 0.8.1"},{"location":"changelog.html#version-080","text":"Release date: 31 May, 2021 Highlights : Additional visualizations: Topic Hierarchy: topic_model.visualize_hierarchy() Topic Similarity Heatmap: topic_model.visualize_heatmap() Topic Representation Barchart: topic_model.visualize_barchart() Term Score Decline: topic_model.visualize_term_rank() Created bertopic.plotting library to easily extend visualizations Improved automatic topic reduction by using HDBSCAN to detect similar topics Sort topic ids by their frequency. -1 is the outlier class and contains typically the most documents. After that 0 is the largest topic, 1 the second largest, etc. Fixes : Fix typo #113 , #117 Fix #121 by removing these two lines Fix mapping of topics after reduction (it now excludes 0) ( #103 )","title":"Version 0.8.0"},{"location":"changelog.html#version-070","text":"Release date: 26 April, 2021 The two main features are (semi-)supervised topic modeling and several backends to use instead of Flair and SentenceTransformers! Highlights : (semi-)supervised topic modeling by leveraging supervised options in UMAP model.fit(docs, y=target_classes) Backends: Added Spacy, Gensim, USE (TFHub) Use a different backend for document embeddings and word embeddings Create your own backends with bertopic.backend.BaseEmbedder Click here for an overview of all new backends Calculate and visualize topics per class Calculate: topics_per_class = topic_model.topics_per_class(docs, topics, classes) Visualize: topic_model.visualize_topics_per_class(topics_per_class) Several tutorials were updated and added: Name Link Topic Modeling with BERTopic (Custom) Embedding Models in BERTopic Advanced Customization in BERTopic (semi-)Supervised Topic Modeling with BERTopic Dynamic Topic Modeling with Trump's Tweets Fixes : Fixed issues with Torch req Prevent saving term frequency matrix in CTFIDF class Fixed DTM not working when reducing topics ( #96 ) Moved visualization dependencies to base BERTopic pip install bertopic[visualization] becomes pip install bertopic Allow precomputed embeddings in bertopic.find_topics() ( #79 ): model = BERTopic ( embedding_model = my_embedding_model ) model . fit ( docs , my_precomputed_embeddings ) model . find_topics ( search_term )","title":"Version 0.7.0"},{"location":"changelog.html#version-060","text":"Release date: 1 March, 2021 Highlights : DTM: Added a basic dynamic topic modeling technique based on the global c-TF-IDF representation model.topics_over_time(docs, timestamps, global_tuning=True) DTM: Option to evolve topics based on t-1 c-TF-IDF representation which results in evolving topics over time Only uses topics at t-1 and skips evolution if there is a gap model.topics_over_time(docs, timestamps, evolution_tuning=True) DTM: Function to visualize topics over time model.visualize_topics_over_time(topics_over_time) DTM: Add binning of timestamps model.topics_over_time(docs, timestamps, nr_bins=10) Add function get general information about topics (id, frequency, name, etc.) get_topic_info() Improved stability of c-TF-IDF by taking the average number of words across all topics instead of the number of documents Fixes : _map_probabilities() does not take into account that there is no probability of the outlier class and the probabilities are mutated instead of copied (#63, #64)","title":"Version 0.6.0"},{"location":"changelog.html#version-050","text":"Release date: 8 Februari, 2021 Highlights : Add Flair to allow for more (custom) token/document embeddings, including \ud83e\udd17 transformers Option to use custom UMAP, HDBSCAN, and CountVectorizer Added low_memory parameter to reduce memory during computation Improved verbosity (shows progress bar) Return the figure of visualize_topics() Expose all parameters with a single function: get_params() Fixes : To simplify the API, the parameters stop_words and n_neighbors were removed. These can still be used when a custom UMAP or CountVectorizer is used. Set calculate_probabilities to False as a default. Calculating probabilities with HDBSCAN significantly increases computation time and memory usage. Better to remove calculating probabilities or only allow it by manually turning this on. Use the newest version of sentence-transformers as it speeds ups encoding significantly","title":"Version 0.5.0"},{"location":"changelog.html#version-042","text":"Release date: 10 Januari, 2021 Fixes : Selecting embedding_model did not work when language was also used. This led to the user needing to set language to None before being able to use embedding_model . Fixed by using embedding_model when language is used (as a default parameter).","title":"Version 0.4.2"},{"location":"changelog.html#version-041","text":"Release date: 07 Januari, 2021 Fixes : Simple fix by lowering the languages variable to match the lowered input language.","title":"Version 0.4.1"},{"location":"changelog.html#version-040","text":"Release date: 21 December, 2020 Highlights : Visualize Topics similar to LDAvis Added option to reduce topics after training Added option to update topic representation after training Added option to search topics using a search term Significantly improved the stability of generating clusters Finetune the topic words by selecting the most coherent words with the highest c-TF-IDF values More extensive tutorials in the documentation Notable Changes : Option to select language instead of sentence-transformers models to minimize the complexity of using BERTopic Improved logging (remove duplicates) Check if BERTopic is fitted Added TF-IDF as an embedder instead of transformer models (see tutorial) Numpy for Python 3.6 will be dropped and was therefore removed from the workflow. Preprocess text before passing it through c-TF-IDF Merged get_topics_freq() with get_topic_freq() Fixes : Fix error handling topic probabilities","title":"Version 0.4.0"},{"location":"changelog.html#version-032","text":"Release date: 16 November, 2020 Highlights : Fixed a bug with the topic reduction method that seems to reduce the number of topics but not to the nr_topics as defined in the class. Since this was, to a certain extend, breaking the topic reduction method a new release was necessary.","title":"Version 0.3.2"},{"location":"changelog.html#version-031","text":"Release date: 4 November, 2020 Highlights : Adding the option to use custom embeddings or embeddings that you generated beforehand with whatever package you'd like to use. This allows users to further customize BERTopic to their liking.","title":"Version 0.3.1"},{"location":"changelog.html#version-030","text":"Release date: 29 October, 2020 Highlights : transform() and fit_transform() now also return the topic probability distributions Added visualize_distribution() which visualizes the topic probability distribution for a single document","title":"Version 0.3.0"},{"location":"changelog.html#version-022","text":"Release date: 17 October, 2020 Highlights : Fixed n_gram_range not being used Added option for using stopwords","title":"Version 0.2.2"},{"location":"changelog.html#version-021","text":"Release date: 11 October, 2020 Highlights : Improved the calculation of the class-based TF-IDF procedure by limiting the calculation to sparse matrices. This prevents out-of-memory problems when faced with large datasets.","title":"Version 0.2.1"},{"location":"changelog.html#version-020","text":"Release date: 11 October, 2020 Highlights : Changed c-TF-IDF procedure such that it implements a version of scikit-learns procedure. This should also speed up the calculation of the sparse matrix and prevent memory errors. Added automated unit tests","title":"Version 0.2.0"},{"location":"changelog.html#version-012","text":"Release date: 1 October, 2020 Highlights : When transforming new documents, self.mapped_topics seemed to be missing. Added to the init.","title":"Version 0.1.2"},{"location":"changelog.html#version-011","text":"Release date: 24 September, 2020 Highlights : Fixed requirements --> Issue with pytorch Update documentation","title":"Version 0.1.1"},{"location":"changelog.html#version-010","text":"Release date: 24 September, 2020 Highlights : First release of BERTopic Added parameters for UMAP and HDBSCAN Option to choose sentence-transformer model Method for transforming unseen documents Save and load trained models (UMAP and HDBSCAN) Extract topics and their sizes Notable Changes : Optimized c-TF-IDF Improved documentation Improved topic reduction","title":"Version 0.1.0"},{"location":"faq.html","text":"Frequently Asked Questions \u00b6 Why are the results not consistent between runs? \u00b6 Due to the stochastic nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using custom embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters. If you want to reproduce the results, at the expense of performance , you can set a random_state in UMAP to prevent any stochastic behavior: from bertopic import BERTopic from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , random_state = 42 ) topic_model = BERTopic ( umap_model = umap_model ) Which embedding model should I choose? \u00b6 Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in BERTopic ( \"all-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using all-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead. SentenceTransformers SentenceTransformers work typically quite well and are the preferred models to use. They are great at generating document embeddings and have several multi-lingual versions available. \ud83e\udd17 transformers BERTopic allows you to use any \ud83e\udd17 transformers model. These models are typically embeddings created on a word/sentence level but can easily be pooled using Flair (see Guides/Embeddings). If you have a specific language for which you want to generate embeddings, you can choose the model here . How do I reduce topic outliers? \u00b6 There are three ways in reducing outliers. First, the amount of datapoint classified as outliers is handled by the min_samples parameters in HDBSCAN. This value is automatically set to the same value of min_cluster_size . However, you can set it indepedently if you want to reduce the number of generated outliers. Lowering this value will result in less noise being generated. from bertopic import BERTopic from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 10 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True , min_samples = 5 ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Note Although this will lower outliers found in the data, this might force outliers to be put into topics where they do not belong. So make sure to strike a balance between keeping noise and reducing outliers. Second, after training our BERTopic model, we can assign outliers to topics. By setting calculate_probabilities=True , we calculate the probability of a document belonging to any topic. That way, we can select, for each document, the topic with the the highest probability. Thus, although we do generate an outlier class in our BERTopic model, we can assign documents to an actual topic. To do this, we can set a probability threshold and assign each document to a topic based on their probabilities: import numpy as np probability_threshold = 0.01 new_topics = [ np . argmax ( prob ) if max ( prob ) >= probability_threshold else - 1 for prob in probs ] Note The topics assigned using the above method can result in topics different from using .fit_transform() . This is expected behavior as HDBSCAN is merely trying to imitate soft clustering after fitting the model and it is not a core component of assigning points to clusters. Third, we can replace HDBSCAN with any other clustering algorithm that we want. So we can choose a clustering algorithm, like k-Means, that does not produce any outliers at all. Using k-Means instead of HDBSCAN is straightforward: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model ) How can I speed up BERTopic? \u00b6 You can speed up BERTopic by either generating your embeddings beforehand or by setting calculate_probabilities to False. Calculating the probabilities is quite expensive and can significantly increase the computation time. Thus, only use it if you do not mind waiting a bit before the model is done running or if you have less than 50_000 documents. Also, make sure to use a GPU when extracting the sentence/document embeddings. Transformer models typically require a GPU and using only a CPU can slow down computation time quite a lot. However, if you do not have access to a GPU, looking into quantization might help. I am facing memory issues. Help! \u00b6 There are several ways to perform computation with large datasets. * First, you can set low_memory to True when instantiating BERTopic. This may prevent blowing up the memory in UMAP. Second, setting calculate_probabilities to False when instantiating BERTopic prevents a huge document-topic probability matrix from being created. Moreover, HDBSCAN is quite slow when it tries to calculate probabilities on large datasets. Third, you can set the minimum frequency of words in the CountVectorizer class to reduce the size of the resulting sparse c-TF-IDF matrix. You can do this as follows: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory. Fourth, you can use online topic modeling instead to use BERTopic on big data by training the model in chunks If the problem persists, then this could be an issue related to your available memory. The processing of millions of documents is quite computationally expensive and sufficient RAM is necessary. I have only a few topics, how do I increase them? \u00b6 There are several reasons why your topic model may result in only a few topics: First, you might only have a few documents (~1000). This makes it very difficult to properly extract topics due to the little amount of data available. Increasing the number of documents might solve your issues. Second, min_topic_size might be simply too large for your number of documents. If you decrease the minimum size of topics, then you are much more likely to increase the number of topics generated. You could also decrease the n_neighbors parameter used in UMAP if this does not work. Third, although this does not happen very often, there simply aren't that many topics to be found in your documents. You can often see this when you have many -1 topics, which is actually not a topic but a category of outliers. I have too many topics, how do I decrease them? \u00b6 If you have a large dataset, then it is possible to generate thousands of topics. Especially with large datasets, there is a good chance they actually contain many small topics. In practice, you might want a few hundred topics at most in order to interpret them nicely. There are a few ways of increasing the number of generated topics: First, we can set the min_topic_size in the BERTopic initialization much higher (e.g., 300) to make sure that those small clusters will not be generated. This is a HDBSCAN parameter that specifies what the minimum number of documents are needed in a cluster. More documents in a cluster means less topics will be generated. Second, you can create a custom UMAP model and set n_neighbors much higher than the default 15 (e.g., 200). This also prevents those micro clusters to be generated as it will needs quite a number of neighboring documents to create a cluster. Third, we can set nr_topics to a value that seems logical to the user. Do note that topics are forced to merge together which might result in a lower quality of topics. In practice, I would advise using nr_topic=\"auto\" as that will merge topics together that are very similar. Dissimilar topics will therefore remain separated. How do I calculate the probabilities of all topics in a document? \u00b6 Although it is possible to calculate all the probabilities, the process of doing so is quite computationally inefficient and might significantly increase the computation time. To prevent this, the probabilities are not calculated as a default. In order to calculate, you will have to set calculate_probabilities to True: from bertopic import BERTopic topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs ) Numpy gives me an error when running BERTopic \u00b6 With the release of Numpy 1.20.0, there have been significant issues with using that version (and previous) due to compilation issues and pypi. This is a known issue with the order of install using pypi. You can find more details about this issue here and here . I would suggest doing one of the following: Install the newest version from BERTopic (>= v0.5). You can install hdbscan with pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation which might resolve the issue Install BERTopic in a fresh environment using these steps. How can I run BERTopic without an internet connection? \u00b6 The great thing about using sentence-transformers is that it searches automatically for an embedding model locally. If it cannot find one, it will download the pre-trained model from its servers. Make sure that you set the correct path for sentence-transformers to work. You can find a bit more about that here . You can download the corresponding model here and unzip it. Then, simply use the following to create your embedding model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( 'path/to/unzipped/model' ) Then, pass it to BERTopic: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = embedding_model ) Can I use the GPU to speed up the model? \u00b6 Yes. The GPU is automatically used when you use a SentenceTransformer or Flair embedding model. Using a CPU would then definitely slow things down. However, you can use other embeddings like TF-IDF or Doc2Vec embeddings in BERTopic which do not depend on GPU acceleration. You can use cuML to speed up both UMAP and HDBSCAN through GPU acceleration: from bertopic import BERTopic from cuml.cluster import HDBSCAN from cuml.manifold import UMAP # Create instances of GPU-accelerated UMAP and HDBSCAN umap_model = UMAP ( n_components = 5 , n_neighbors = 15 , min_dist = 0.0 ) hdbscan_model = HDBSCAN ( min_samples = 10 , gen_min_span_tree = True ) # Pass the above models to be used in BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Depending on the embeddings you are using, you might want to normalize them first in order to force a cosine-related distance metric in UMAP: from cuml.preprocessing import normalize embeddings = normalize ( embeddings ) How can I use BERTopic with Chinese documents? \u00b6 Currently, CountVectorizer tokenizes text by splitting whitespace which does not work for Chinese. In order to get it to work, you will have to create a custom CountVectorizer with jieba : from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Next, we pass our custom vectorizer to BERTopic and create our topic model: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = model , verbose = True , vectorizer_model = vectorizer ) topics , _ = topic_model . fit_transform ( docs , embeddings = embeddings ) Why does it take so long to import BERTopic? \u00b6 The main culprit here seems to be UMAP. After running tests with Tuna we can see that most of the resources when importing BERTopic can be dedicated to UMAP: Unfortunately, there currently is no fix for this issue. The most recent ticket regarding this issue can be found here . Should I preprocess the data? \u00b6 No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#why-are-the-results-not-consistent-between-runs","text":"Due to the stochastic nature of UMAP, the results from BERTopic might differ even if you run the same code multiple times. Using custom embeddings allows you to try out BERTopic several times until you find the topics that suit you best. You only need to generate the embeddings itself once and run BERTopic several times with different parameters. If you want to reproduce the results, at the expense of performance , you can set a random_state in UMAP to prevent any stochastic behavior: from bertopic import BERTopic from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , random_state = 42 ) topic_model = BERTopic ( umap_model = umap_model )","title":"Why are the results not consistent between runs?"},{"location":"faq.html#which-embedding-model-should-i-choose","text":"Unfortunately, there is not a definitive list of the best models for each language, this highly depends on your data, the model, and your specific use-case. However, the default model in BERTopic ( \"all-MiniLM-L6-v2\" ) works great for English documents. In contrast, for multi-lingual documents or any other language, \"paraphrase-multilingual-MiniLM-L12-v2\"\" has shown great performance. If you want to use a model that provides a higher quality, but takes more compute time, then I would advise using all-mpnet-base-v2 and paraphrase-multilingual-mpnet-base-v2 instead. SentenceTransformers SentenceTransformers work typically quite well and are the preferred models to use. They are great at generating document embeddings and have several multi-lingual versions available. \ud83e\udd17 transformers BERTopic allows you to use any \ud83e\udd17 transformers model. These models are typically embeddings created on a word/sentence level but can easily be pooled using Flair (see Guides/Embeddings). If you have a specific language for which you want to generate embeddings, you can choose the model here .","title":"Which embedding model should I choose?"},{"location":"faq.html#how-do-i-reduce-topic-outliers","text":"There are three ways in reducing outliers. First, the amount of datapoint classified as outliers is handled by the min_samples parameters in HDBSCAN. This value is automatically set to the same value of min_cluster_size . However, you can set it indepedently if you want to reduce the number of generated outliers. Lowering this value will result in less noise being generated. from bertopic import BERTopic from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 10 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True , min_samples = 5 ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Note Although this will lower outliers found in the data, this might force outliers to be put into topics where they do not belong. So make sure to strike a balance between keeping noise and reducing outliers. Second, after training our BERTopic model, we can assign outliers to topics. By setting calculate_probabilities=True , we calculate the probability of a document belonging to any topic. That way, we can select, for each document, the topic with the the highest probability. Thus, although we do generate an outlier class in our BERTopic model, we can assign documents to an actual topic. To do this, we can set a probability threshold and assign each document to a topic based on their probabilities: import numpy as np probability_threshold = 0.01 new_topics = [ np . argmax ( prob ) if max ( prob ) >= probability_threshold else - 1 for prob in probs ] Note The topics assigned using the above method can result in topics different from using .fit_transform() . This is expected behavior as HDBSCAN is merely trying to imitate soft clustering after fitting the model and it is not a core component of assigning points to clusters. Third, we can replace HDBSCAN with any other clustering algorithm that we want. So we can choose a clustering algorithm, like k-Means, that does not produce any outliers at all. Using k-Means instead of HDBSCAN is straightforward: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model )","title":"How do I reduce topic outliers?"},{"location":"faq.html#how-can-i-speed-up-bertopic","text":"You can speed up BERTopic by either generating your embeddings beforehand or by setting calculate_probabilities to False. Calculating the probabilities is quite expensive and can significantly increase the computation time. Thus, only use it if you do not mind waiting a bit before the model is done running or if you have less than 50_000 documents. Also, make sure to use a GPU when extracting the sentence/document embeddings. Transformer models typically require a GPU and using only a CPU can slow down computation time quite a lot. However, if you do not have access to a GPU, looking into quantization might help.","title":"How can I speed up BERTopic?"},{"location":"faq.html#i-am-facing-memory-issues-help","text":"There are several ways to perform computation with large datasets. * First, you can set low_memory to True when instantiating BERTopic. This may prevent blowing up the memory in UMAP. Second, setting calculate_probabilities to False when instantiating BERTopic prevents a huge document-topic probability matrix from being created. Moreover, HDBSCAN is quite slow when it tries to calculate probabilities on large datasets. Third, you can set the minimum frequency of words in the CountVectorizer class to reduce the size of the resulting sparse c-TF-IDF matrix. You can do this as follows: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) The min_df parameter is used to indicate the minimum frequency of words. Setting this value larger than 1 can significantly reduce memory. Fourth, you can use online topic modeling instead to use BERTopic on big data by training the model in chunks If the problem persists, then this could be an issue related to your available memory. The processing of millions of documents is quite computationally expensive and sufficient RAM is necessary.","title":"I am facing memory issues. Help!"},{"location":"faq.html#i-have-only-a-few-topics-how-do-i-increase-them","text":"There are several reasons why your topic model may result in only a few topics: First, you might only have a few documents (~1000). This makes it very difficult to properly extract topics due to the little amount of data available. Increasing the number of documents might solve your issues. Second, min_topic_size might be simply too large for your number of documents. If you decrease the minimum size of topics, then you are much more likely to increase the number of topics generated. You could also decrease the n_neighbors parameter used in UMAP if this does not work. Third, although this does not happen very often, there simply aren't that many topics to be found in your documents. You can often see this when you have many -1 topics, which is actually not a topic but a category of outliers.","title":"I have only a few topics, how do I increase them?"},{"location":"faq.html#i-have-too-many-topics-how-do-i-decrease-them","text":"If you have a large dataset, then it is possible to generate thousands of topics. Especially with large datasets, there is a good chance they actually contain many small topics. In practice, you might want a few hundred topics at most in order to interpret them nicely. There are a few ways of increasing the number of generated topics: First, we can set the min_topic_size in the BERTopic initialization much higher (e.g., 300) to make sure that those small clusters will not be generated. This is a HDBSCAN parameter that specifies what the minimum number of documents are needed in a cluster. More documents in a cluster means less topics will be generated. Second, you can create a custom UMAP model and set n_neighbors much higher than the default 15 (e.g., 200). This also prevents those micro clusters to be generated as it will needs quite a number of neighboring documents to create a cluster. Third, we can set nr_topics to a value that seems logical to the user. Do note that topics are forced to merge together which might result in a lower quality of topics. In practice, I would advise using nr_topic=\"auto\" as that will merge topics together that are very similar. Dissimilar topics will therefore remain separated.","title":"I have too many topics, how do I decrease them?"},{"location":"faq.html#how-do-i-calculate-the-probabilities-of-all-topics-in-a-document","text":"Although it is possible to calculate all the probabilities, the process of doing so is quite computationally inefficient and might significantly increase the computation time. To prevent this, the probabilities are not calculated as a default. In order to calculate, you will have to set calculate_probabilities to True: from bertopic import BERTopic topic_model = BERTopic ( calculate_probabilities = True ) topics , probs = topic_model . fit_transform ( docs )","title":"How do I calculate the probabilities of all topics in a document?"},{"location":"faq.html#numpy-gives-me-an-error-when-running-bertopic","text":"With the release of Numpy 1.20.0, there have been significant issues with using that version (and previous) due to compilation issues and pypi. This is a known issue with the order of install using pypi. You can find more details about this issue here and here . I would suggest doing one of the following: Install the newest version from BERTopic (>= v0.5). You can install hdbscan with pip install hdbscan --no-cache-dir --no-binary :all: --no-build-isolation which might resolve the issue Install BERTopic in a fresh environment using these steps.","title":"Numpy gives me an error when running BERTopic"},{"location":"faq.html#how-can-i-run-bertopic-without-an-internet-connection","text":"The great thing about using sentence-transformers is that it searches automatically for an embedding model locally. If it cannot find one, it will download the pre-trained model from its servers. Make sure that you set the correct path for sentence-transformers to work. You can find a bit more about that here . You can download the corresponding model here and unzip it. Then, simply use the following to create your embedding model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( 'path/to/unzipped/model' ) Then, pass it to BERTopic: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = embedding_model )","title":"How can I run BERTopic without an internet connection?"},{"location":"faq.html#can-i-use-the-gpu-to-speed-up-the-model","text":"Yes. The GPU is automatically used when you use a SentenceTransformer or Flair embedding model. Using a CPU would then definitely slow things down. However, you can use other embeddings like TF-IDF or Doc2Vec embeddings in BERTopic which do not depend on GPU acceleration. You can use cuML to speed up both UMAP and HDBSCAN through GPU acceleration: from bertopic import BERTopic from cuml.cluster import HDBSCAN from cuml.manifold import UMAP # Create instances of GPU-accelerated UMAP and HDBSCAN umap_model = UMAP ( n_components = 5 , n_neighbors = 15 , min_dist = 0.0 ) hdbscan_model = HDBSCAN ( min_samples = 10 , gen_min_span_tree = True ) # Pass the above models to be used in BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Depending on the embeddings you are using, you might want to normalize them first in order to force a cosine-related distance metric in UMAP: from cuml.preprocessing import normalize embeddings = normalize ( embeddings )","title":"Can I use the GPU to speed up the model?"},{"location":"faq.html#how-can-i-use-bertopic-with-chinese-documents","text":"Currently, CountVectorizer tokenizes text by splitting whitespace which does not work for Chinese. In order to get it to work, you will have to create a custom CountVectorizer with jieba : from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Next, we pass our custom vectorizer to BERTopic and create our topic model: from bertopic import BERTopic topic_model = BERTopic ( embedding_model = model , verbose = True , vectorizer_model = vectorizer ) topics , _ = topic_model . fit_transform ( docs , embeddings = embeddings )","title":"How can I use BERTopic with Chinese documents?"},{"location":"faq.html#why-does-it-take-so-long-to-import-bertopic","text":"The main culprit here seems to be UMAP. After running tests with Tuna we can see that most of the resources when importing BERTopic can be dedicated to UMAP: Unfortunately, there currently is no fix for this issue. The most recent ticket regarding this issue can be found here .","title":"Why does it take so long to import BERTopic?"},{"location":"faq.html#should-i-preprocess-the-data","text":"No. By using document embeddings there is typically no need to preprocess the data as all parts of a document are important in understanding the general topic of the document. Although this holds true in 99% of cases, if you have data that contains a lot of noise, for example, HTML-tags, then it would be best to remove them. HTML-tags typically do not contribute to the meaning of a document and should therefore be removed. However, if you apply topic modeling to HTML-code to extract topics of code, then it becomes important.","title":"Should I preprocess the data?"},{"location":"algorithm/algorithm.html","text":"The Algorithm \u00b6 Below, you will find different types of overviews of each step in BERTopic's main algorithm. Each successive overview will be more in-depth than the previous overview. The aim of this approach is to make the underlying algorithm as intuitive as possible for a wide range of users. Visual Overview \u00b6 This visual overview reduces BERTopic to four main steps, namely the embedding of documents, the clustering of documents, the topic extraction, and the topic diversification. Each step contains one or more sub-steps that you can read a bit more about below. Embed documents 1. Embeddings We start by converting our documents to vector representations through the use of language models. SBERT \ud83e\udd17 Transformers spaCy Cluster embeddings 2. Dimension Reduction The vector representations are reduced in dimensionality so that clustering algorithms have an easier time finding clusters. UMAP PCA Truncated SVD 3. Clustering A clustering algorithm is used to cluster the reduced vectors in order to find semantically similar documents. HDBSCAN k-Means BIRCH Topic Representation 4. Bag-of-Words We tokenize each topic into a bag-of-words representation that allows us to process the data without affecting the input embeddings. CountVectorizer 5. Topic Representation We calculate words that are interesting to each topic with a class-based TF-IDF procedure called c-TF-IDF. c-TF-IDF BM-25 (Optional) Topic Diversity 6. Topic Diversity Maximal Marginal Relevance is used to diversify words in each topic which removes repeating and similar words. MMR Code Overview \u00b6 After going through the visual overview, this code overview demonstrates the algorithm using BERTopic. An advantage of using BERTopic is each major step in its algorithm can be explicitly defined, thereby making the process not only transparent but also more intuitive. from sentence_transformers import SentenceTransformer from umap import UMAP from hdbscan import HDBSCAN from sklearn.feature_extraction.text import CountVectorizer from bertopic.vectorizers import ClassTfidfTransformer from bertopic import BERTopic # Step 1 - Extract embeddings embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Step 2 - Reduce dimensionality umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) # Step 3 - Cluster reduced embeddings hdbscan_model = HDBSCAN ( min_cluster_size = 15 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Step 4 - Tokenize topics vectorizer_model = CountVectorizer ( stop_words = \"english\" ) # Step 5 - Create topic representation ctfidf_model = ClassTfidfTransformer () # All steps together topic_model = BERTopic ( embedding_model = embedding_model , # Step 1 - Extract embeddings umap_model = umap_model , # Step 2 - Reduce dimensionality hdbscan_model = hdbscan_model , # Step 3 - Cluster reduced embeddings vectorizer_model = vectorizer_model , # Step 4 - Tokenize topics ctfidf_model = ctfidf_model , # Step 5 - Extract topic words diversity = 0.5 # Step 6 - Diversify topic words ) Detailed Overview \u00b6 This overview describes each step in more detail such that you can get an intuitive feeling as to what models might fit best at each step in your use case. 1. Embed documents \u00b6 We start by converting our documents to numerical representations. Although there are many methods for doing so the default in BERTopic is sentence-transformers . These models are often optimized for semantic similarity which helps tremendously in our clustering task. Moreover, they are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence-transformers model but there are two models that are set as defaults: \"all-MiniLM-L6-v2\" \"paraphrase-multilingual-MiniLM-L12-v2\" The first is an English language model trained specifically for semantic similarity tasks which work quite well for most use-cases. The second model is very similar to the first with one major difference is that the multilingual models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English. Tip Although BERTopic uses sentence-transformers models as a default, you can choose any embedding model that fits your use case. Follow the guide here for selecting and customizing your model. 2. Dimensionality reduction \u00b6 After having created our numerical representations of the documents we have to reduce the dimensionality of these representations. Cluster models typically have difficulty handling high dimensional data due to the curse of dimensionality. There are great approaches that can reduce dimensionality, such as PCA, but as a default UMAP is selected in BERTopic. It is a technique that can keep some of a dataset's local and global structure when reducing its dimensionality. This structure is important to keep as it contains the information necessary to create clusters of semantically similar documents. Tip Although BERTopic uses UMAP as a default, you can choose any dimensionality reduction model that fits your use case. Follow the guide here for selecting and customizing your model. 3. Cluster Documents \u00b6 After having reduced our embeddings, we can start clustering our data. For that, we leverage a density-based clustering technique, HDBSCAN. It can find clusters of different shapes and has the nice feature of identifying outliers where possible. As a result, we do not force documents in a cluster where they might note belong. This will improve the resulting topic representation as there is less noise to draw from. Tip Although BERTopic uses HDBSCAN as a default, you can choose any cluster model that fits your use case. Follow the guide here for selecting and customizing your model. 4. Bag-of-words \u00b6 Before we can start creating the topic representation we first need to select a technique that allows for modularity in BERTopic's algorithm. When we use HDBSCAN as a cluster model, we may assume that our clusters having different degrees of density and different shapes. This means that a centroid-based topic representation technique might not be the best fitting model. In other words, we want a topic representation technique that makes little to no assumption on the expected structure of the clusters. To do this, we first combine all documents in a cluster into a single document. That, very long, document then represents the cluster. Then, we can count how often each word appears in each cluster. This generates something called a bag-of-words representation in which resides the frequency of each word in each cluster. This bag-of-words representation is therefore on a cluster-level and not on a document-level. This distinction is important as we are interested in words on a topic-level (i.e., cluster-level). By using a bag-of-words representation, no assumption is made with respect to the structure of the clusters. Moreover, the bag-of-words representation is L1-normalized to account for clusters that have different sizes. Tip There are many ways you can tune or change the bag-of-words step. This step allows for processing the documents however you want without affecting the first step, embedding the documents. You can follow the guide here for more information about tokenization options in BERTopic. 5. Topic representation \u00b6 From the generated bag-of-words representation, we want to know what makes one cluster different from another? Which words are typical for cluster 1 and not so much for all other clusters? To solve this, we need to modify TF-IDF such that it considers topics (i.e., clusters) instead of documents. When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF : Each cluster is converted to a single document instead of a set of documents. Then, we extract the frequency of word x in class c , where c refers to the cluster we created before. This results in our class-based tf representation. This representation is L1-normalized to account for the differences in topic sizes. Then, we take take the logarithm of one plus the average number of words per class A divided by the frequency of word x across all classes. We add plus one within the logarithm to force values to be positive. This results in our class-based idf representation. Like with the classic TF-IDF, we then multiply tf with idf to get the importance score per word in each class. In other words, the classical TF-IDF procedure is not used here but a modified version of the algorithm that allows for a much better representation. Tip In the ClassTfidfTransformer , there are a few parameters that might be worth exploring, including an option to perform additional BM-25 weighting. You can find more information about that here . 6. (Optional) Maximal Marginal Relevance \u00b6 After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit the documents. For example, if you have a set of documents that are written by the same person whose signature will be in the topic description. To improve the coherence of words, Maximal Marginal Relevance was used to find the most coherent words without having too much overlap between the words themselves. This results in the removal of words that do not contribute to a topic. You can also use this technique to diversify the words generated in the topic representation. At times, many variations of the same word can end up in the topic representation. To reduce the number of synonyms, we can increase the diversity among words whilst still being similar to the topic representation.","title":"The Algorithm"},{"location":"algorithm/algorithm.html#the-algorithm","text":"Below, you will find different types of overviews of each step in BERTopic's main algorithm. Each successive overview will be more in-depth than the previous overview. The aim of this approach is to make the underlying algorithm as intuitive as possible for a wide range of users.","title":"The Algorithm"},{"location":"algorithm/algorithm.html#visual-overview","text":"This visual overview reduces BERTopic to four main steps, namely the embedding of documents, the clustering of documents, the topic extraction, and the topic diversification. Each step contains one or more sub-steps that you can read a bit more about below. Embed documents 1. Embeddings We start by converting our documents to vector representations through the use of language models. SBERT \ud83e\udd17 Transformers spaCy Cluster embeddings 2. Dimension Reduction The vector representations are reduced in dimensionality so that clustering algorithms have an easier time finding clusters. UMAP PCA Truncated SVD 3. Clustering A clustering algorithm is used to cluster the reduced vectors in order to find semantically similar documents. HDBSCAN k-Means BIRCH Topic Representation 4. Bag-of-Words We tokenize each topic into a bag-of-words representation that allows us to process the data without affecting the input embeddings. CountVectorizer 5. Topic Representation We calculate words that are interesting to each topic with a class-based TF-IDF procedure called c-TF-IDF. c-TF-IDF BM-25 (Optional) Topic Diversity 6. Topic Diversity Maximal Marginal Relevance is used to diversify words in each topic which removes repeating and similar words. MMR","title":"Visual Overview"},{"location":"algorithm/algorithm.html#code-overview","text":"After going through the visual overview, this code overview demonstrates the algorithm using BERTopic. An advantage of using BERTopic is each major step in its algorithm can be explicitly defined, thereby making the process not only transparent but also more intuitive. from sentence_transformers import SentenceTransformer from umap import UMAP from hdbscan import HDBSCAN from sklearn.feature_extraction.text import CountVectorizer from bertopic.vectorizers import ClassTfidfTransformer from bertopic import BERTopic # Step 1 - Extract embeddings embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Step 2 - Reduce dimensionality umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) # Step 3 - Cluster reduced embeddings hdbscan_model = HDBSCAN ( min_cluster_size = 15 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Step 4 - Tokenize topics vectorizer_model = CountVectorizer ( stop_words = \"english\" ) # Step 5 - Create topic representation ctfidf_model = ClassTfidfTransformer () # All steps together topic_model = BERTopic ( embedding_model = embedding_model , # Step 1 - Extract embeddings umap_model = umap_model , # Step 2 - Reduce dimensionality hdbscan_model = hdbscan_model , # Step 3 - Cluster reduced embeddings vectorizer_model = vectorizer_model , # Step 4 - Tokenize topics ctfidf_model = ctfidf_model , # Step 5 - Extract topic words diversity = 0.5 # Step 6 - Diversify topic words )","title":"Code Overview"},{"location":"algorithm/algorithm.html#detailed-overview","text":"This overview describes each step in more detail such that you can get an intuitive feeling as to what models might fit best at each step in your use case.","title":"Detailed Overview"},{"location":"algorithm/algorithm.html#1-embed-documents","text":"We start by converting our documents to numerical representations. Although there are many methods for doing so the default in BERTopic is sentence-transformers . These models are often optimized for semantic similarity which helps tremendously in our clustering task. Moreover, they are great for creating either document- or sentence-embeddings. In BERTopic, you can choose any sentence-transformers model but there are two models that are set as defaults: \"all-MiniLM-L6-v2\" \"paraphrase-multilingual-MiniLM-L12-v2\" The first is an English language model trained specifically for semantic similarity tasks which work quite well for most use-cases. The second model is very similar to the first with one major difference is that the multilingual models work for 50+ languages. This model is quite a bit larger than the first and is only selected if you select any language other than English. Tip Although BERTopic uses sentence-transformers models as a default, you can choose any embedding model that fits your use case. Follow the guide here for selecting and customizing your model.","title":"1. Embed documents"},{"location":"algorithm/algorithm.html#2-dimensionality-reduction","text":"After having created our numerical representations of the documents we have to reduce the dimensionality of these representations. Cluster models typically have difficulty handling high dimensional data due to the curse of dimensionality. There are great approaches that can reduce dimensionality, such as PCA, but as a default UMAP is selected in BERTopic. It is a technique that can keep some of a dataset's local and global structure when reducing its dimensionality. This structure is important to keep as it contains the information necessary to create clusters of semantically similar documents. Tip Although BERTopic uses UMAP as a default, you can choose any dimensionality reduction model that fits your use case. Follow the guide here for selecting and customizing your model.","title":"2. Dimensionality reduction"},{"location":"algorithm/algorithm.html#3-cluster-documents","text":"After having reduced our embeddings, we can start clustering our data. For that, we leverage a density-based clustering technique, HDBSCAN. It can find clusters of different shapes and has the nice feature of identifying outliers where possible. As a result, we do not force documents in a cluster where they might note belong. This will improve the resulting topic representation as there is less noise to draw from. Tip Although BERTopic uses HDBSCAN as a default, you can choose any cluster model that fits your use case. Follow the guide here for selecting and customizing your model.","title":"3. Cluster Documents"},{"location":"algorithm/algorithm.html#4-bag-of-words","text":"Before we can start creating the topic representation we first need to select a technique that allows for modularity in BERTopic's algorithm. When we use HDBSCAN as a cluster model, we may assume that our clusters having different degrees of density and different shapes. This means that a centroid-based topic representation technique might not be the best fitting model. In other words, we want a topic representation technique that makes little to no assumption on the expected structure of the clusters. To do this, we first combine all documents in a cluster into a single document. That, very long, document then represents the cluster. Then, we can count how often each word appears in each cluster. This generates something called a bag-of-words representation in which resides the frequency of each word in each cluster. This bag-of-words representation is therefore on a cluster-level and not on a document-level. This distinction is important as we are interested in words on a topic-level (i.e., cluster-level). By using a bag-of-words representation, no assumption is made with respect to the structure of the clusters. Moreover, the bag-of-words representation is L1-normalized to account for clusters that have different sizes. Tip There are many ways you can tune or change the bag-of-words step. This step allows for processing the documents however you want without affecting the first step, embedding the documents. You can follow the guide here for more information about tokenization options in BERTopic.","title":"4. Bag-of-words"},{"location":"algorithm/algorithm.html#5-topic-representation","text":"From the generated bag-of-words representation, we want to know what makes one cluster different from another? Which words are typical for cluster 1 and not so much for all other clusters? To solve this, we need to modify TF-IDF such that it considers topics (i.e., clusters) instead of documents. When you apply TF-IDF as usual on a set of documents, what you are doing is comparing the importance of words between documents. Now, what if, we instead treat all documents in a single category (e.g., a cluster) as a single document and then apply TF-IDF? The result would be importance scores for words within a cluster. The more important words are within a cluster, the more it is representative of that topic. In other words, if we extract the most important words per cluster, we get descriptions of topics ! This model is called class-based TF-IDF : Each cluster is converted to a single document instead of a set of documents. Then, we extract the frequency of word x in class c , where c refers to the cluster we created before. This results in our class-based tf representation. This representation is L1-normalized to account for the differences in topic sizes. Then, we take take the logarithm of one plus the average number of words per class A divided by the frequency of word x across all classes. We add plus one within the logarithm to force values to be positive. This results in our class-based idf representation. Like with the classic TF-IDF, we then multiply tf with idf to get the importance score per word in each class. In other words, the classical TF-IDF procedure is not used here but a modified version of the algorithm that allows for a much better representation. Tip In the ClassTfidfTransformer , there are a few parameters that might be worth exploring, including an option to perform additional BM-25 weighting. You can find more information about that here .","title":"5. Topic representation"},{"location":"algorithm/algorithm.html#6-optional-maximal-marginal-relevance","text":"After having generated the c-TF-IDF representations, we have a set of words that describe a collection of documents. Technically, this does not mean that this collection of words describes a coherent topic. In practice, we will see that many of the words do describe a similar topic but some words will, in a way, overfit the documents. For example, if you have a set of documents that are written by the same person whose signature will be in the topic description. To improve the coherence of words, Maximal Marginal Relevance was used to find the most coherent words without having too much overlap between the words themselves. This results in the removal of words that do not contribute to a topic. You can also use this technique to diversify the words generated in the topic representation. At times, many variations of the same word can end up in the topic representation. To reduce the number of synonyms, we can increase the diversity among words whilst still being similar to the topic representation.","title":"6. (Optional) Maximal Marginal Relevance"},{"location":"api/bertopic.html","text":"BERTopic \u00b6 BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. The default embedding model is all-MiniLM-L6-v2 when selecting language=\"english\" and paraphrase-multilingual-MiniLM-L12-v2 when selecting language=\"multilingual\" . Attributes: Name Type Description topics_ List[int]) The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ List [ float ] The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When calculate_probabilities=True , then it is the probabilities of all topics per document. topic_sizes_ Mapping[int, int]) The size of each topic topic_mapper_ TopicMapper) A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ Mapping[int, Tuple[int, float]]) The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ csr_matrix) The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run .vectorizer_model.get_feature_names() or .vectorizer_model.get_feature_names_out() topic_labels_ Mapping[int, str]) The default labels for each topic. custom_labels_ List[str]) Custom labels for each topic. topic_embeddings_ np.ndarray) The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ Mapping[int, str]) The representative documents for each topic. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probabilities = topic_model . fit_transform ( docs ) If you want to use your own embedding model, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. Source code in bertopic\\_bertopic.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 class BERTopic : \"\"\"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. The default embedding model is `all-MiniLM-L6-v2` when selecting `language=\"english\"` and `paraphrase-multilingual-MiniLM-L12-v2` when selecting `language=\"multilingual\"`. Attributes: topics_ (List[int]) : The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ (List[float]): The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When `calculate_probabilities=True`, then it is the probabilities of all topics per document. topic_sizes_ (Mapping[int, int]) : The size of each topic topic_mapper_ (TopicMapper) : A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ (Mapping[int, Tuple[int, float]]) : The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ (csr_matrix) : The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run `.vectorizer_model.get_feature_names()` or `.vectorizer_model.get_feature_names_out()` topic_labels_ (Mapping[int, str]) : The default labels for each topic. custom_labels_ (List[str]) : Custom labels for each topic. topic_embeddings_ (np.ndarray) : The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ (Mapping[int, str]) : The representative documents for each topic. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probabilities = topic_model.fit_transform(docs) ``` If you want to use your own embedding model, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") topic_model = BERTopic(embedding_model=sentence_model) ``` Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. \"\"\" def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , diversity : float = None , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , ctfidf_model : TfidfTransformer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. The default sentence-transformers model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2` sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. low_memory: Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. diversity: Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has `.fit` and `.transform` functions. hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has `.fit` and `.predict` functions along with the `.labels_` variable. vectorizer_model: Pass in a custom `CountVectorizer` instead of the default model. ctfidf_model: Pass in a custom ClassTfidfTransformer instead of the default model. \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . diversity = diversity self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () # UMAP or another algorithm that has .fit and .transform functions self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN or another clustering algorithm that has .fit and .predict functions and # the .labels_ variable to extract the labels self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Public attributes self . topics_ = None self . probabilities_ = None self . topic_sizes_ = None self . topic_mapper_ = None self . topic_representations_ = None self . topic_embeddings_ = None self . topic_labels_ = None self . custom_labels_ = None self . representative_docs_ = None self . c_tf_idf_ = None # Private attributes for internal tracking purposes self . _outliers = 1 self . _merged_topics = None if verbose : logger . set_level ( \"DEBUG\" ) def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster reduced embeddings documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs ( original_topics = True ) self . probabilities_ = self . _map_probabilities ( probabilities , original_topics = True ) predictions = documents . Topic . to_list () return predictions , self . probabilities_ def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) # Extract predictions and probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) # Calculate probabilities if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) logger . info ( \"Calculated probabilities with HDBSCAN\" ) else : predictions = self . hdbscan_model . predict ( umap_embeddings ) probabilities = None logger . info ( \"Predicted clusters\" ) # Map probabilities and predictions probabilities = self . _map_probabilities ( probabilities , original_topics = True ) predictions = self . _map_predictions ( predictions ) return predictions , probabilities def partial_fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a `partial_fit` method in order to incrementally train the topic model. Likewise, the `bertopic.vectorizers.OnlineCountVectorizer` is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: 1. Generate embeddings with a pre-traing language model 2. Incrementally update the dimensionality reduction algorithm with `partial_fit` 3. Incrementally update the cluster algorithm with `partial_fit` 4. Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use `partial_fit` with batches and not single documents for the best performance. Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'))[\"data\"] # Prepare sub-models that support online learning umap_model = IncrementalPCA(n_components=5) cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0) vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01) topic_model = BERTopic(umap_model=umap_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model) # Incrementally fit the topic model by training on 1000 documents at a time for index in range(0, len(docs), 1000): topic_model.partial_fit(docs[index: index+1000]) ``` \"\"\" # Checks check_embeddings_shape ( embeddings , documents ) if not hasattr ( self . hdbscan_model , \"partial_fit\" ): raise ValueError ( \"In order to use `.partial_fit`, the cluster model should have \" \"a `.partial_fit` function.\" ) # Prepare documents if isinstance ( documents , str ): documents = [ documents ] documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : if self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) else : if self . embedding_model is not None and self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y , partial_fit = True ) # Cluster reduced embeddings documents , self . probabilities_ = self . _cluster_embeddings ( umap_embeddings , documents , partial_fit = True ) topics = documents . Topic . to_list () # Map and find new topics if not self . topic_mapper_ : self . topic_mapper_ = TopicMapper ( topics ) mappings = self . topic_mapper_ . get_mappings () new_topics = set ( topics ) . difference ( set ( mappings . keys ())) new_topic_ids = { topic : max ( mappings . values ()) + index + 1 for index , topic in enumerate ( new_topics )} self . topic_mapper_ . add_new_topics ( new_topic_ids ) updated_mappings = self . topic_mapper_ . get_mappings () updated_topics = [ updated_mappings [ topic ] for topic in topics ] documents [ \"Topic\" ] = updated_topics # Add missing topics (topics that were originally created but are now missing) if self . topic_representations_ : missing_topics = set ( self . topic_representations_ . keys ()) . difference ( set ( updated_topics )) for missing_topic in missing_topics : documents . loc [ len ( documents ), :] = [ \" \" , len ( documents ), missing_topic ] else : missing_topics = {} # Prepare documents documents_per_topic = documents . sort_values ( \"Topic\" ) . groupby ([ 'Topic' ], as_index = False ) updated_topics = documents_per_topic . first () . Topic . astype ( int ) documents_per_topic = documents_per_topic . agg ({ 'Document' : ' ' . join }) # Update topic representations self . c_tf_idf_ , updated_words = self . _c_tf_idf ( documents_per_topic , partial_fit = True ) self . topic_representations_ = self . _extract_words_per_topic ( updated_words , self . c_tf_idf_ , labels = updated_topics ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} # Update topic sizes if len ( missing_topics ) > 0 : documents = documents . iloc [: - len ( missing_topics )] if self . topic_sizes_ is None : self . _update_topic_size ( documents ) else : sizes = documents . groupby ([ 'Topic' ], as_index = False ) . count () for _ , row in sizes . iterrows (): topic = int ( row . Topic ) if self . topic_sizes_ . get ( topic ) is not None and topic not in missing_topics : self . topic_sizes_ [ topic ] += int ( row . Document ) elif self . topic_sizes_ . get ( topic ) is None : self . topic_sizes_ [ topic ] = int ( row . Document ) self . topics_ = documents . Topic . astype ( int ) . tolist () return self def topics_over_time ( self , docs : List [ str ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp *t*. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp *t*. Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ]) def topics_per_class ( self , docs : List [ str ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for _ , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + self . _outliers ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class def hierarchical_topics ( self , docs : List [ int ], linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None ) -> pd . DataFrame : \"\"\" Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using `scipy.cluster.hierarchy.linkage`. Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` Returns: hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) ``` A custom linkage function can be used as follows: ```python from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) # Hierarchical topics linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True) hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function) ``` \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Calculate linkage embeddings = self . c_tf_idf_ [ self . _outliers :] X = distance_function ( embeddings ) Z = linkage_function ( X ) # Calculate basic bag-of-words to be iteratively merged later documents = pd . DataFrame ({ \"Document\" : docs , \"ID\" : range ( len ( docs )), \"Topic\" : self . topics_ }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) documents_per_topic = documents_per_topic . loc [ documents_per_topic . Topic != - 1 , :] documents = self . _preprocess_text ( documents_per_topic . Document . values ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () bow = self . vectorizer_model . transform ( documents ) # Extract clusters hier_topics = pd . DataFrame ( columns = [ \"Parent_ID\" , \"Parent_Name\" , \"Topics\" , \"Child_Left_ID\" , \"Child_Left_Name\" , \"Child_Right_ID\" , \"Child_Right_Name\" ]) for index in tqdm ( range ( len ( Z ))): # Find clustered documents clusters = sch . fcluster ( Z , t = Z [ index ][ 2 ], criterion = 'distance' ) - self . _outliers cluster_df = pd . DataFrame ({ \"Topic\" : range ( len ( clusters )), \"Cluster\" : clusters }) cluster_df = cluster_df . groupby ( \"Cluster\" ) . agg ({ 'Topic' : lambda x : list ( x )}) . reset_index () nr_clusters = len ( clusters ) # Extract first topic we find to get the set of topics in a merged topic topic = None val = Z [ index ][ 0 ] while topic is None : if val - len ( clusters ) < 0 : topic = int ( val ) else : val = Z [ int ( val - len ( clusters ))][ 0 ] clustered_topics = [ i for i , x in enumerate ( clusters ) if x == clusters [ topic ]] # Group bow per cluster, calculate c-TF-IDF and extract words grouped = csr_matrix ( bow [ clustered_topics ] . sum ( axis = 0 )) c_tf_idf = self . ctfidf_model . transform ( grouped ) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels = [ 0 ]) # Extract parent's name and ID parent_id = index + len ( clusters ) parent_name = \"_\" . join ([ x [ 0 ] for x in words_per_topic [ 0 ]][: 5 ]) # Extract child's name and ID Z_id = Z [ index ][ 0 ] child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_left_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_left_name = hier_topics . iloc [ int ( child_left_id )] . Parent_Name # Extract child's name and ID Z_id = Z [ index ][ 1 ] child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_right_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_right_name = hier_topics . iloc [ int ( child_right_id )] . Parent_Name # Save results hier_topics . loc [ len ( hier_topics ), :] = [ parent_id , parent_name , clustered_topics , int ( Z [ index ][ 0 ]), child_left_name , int ( Z [ index ][ 1 ]), child_right_name ] hier_topics [ \"Distance\" ] = Z [:, 2 ] hier_topics = hier_topics . sort_values ( \"Parent_ID\" , ascending = False ) hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] = hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] . astype ( str ) return hier_topics def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings_ ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity def update_topics ( self , docs : List [ str ], topics : List [ int ] = None , n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None , ctfidf_model : ClassTfidfTransformer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: A list of topics where each topic is related to a document in `docs`. Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn ctfidf_model: Pass in your own c-TF-IDF model to update the representations Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, n_gram_range=(2, 3)) ``` You can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, vectorizer_model=vectorizer_model) ``` You can also use this function to change or map the topics to something else. You can update them as follows: ```python topic_model.update_topics(docs, my_updated_topics) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () if topics is None : topics = self . topics_ labels = None else : labels = sorted ( list ( set ( topics ))) warnings . warn ( \"Using a custom list of topic assignments may lead to errors if \" \"topic reduction techniques are used afterwards. Make sure that \" \"manually assigning topics is the last step in the pipeline.\" ) # Extract words documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words , labels = labels ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} self . _update_topic_size ( documents ) def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topic_representations_ def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Examples: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topic_representations_ : return self . topic_representations_ [ topic ] else : return False def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its ID, frequency, and name. Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Examples: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Topic\" , \"Count\" ]) . sort_values ( \"Topic\" ) info [ \"Name\" ] = info . Topic . map ( self . topic_labels_ ) if self . custom_labels_ is not None : if len ( self . custom_labels_ ) == len ( info ): labels = { topic - self . _outliers : label for topic , label in enumerate ( self . custom_labels_ )} info [ \"CustomName\" ] = info [ \"Topic\" ] . map ( labels ) if topic is not None : info = info . loc [ info . Topic == topic , :] return info . reset_index ( drop = True ) def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Examples: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes_ [ topic ] else : return pd . DataFrame ( self . topic_sizes_ . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) def get_representative_docs ( self , topic : int = None ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Examples: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs_ [ topic ] else : return self . representative_docs_ @staticmethod def get_topic_tree ( hier_topics : pd . DataFrame , max_distance : float = None , tight_layout : bool = False ) -> str : \"\"\" Extract the topic tree such that it can be printed Arguments: hier_topics: A dataframe containing the structure of the topic tree. This is the output of `topic_model.hierachical_topics()` max_distance: The maximum distance between two topics. This value is based on the Distance column in `hier_topics`. tight_layout: Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. Returns: A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 The blocks (\u25a0) indicate that the topic is one you can directly access from `topic_model.get_topic`. In other words, they are the original un-grouped topics. Examples: ```python # Train model from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) # Print topic tree tree = topic_model.get_topic_tree(hierarchical_topics) print(tree) ``` \"\"\" width = 1 if tight_layout else 4 if max_distance is None : max_distance = hier_topics . Distance . max () + 1 max_original_topic = hier_topics . Parent_ID . astype ( int ) . min () - 1 # Extract mapping from ID to name topic_to_name = dict ( zip ( hier_topics . Child_Left_ID , hier_topics . Child_Left_Name )) topic_to_name . update ( dict ( zip ( hier_topics . Child_Right_ID , hier_topics . Child_Right_Name ))) topic_to_name = { topic : name [: 100 ] for topic , name in topic_to_name . items ()} # Create tree tree = { str ( row [ 1 ] . Parent_ID ): [ str ( row [ 1 ] . Child_Left_ID ), str ( row [ 1 ] . Child_Right_ID )] for row in hier_topics . iterrows ()} def get_tree ( start , tree ): \"\"\" Based on: https://stackoverflow.com/a/51920869/10532563 \"\"\" def _tree ( to_print , start , parent , tree , grandpa = None , indent = \"\" ): # Get distance between merged topics distance = hier_topics . loc [( hier_topics . Child_Left_ID == parent ) | ( hier_topics . Child_Right_ID == parent ), \"Distance\" ] distance = distance . values [ 0 ] if len ( distance ) > 0 else 10 if parent != start : if grandpa is None : to_print += topic_to_name [ parent ] else : if int ( parent ) <= max_original_topic : # Do not append topic ID if they are not merged if distance < max_distance : to_print += \"\u25a0\u2500\u2500\" + topic_to_name [ parent ] + f \" \u2500\u2500 Topic: { parent } \" + \" \\n \" else : to_print += \"O \\n \" else : to_print += topic_to_name [ parent ] + \" \\n \" if parent not in tree : return to_print for child in tree [ parent ][: - 1 ]: to_print += indent + \"\u251c\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \"\u2502\" + \" \" * width ) child = tree [ parent ][ - 1 ] to_print += indent + \"\u2514\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \" \" * ( width + 1 )) return to_print to_print = \".\" + \" \\n \" to_print = _tree ( to_print , start , start , tree ) return to_print start = str ( hier_topics . Parent_ID . astype ( int ) . max ()) return get_tree ( start , tree ) def set_topic_labels ( self , topic_labels : Union [ List [ str ], Mapping [ int , str ]]) -> None : \"\"\" Set custom topic labels in your fitted BERTopic model Arguments: topic_labels: If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of `topic ID`: `topic_label`, it can have any number of topics as it will only map the topics found in the dictionary. Examples: First, we define our topic labels with `.get_topic_labels` in which we can customize our topic labels: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, topic_prefix=True, word_length=10, separator=\", \") ``` Then, we pass these `topic_labels` to our topic model which can be accessed at any time with `.custom_labels_`: ```python topic_model.set_topic_labels(topic_labels) topic_model.custom_labels_ ``` You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: ```python topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"}) topic_model.custom_labels_ ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) if isinstance ( topic_labels , dict ): if self . custom_labels_ is not None : original_labels = { topic : label for topic , label in zip ( unique_topics , self . custom_labels_ )} else : info = self . get_topic_info () original_labels = dict ( zip ( info . Topic , info . Name )) custom_labels = [ topic_labels . get ( topic ) if topic_labels . get ( topic ) else original_labels [ topic ] for topic in unique_topics ] elif isinstance ( topic_labels , list ): if len ( topic_labels ) == len ( unique_topics ): custom_labels = topic_labels else : raise ValueError ( \"Make sure that `topic_labels` contains the same number \" \"of labels as that there are topics.\" ) self . custom_labels_ = custom_labels def generate_topic_labels ( self , nr_words : int = 3 , topic_prefix : bool = True , word_length : int = None , separator : str = \"_\" ) -> List [ str ]: \"\"\" Get labels for each topic in a user-defined format Arguments: original_labels: nr_words: Top `n` words per topic to use topic_prefix: Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the `separator` word_length: The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. separator: The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is `\", \"`. Returns: topic_labels: A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \") ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) topic_labels = [] for topic in unique_topics : words , _ = zip ( * self . get_topic ( topic )) if word_length : words = [ word [: word_length ] for word in words ][: nr_words ] else : words = list ( words )[: nr_words ] if topic_prefix : topic_label = f \" { topic }{ separator } \" + separator . join ( words ) else : topic_label = separator . join ( words ) topic_labels . append ( topic_label ) return topic_labels def merge_topics ( self , docs : List [ str ], topics_to_merge : List [ Union [ Iterable [ int ], int ]]) -> None : \"\"\" Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics_to_merge: Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. Examples: If you want to merge topics 1, 2, and 3: ```python topics_to_merge = [1, 2, 3] topic_model.merge_topics(docs, topics_to_merge) ``` or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: ```python topics_to_merge = [[1, 2] [3, 4]] topic_model.merge_topics(docs, topics_to_merge) ``` \"\"\" check_is_fitted ( self ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) mapping = { topic : topic for topic in set ( self . topics_ )} if isinstance ( topics_to_merge [ 0 ], int ): for topic in sorted ( topics_to_merge ): mapping [ topic ] = topics_to_merge [ 0 ] elif isinstance ( topics_to_merge [ 0 ], Iterable ): for topic_group in sorted ( topics_to_merge ): for topic in topic_group : mapping [ topic ] = topic_group [ 0 ] else : raise ValueError ( \"Make sure that `topics_to_merge` is either\" \"a list of topics or a list of list of topics.\" ) documents . Topic = documents . Topic . map ( mapping ) self . topic_mapper_ . add_mappings ( mapping ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) self . _map_representative_docs () self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) def reduce_topics ( self , docs : List [ str ], nr_topics : int = 20 ) -> None : \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Updates: topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python topic_model.reduce_topics(docs, nr_topics=30) ``` You can then access the updated topics and probabilities with: ```python topics = topic_model.topics_ probabilities = topic_model.probabilities_ ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) # Reduce number of topics documents = self . _reduce_topics ( documents ) self . _merged_topics = None self . _map_representative_docs () # Map probabilities self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) return self def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height ) def visualize_documents ( self , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_documents ( self , docs = docs , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , custom_labels = custom_labels , width = width , height = height ) def visualize_hierarchical_documents ( self , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchical_documents ( self , docs = docs , hierarchical_topics = hierarchical_topics , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , nr_levels = nr_levels , custom_labels = custom_labels , width = width , height = height ) def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , custom_labels = custom_labels , width = width , height = height ) def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height ) def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height ) def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(topic_model.probabilities_[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(topic_model.probabilities_[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , custom_labels = custom_labels , width = width , height = height ) def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , custom_labels = custom_labels , width = width , height = height , hierarchical_topics = hierarchical_topics , linkage_function = linkage_function , distance_function = distance_function , color_threshold = color_threshold ) def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , custom_labels = custom_labels , width = width , height = height ) def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , custom_labels = custom_labels , title = title , width = width , height = height ) def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Examples: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : # This prevents the vectorizer from being too large in size if `min_df` was # set to a value higher than 1 self . vectorizer_model . stop_words_ = None if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file ) @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Examples: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out def _extract_embeddings ( self , documents : Union [ List [ str ], str ], method : str = \"document\" , verbose : bool = None ) -> np . ndarray : \"\"\" Extract sentence/document embeddings through pre-trained embeddings For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html Arguments: documents: Dataframe with documents and their corresponding IDs method: Whether to extract document or word-embeddings, options are \"document\" and \"word\" verbose: Whether to show a progressbar demonstrating the time to extract embeddings Returns: embeddings: The extracted embeddings. \"\"\" if isinstance ( documents , str ): documents = [ documents ] if method == \"word\" : embeddings = self . embedding_model . embed_words ( documents , verbose ) elif method == \"document\" : embeddings = self . embedding_model . embed_documents ( documents , verbose ) else : raise ValueError ( \"Wrong method for extracting document/word embeddings. \" \"Either choose 'word' or 'document' as the method. \" ) return embeddings def _map_predictions ( self , predictions : List [ int ]) -> List [ int ]: \"\"\" Map predictions to the correct topics if topics were reduced \"\"\" mappings = self . topic_mapper_ . get_mappings ( original_topics = True ) mapped_predictions = [ mappings [ prediction ] if prediction in mappings else - 1 for prediction in predictions ] return mapped_predictions def _reduce_dimensionality ( self , embeddings : Union [ np . ndarray , csr_matrix ], y : Union [ List [ int ], np . ndarray ] = None , partial_fit : bool = False ) -> np . ndarray : \"\"\" Reduce dimensionality of embeddings using UMAP and train a UMAP model Arguments: embeddings: The extracted embeddings using the sentence transformer module. y: The target class for (semi)-supervised dimensionality reduction partial_fit: Whether to run `partial_fit` for online learning Returns: umap_embeddings: The reduced embeddings \"\"\" # Partial fit if partial_fit : if hasattr ( self . umap_model , \"partial_fit\" ): self . umap_model = self . umap_model . partial_fit ( embeddings ) elif self . topic_representations_ is None : self . umap_model . fit ( embeddings ) # Regular fit else : try : self . umap_model . fit ( embeddings , y = y ) except TypeError : logger . info ( \"The dimensionality reduction algorithm did not contain the `y` parameter and\" \" therefore the `y` parameter was not used\" ) self . umap_model . fit ( embeddings ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) return np . nan_to_num ( umap_embeddings ) def _cluster_embeddings ( self , umap_embeddings : np . ndarray , documents : pd . DataFrame , partial_fit : bool = False ) -> Tuple [ pd . DataFrame , np . ndarray ]: \"\"\" Cluster UMAP embeddings with HDBSCAN Arguments: umap_embeddings: The reduced sentence embeddings with UMAP documents: Dataframe with documents and their corresponding IDs partial_fit: Whether to run `partial_fit` for online learning Returns: documents: Updated dataframe with documents and their corresponding IDs and newly added Topics probabilities: The distribution of probabilities \"\"\" if partial_fit : self . hdbscan_model = self . hdbscan_model . partial_fit ( umap_embeddings ) labels = self . hdbscan_model . labels_ documents [ 'Topic' ] = labels self . topics_ = labels else : self . hdbscan_model . fit ( umap_embeddings ) labels = self . hdbscan_model . labels_ documents [ 'Topic' ] = labels self . _update_topic_size ( documents ) # Some algorithms have outlier labels (-1) that can be tricky to work # with if you are slicing data based on that labels. Therefore, we # track if there are outlier labels and act accordingly when slicing. self . _outliers = 1 if - 1 in set ( labels ) else 0 # Save representative docs and calculate probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): probabilities = self . hdbscan_model . probabilities_ self . _save_representative_docs ( documents ) if self . calculate_probabilities : probabilities = hdbscan . all_points_membership_vectors ( self . hdbscan_model ) else : probabilities = None if not partial_fit : self . topic_mapper_ = TopicMapper ( self . topics_ ) logger . info ( \"Clustered reduced embeddings\" ) return documents , probabilities def _guided_topic_modeling ( self , embeddings : np . ndarray ) -> Tuple [ List [ int ], np . array ]: \"\"\" Apply Guided Topic Modeling We transform the seeded topics to embeddings using the same embedder as used for generating document embeddings. Then, we apply cosine similarity between the embeddings and set labels for documents that are more similar to one of the topics, then the average document. If a document is more similar to the average document than any of the topics, it gets the -1 label and is thereby not included in UMAP. Arguments: embeddings: The document embeddings Returns y: The labels for each seeded topic embeddings: Updated embeddings \"\"\" # Create embeddings from the seeded topics seed_topic_list = [ \" \" . join ( seed_topic ) for seed_topic in self . seed_topic_list ] seed_topic_embeddings = self . _extract_embeddings ( seed_topic_list , verbose = self . verbose ) seed_topic_embeddings = np . vstack ([ seed_topic_embeddings , embeddings . mean ( axis = 0 )]) # Label documents that are most similar to one of the seeded topics sim_matrix = cosine_similarity ( embeddings , seed_topic_embeddings ) y = [ np . argmax ( sim_matrix [ index ]) for index in range ( sim_matrix . shape [ 0 ])] y = [ val if val != len ( seed_topic_list ) else - 1 for val in y ] # Average the document embeddings related to the seeded topics with the # embedding of the seeded topic to force the documents in a cluster for seed_topic in range ( len ( seed_topic_list )): indices = [ index for index , topic in enumerate ( y ) if topic == seed_topic ] embeddings [ indices ] = np . average ([ embeddings [ indices ], seed_topic_embeddings [ seed_topic ]], weights = [ 3 , 1 ]) return y , embeddings def _extract_topics ( self , documents : pd . DataFrame ): \"\"\" Extract topics from the clusters using a class-based TF-IDF Arguments: documents: Dataframe with documents and their corresponding IDs Returns: c_tf_idf: The resulting matrix giving a value (importance score) for each word per topic \"\"\" documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} def _save_representative_docs ( self , documents : pd . DataFrame ): \"\"\" Save the most representative docs (3) per topic The most representative docs are extracted by taking the exemplars from the HDBSCAN-generated clusters. Full instructions can be found here: https://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html Arguments: documents: Dataframe with documents and their corresponding IDs \"\"\" # Prepare the condensed tree and luf clusters beneath a given cluster condensed_tree = self . hdbscan_model . condensed_tree_ raw_tree = condensed_tree . _raw_tree clusters = sorted ( condensed_tree . _select_clusters ()) cluster_tree = raw_tree [ raw_tree [ 'child_size' ] > 1 ] # Find the points with maximum lambda value in each leaf representative_docs = {} for topic in documents [ 'Topic' ] . unique (): if topic != - 1 : leaves = hdbscan . plots . _recurse_leaf_dfs ( cluster_tree , clusters [ topic ]) result = np . array ([]) for leaf in leaves : max_lambda = raw_tree [ 'lambda_val' ][ raw_tree [ 'parent' ] == leaf ] . max () points = raw_tree [ 'child' ][( raw_tree [ 'parent' ] == leaf ) & ( raw_tree [ 'lambda_val' ] == max_lambda )] result = np . hstack (( result , points )) representative_docs [ topic ] = list ( np . random . choice ( result , 3 , replace = False ) . astype ( int )) # Convert indices to documents self . representative_docs_ = { topic : [ documents . iloc [ doc_id ] . Document for doc_id in doc_ids ] for topic , doc_ids in representative_docs . items ()} def _map_representative_docs ( self , original_topics : bool = False ): \"\"\" Map the representative docs per topic to the correct topics If topics were reduced, remove documents from topics that were merged into larger topics as we assume that the documents from larger topics are better representative of the entire merged topic. Arguments: original_topics: Whether we want to map from the original topics to the most recent topics or from the second-most recent topics. \"\"\" if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): mappings = self . topic_mapper_ . get_mappings ( original_topics ) representative_docs = self . representative_docs_ . copy () # Update the representative documents updated_representative_docs = { mappings [ old_topic ]: [] for old_topic , _ in representative_docs . items ()} for old_topic , docs in representative_docs . items (): new_topic = mappings [ old_topic ] updated_representative_docs [ new_topic ] . extend ( docs ) self . representative_docs_ = updated_representative_docs self . representative_docs_ . pop ( - 1 , None ) def _create_topic_vectors ( self ): \"\"\" Creates embeddings per topics based on their topic representation We start by creating embeddings out of the topic representation. This results in a number of embeddings per topic. Then, we take the weighted average of embeddings in a topic by their c-TF-IDF score. This will put more emphasis to words that represent a topic best. Only allow topic vectors to be created if there are no custom embeddings and therefore a sentence-transformer model to be used or there are custom embeddings but it is allowed to use a different multi-lingual sentence-transformer model \"\"\" if self . embedding_model is not None : topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () n = self . top_n_words # Extract embeddings for all words in all topics topic_words = [ self . get_topic ( topic ) for topic in topic_list ] topic_words = [ word [ 0 ] for topic in topic_words for word in topic ] embeddings = self . _extract_embeddings ( topic_words , method = \"word\" , verbose = False ) # Take the weighted average of word embeddings in a topic based on their c-TF-IDF value # The embeddings var is a single numpy matrix and therefore slicing is necessary to # access the words per topic topic_embeddings = [] for i , topic in enumerate ( topic_list ): word_importance = [ val [ 1 ] for val in self . get_topic ( topic )] if sum ( word_importance ) == 0 : word_importance = [ 1 for _ in range ( len ( self . get_topic ( topic )))] topic_embedding = np . average ( embeddings [ i * n : n + ( i * n )], weights = word_importance , axis = 0 ) topic_embeddings . append ( topic_embedding ) self . topic_embeddings_ = topic_embeddings def _c_tf_idf ( self , documents_per_topic : pd . DataFrame , fit : bool = True , partial_fit : bool = False ) -> Tuple [ csr_matrix , List [ str ]]: \"\"\" Calculate a class-based TF-IDF where m is the number of total documents. Arguments: documents_per_topic: The joined documents per topic such that each topic has a single string made out of multiple documents m: The total number of documents (unjoined) fit: Whether to fit a new vectorizer or use the fitted self.vectorizer_model partial_fit: Whether to run `partial_fit` for online learning Returns: tf_idf: The resulting matrix giving a value (importance score) for each word per topic words: The names of the words to which values were given \"\"\" documents = self . _preprocess_text ( documents_per_topic . Document . values ) if partial_fit : X = self . vectorizer_model . partial_fit ( documents ) . update_bow ( documents ) elif fit : self . vectorizer_model . fit ( documents ) X = self . vectorizer_model . transform ( documents ) else : X = self . vectorizer_model . transform ( documents ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () if self . seed_topic_list : seed_topic_list = [ seed for seeds in self . seed_topic_list for seed in seeds ] multiplier = np . array ([ 1.2 if word in seed_topic_list else 1 for word in words ]) else : multiplier = None if fit : self . ctfidf_model = self . ctfidf_model . fit ( X , multiplier = multiplier ) c_tf_idf = self . ctfidf_model . transform ( X ) return c_tf_idf , words def _update_topic_size ( self , documents : pd . DataFrame ): \"\"\" Calculate the topic sizes Arguments: documents: Updated dataframe with documents and their corresponding IDs and newly added Topics \"\"\" sizes = documents . groupby ([ 'Topic' ]) . count () . sort_values ( \"Document\" , ascending = False ) . reset_index () self . topic_sizes_ = dict ( zip ( sizes . Topic , sizes . Document )) self . topics_ = documents . Topic . astype ( int ) . tolist () def _extract_words_per_topic ( self , words : List [ str ], c_tf_idf : csr_matrix = None , labels : List [ int ] = None ) -> Mapping [ str , List [ Tuple [ str , float ]]]: \"\"\" Based on tf_idf scores per topic, extract the top n words per topic If the top words per topic need to be extracted, then only the `words` parameter needs to be passed. If the top words per topic in a specific timestamp, then it is important to pass the timestamp-based c-TF-IDF matrix and its corresponding labels. Arguments: words: List of all words (sorted according to tf_idf matrix position) c_tf_idf: A c-TF-IDF matrix from which to calculate the top words labels: A list of topic labels Returns: topics: The top words per topic \"\"\" if c_tf_idf is None : c_tf_idf = self . c_tf_idf_ if labels is None : labels = sorted ( list ( self . topic_sizes_ . keys ())) # Get the top 30 indices and values per row in a sparse c-TF-IDF matrix indices = self . _top_n_idx_sparse ( c_tf_idf , 30 ) scores = self . _top_n_values_sparse ( c_tf_idf , indices ) sorted_indices = np . argsort ( scores , 1 ) indices = np . take_along_axis ( indices , sorted_indices , axis = 1 ) scores = np . take_along_axis ( scores , sorted_indices , axis = 1 ) # Get top 30 words per topic based on c-TF-IDF score topics = { label : [( words [ word_index ], score ) if word_index is not None and score > 0 else ( \"\" , 0.00001 ) for word_index , score in zip ( indices [ index ][:: - 1 ], scores [ index ][:: - 1 ]) ] for index , label in enumerate ( labels )} # Extract word embeddings for the top 30 words per topic and compare it # with the topic embedding to keep only the words most similar to the topic embedding if self . diversity is not None : if self . embedding_model is not None : for topic , topic_words in topics . items (): words = [ word [ 0 ] for word in topic_words ] word_embeddings = self . _extract_embeddings ( words , method = \"word\" , verbose = False ) topic_embedding = self . _extract_embeddings ( \" \" . join ( words ), method = \"word\" , verbose = False ) . reshape ( 1 , - 1 ) topic_words = mmr ( topic_embedding , word_embeddings , words , top_n = self . top_n_words , diversity = self . diversity ) topics [ topic ] = [( word , value ) for word , value in topics [ topic ] if word in topic_words ] topics = { label : values [: self . top_n_words ] for label , values in topics . items ()} return topics def _reduce_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce topics to self.nr_topics Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" initial_nr_topics = len ( self . get_topics ()) if isinstance ( self . nr_topics , int ): if self . nr_topics < initial_nr_topics : documents = self . _reduce_to_n_topics ( documents ) elif isinstance ( self . nr_topics , str ): documents = self . _auto_reduce_topics ( documents ) else : raise ValueError ( \"nr_topics needs to be an int or 'auto'! \" ) logger . info ( f \"Reduced number of topics from { initial_nr_topics } to { len ( self . get_topic_freq ()) } \" ) return documents def _reduce_to_n_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce topics to self.nr_topics Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" # Track which topics where originally merged if not self . _merged_topics : self . _merged_topics = [] # Create topic similarity matrix similarities = cosine_similarity ( self . c_tf_idf_ ) np . fill_diagonal ( similarities , 0 ) # Find most similar topic to least common topic topics = documents . Topic . tolist () . copy () mapped_topics = {} while len ( self . get_topic_freq ()) > self . nr_topics + self . _outliers : topic_to_merge = self . get_topic_freq () . iloc [ - 1 ] . Topic topic_to_merge_into = np . argmax ( similarities [ topic_to_merge + self . _outliers ]) - self . _outliers similarities [:, topic_to_merge + self . _outliers ] = - self . _outliers self . _merged_topics . append ( topic_to_merge ) # Update Topic labels documents . loc [ documents . Topic == topic_to_merge , \"Topic\" ] = topic_to_merge_into mapped_topics [ topic_to_merge ] = topic_to_merge_into self . _update_topic_size ( documents ) # Map topics mapped_topics = { from_topic : to_topic for from_topic , to_topic in zip ( topics , documents . Topic . tolist ())} self . topic_mapper_ . add_mappings ( mapped_topics ) # Update representations documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) return documents def _auto_reduce_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce the number of topics automatically using HDBSCAN Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" topics = documents . Topic . tolist () . copy () unique_topics = sorted ( list ( documents . Topic . unique ()))[ self . _outliers :] max_topic = unique_topics [ - 1 ] # Find similar topics if self . topic_embeddings_ is not None : embeddings = np . array ( self . topic_embeddings_ ) else : embeddings = self . c_tf_idf_ . toarray () norm_data = normalize ( embeddings , norm = 'l2' ) predictions = hdbscan . HDBSCAN ( min_cluster_size = 2 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) . fit_predict ( norm_data [ self . _outliers :]) # Map similar topics mapped_topics = { unique_topics [ index ]: prediction + max_topic for index , prediction in enumerate ( predictions ) if prediction != - 1 } documents . Topic = documents . Topic . map ( mapped_topics ) . fillna ( documents . Topic ) . astype ( int ) mapped_topics = { from_topic : to_topic for from_topic , to_topic in zip ( topics , documents . Topic . tolist ())} # Update documents and topics self . topic_mapper_ . add_mappings ( mapped_topics ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) return documents def _sort_mappings_by_frequency ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reorder mappings by their frequency. For example, if topic 88 was mapped to topic 5 and topic 5 turns out to be the largest topic, then topic 5 will be topic 0. The second largest, will be topic 1, etc. If there are no mappings since no reduction of topics took place, then the topics will simply be ordered by their frequency and will get the topic ids based on that order. This means that -1 will remain the outlier class, and that the rest of the topics will be in descending order of ids and frequency. Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the mapped and re-ordered topic ids \"\"\" self . _update_topic_size ( documents ) # Map topics based on frequency df = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Old_Topic\" , \"Size\" ]) . sort_values ( \"Size\" , ascending = False ) df = df [ df . Old_Topic != - 1 ] sorted_topics = { ** { - 1 : - 1 }, ** dict ( zip ( df . Old_Topic , range ( len ( df ))))} self . topic_mapper_ . add_mappings ( sorted_topics ) # Map documents documents . Topic = documents . Topic . map ( sorted_topics ) . fillna ( documents . Topic ) . astype ( int ) self . _update_topic_size ( documents ) return documents def _map_probabilities ( self , probabilities : Union [ np . ndarray , None ], original_topics : bool = False ) -> Union [ np . ndarray , None ]: \"\"\" Map the probabilities to the reduced topics. This is achieved by adding the probabilities together of all topics that were mapped to the same topic. Then, the topics that were mapped from were set to 0 as they were reduced. Arguments: probabilities: An array containing probabilities original_topics: Whether we want to map from the original topics to the most recent topics or from the second-most recent topics. Returns: mapped_probabilities: Updated probabilities \"\"\" mappings = self . topic_mapper_ . get_mappings ( original_topics ) # Map array of probabilities (probability for assigned topic per document) if probabilities is not None : if len ( probabilities . shape ) == 2 and self . get_topic ( - 1 ): mapped_probabilities = np . zeros (( probabilities . shape [ 0 ], len ( set ( mappings . values ())) - 1 )) for from_topic , to_topic in mappings . items (): if to_topic != - 1 and from_topic != - 1 : mapped_probabilities [:, to_topic ] += probabilities [:, from_topic ] return mapped_probabilities return probabilities def _preprocess_text ( self , documents : np . ndarray ) -> List [ str ]: \"\"\" Basic preprocessing of text Steps: * Replace \\n and \\t with whitespace * Only keep alpha-numerical characters \"\"\" cleaned_documents = [ doc . replace ( \" \\n \" , \" \" ) for doc in documents ] cleaned_documents = [ doc . replace ( \" \\t \" , \" \" ) for doc in cleaned_documents ] if self . language == \"english\" : cleaned_documents = [ re . sub ( r '[^A-Za-z0-9 ]+' , '' , doc ) for doc in cleaned_documents ] cleaned_documents = [ doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents ] return cleaned_documents @staticmethod def _top_n_idx_sparse ( matrix : csr_matrix , n : int ) -> np . ndarray : \"\"\" Return indices of top n values in each row of a sparse matrix Retrieved from: https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix Arguments: matrix: The sparse matrix from which to get the top n indices per row n: The number of highest values to extract from each row Returns: indices: The top n indices per row \"\"\" indices = [] for le , ri in zip ( matrix . indptr [: - 1 ], matrix . indptr [ 1 :]): n_row_pick = min ( n , ri - le ) values = matrix . indices [ le + np . argpartition ( matrix . data [ le : ri ], - n_row_pick )[ - n_row_pick :]] values = [ values [ index ] if len ( values ) >= index + 1 else None for index in range ( n )] indices . append ( values ) return np . array ( indices ) @staticmethod def _top_n_values_sparse ( matrix : csr_matrix , indices : np . ndarray ) -> np . ndarray : \"\"\" Return the top n values for each row in a sparse matrix Arguments: matrix: The sparse matrix from which to get the top n indices per row indices: The top n indices per row Returns: top_values: The top n scores per row \"\"\" top_values = [] for row , values in enumerate ( indices ): scores = np . array ([ matrix [ row , value ] if value is not None else 0 for value in values ]) top_values . append ( scores ) return np . array ( top_values ) @classmethod def _get_param_names ( cls ): \"\"\"Get parameter names for the estimator Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 \"\"\" init_signature = inspect . signature ( cls . __init__ ) parameters = sorted ([ p . name for p in init_signature . parameters . values () if p . name != 'self' and p . kind != p . VAR_KEYWORD ]) return parameters def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\" __init__ ( language = 'english' , top_n_words = 10 , n_gram_range = ( 1 , 1 ), min_topic_size = 10 , nr_topics = None , low_memory = False , calculate_probabilities = False , diversity = None , seed_topic_list = None , embedding_model = None , umap_model = None , hdbscan_model = None , vectorizer_model = None , ctfidf_model = None , verbose = False ) \u00b6 BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. The default sentence-transformers model for \"english\" is all-MiniLM-L6-v2 . For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the paraphrase-multilingual-MiniLM-L12-v2 sentence-tranformers model that supports 50+ languages. 'english' top_n_words int The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. 10 n_gram_range Tuple [ int , int ] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. 10 nr_topics Union [ int , str ] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. None low_memory bool Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. False calculate_probabilities bool Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method visualize_probabilities . False diversity float Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. None seed_topic_list List [ List [ str ]] A list of seed words per topic to converge around None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False embedding_model Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html None umap_model UMAP Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has .fit and .transform functions. None hdbscan_model hdbscan . HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has .fit and .predict functions along with the .labels_ variable. None vectorizer_model CountVectorizer Pass in a custom CountVectorizer instead of the default model. None ctfidf_model TfidfTransformer Pass in a custom ClassTfidfTransformer instead of the default model. None Source code in bertopic\\_bertopic.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , diversity : float = None , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , ctfidf_model : TfidfTransformer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. The default sentence-transformers model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2` sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. low_memory: Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. diversity: Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has `.fit` and `.transform` functions. hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has `.fit` and `.predict` functions along with the `.labels_` variable. vectorizer_model: Pass in a custom `CountVectorizer` instead of the default model. ctfidf_model: Pass in a custom ClassTfidfTransformer instead of the default model. \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . diversity = diversity self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () # UMAP or another algorithm that has .fit and .transform functions self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN or another clustering algorithm that has .fit and .predict functions and # the .labels_ variable to extract the labels self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Public attributes self . topics_ = None self . probabilities_ = None self . topic_sizes_ = None self . topic_mapper_ = None self . topic_representations_ = None self . topic_embeddings_ = None self . topic_labels_ = None self . custom_labels_ = None self . representative_docs_ = None self . c_tf_idf_ = None # Private attributes for internal tracking purposes self . _outliers = 1 self . _merged_topics = None if verbose : logger . set_level ( \"DEBUG\" ) __str__ () \u00b6 Get a string representation of the current object. Returns: Name Type Description str Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. Source code in bertopic\\_bertopic.py 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\" find_topics ( search_term , top_n = 5 ) \u00b6 Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Name Type Description similar_topics List [ int ] the most similar topics from high to low similarity List [ float ] the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: topics , similarity = topic_model . find_topics ( \"sports\" , top_n = 5 ) Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. Source code in bertopic\\_bertopic.py 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings_ ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity fit ( documents , embeddings = None , y = None ) \u00b6 Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self fit_transform ( documents , embeddings = None , y = None ) \u00b6 Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Returns: Name Type Description predictions List [ int ] Topic predictions for each documents probabilities Union [ np . ndarray , None] The probability of the assigned topic per document. If calculate_probabilities in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster reduced embeddings documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs ( original_topics = True ) self . probabilities_ = self . _map_probabilities ( probabilities , original_topics = True ) predictions = documents . Topic . to_list () return predictions , self . probabilities_ generate_topic_labels ( nr_words = 3 , topic_prefix = True , word_length = None , separator = '_' ) \u00b6 Get labels for each topic in a user-defined format Parameters: Name Type Description Default original_labels required nr_words int Top n words per topic to use 3 topic_prefix bool Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the separator True word_length int The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. None separator str The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is \", \" . '_' Returns: Name Type Description topic_labels List [ str ] A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: topic_labels = topic_model . get_topic_labels ( nr_words = 2 , separator = \", \" ) Source code in bertopic\\_bertopic.py 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 def generate_topic_labels ( self , nr_words : int = 3 , topic_prefix : bool = True , word_length : int = None , separator : str = \"_\" ) -> List [ str ]: \"\"\" Get labels for each topic in a user-defined format Arguments: original_labels: nr_words: Top `n` words per topic to use topic_prefix: Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the `separator` word_length: The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. separator: The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is `\", \"`. Returns: topic_labels: A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \") ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) topic_labels = [] for topic in unique_topics : words , _ = zip ( * self . get_topic ( topic )) if word_length : words = [ word [: word_length ] for word in words ][: nr_words ] else : words = list ( words )[: nr_words ] if topic_prefix : topic_label = f \" { topic }{ separator } \" + separator . join ( words ) else : topic_label = separator . join ( words ) topic_labels . append ( topic_label ) return topic_labels get_params ( deep = False ) \u00b6 Get parameters for this estimator. Adapted from https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Parameters: Name Type Description Default deep bool bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. False Returns: Name Type Description out Mapping [ str , Any ] Parameter names mapped to their values. Source code in bertopic\\_bertopic.py 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out get_representative_docs ( topic = None ) \u00b6 Extract representative documents per topic Parameters: Name Type Description Default topic int A specific topic for which you want the representative documents None Returns: Type Description List [ str ] Representative documents of the chosen topic Examples: To extract the representative docs of all topics: representative_docs = topic_model . get_representative_docs () To get the representative docs of a single topic: representative_docs = topic_model . get_representative_docs ( 12 ) Source code in bertopic\\_bertopic.py 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 def get_representative_docs ( self , topic : int = None ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Examples: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs_ [ topic ] else : return self . representative_docs_ get_topic ( topic ) \u00b6 Return top n words for a specific topic and their c-TF-IDF scores Parameters: Name Type Description Default topic int A specific topic for which you want its representation required Returns: Type Description Union [ Mapping [ str , Tuple [ str , float ]], bool ] The top n words for a specific word and its respective c-TF-IDF scores Examples: topic = topic_model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Examples: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topic_representations_ : return self . topic_representations_ [ topic ] else : return False get_topic_freq ( topic = None ) \u00b6 Return the the size of topics (descending order) Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description Union [ pd . DataFrame , int ] Either the frequency of a single topic or dataframe with Union [ pd . DataFrame , int ] the frequencies of all topics Examples: To extract the frequency of all topics: frequency = topic_model . get_topic_freq () To get the frequency of a single topic: frequency = topic_model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Examples: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes_ [ topic ] else : return pd . DataFrame ( self . topic_sizes_ . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) get_topic_info ( topic = None ) \u00b6 Get information about each topic including its ID, frequency, and name. Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Name Type Description info pd . DataFrame The information relating to either a single topic or all topics Examples: info_df = topic_model . get_topic_info () Source code in bertopic\\_bertopic.py 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its ID, frequency, and name. Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Examples: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Topic\" , \"Count\" ]) . sort_values ( \"Topic\" ) info [ \"Name\" ] = info . Topic . map ( self . topic_labels_ ) if self . custom_labels_ is not None : if len ( self . custom_labels_ ) == len ( info ): labels = { topic - self . _outliers : label for topic , label in enumerate ( self . custom_labels_ )} info [ \"CustomName\" ] = info [ \"Topic\" ] . map ( labels ) if topic is not None : info = info . loc [ info . Topic == topic , :] return info . reset_index ( drop = True ) get_topic_tree ( hier_topics , max_distance = None , tight_layout = False ) staticmethod \u00b6 Extract the topic tree such that it can be printed Parameters: Name Type Description Default hier_topics pd . DataFrame A dataframe containing the structure of the topic tree. This is the output of topic_model.hierachical_topics() required max_distance float The maximum distance between two topics. This value is based on the Distance column in hier_topics . None tight_layout bool Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. False Returns: Type Description str A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 str The blocks (\u25a0) indicate that the topic is one you can directly access str from topic_model.get_topic . In other words, they are the original un-grouped topics. Examples: # Train model from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Print topic tree tree = topic_model . get_topic_tree ( hierarchical_topics ) print ( tree ) Source code in bertopic\\_bertopic.py 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 @staticmethod def get_topic_tree ( hier_topics : pd . DataFrame , max_distance : float = None , tight_layout : bool = False ) -> str : \"\"\" Extract the topic tree such that it can be printed Arguments: hier_topics: A dataframe containing the structure of the topic tree. This is the output of `topic_model.hierachical_topics()` max_distance: The maximum distance between two topics. This value is based on the Distance column in `hier_topics`. tight_layout: Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. Returns: A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 The blocks (\u25a0) indicate that the topic is one you can directly access from `topic_model.get_topic`. In other words, they are the original un-grouped topics. Examples: ```python # Train model from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) # Print topic tree tree = topic_model.get_topic_tree(hierarchical_topics) print(tree) ``` \"\"\" width = 1 if tight_layout else 4 if max_distance is None : max_distance = hier_topics . Distance . max () + 1 max_original_topic = hier_topics . Parent_ID . astype ( int ) . min () - 1 # Extract mapping from ID to name topic_to_name = dict ( zip ( hier_topics . Child_Left_ID , hier_topics . Child_Left_Name )) topic_to_name . update ( dict ( zip ( hier_topics . Child_Right_ID , hier_topics . Child_Right_Name ))) topic_to_name = { topic : name [: 100 ] for topic , name in topic_to_name . items ()} # Create tree tree = { str ( row [ 1 ] . Parent_ID ): [ str ( row [ 1 ] . Child_Left_ID ), str ( row [ 1 ] . Child_Right_ID )] for row in hier_topics . iterrows ()} def get_tree ( start , tree ): \"\"\" Based on: https://stackoverflow.com/a/51920869/10532563 \"\"\" def _tree ( to_print , start , parent , tree , grandpa = None , indent = \"\" ): # Get distance between merged topics distance = hier_topics . loc [( hier_topics . Child_Left_ID == parent ) | ( hier_topics . Child_Right_ID == parent ), \"Distance\" ] distance = distance . values [ 0 ] if len ( distance ) > 0 else 10 if parent != start : if grandpa is None : to_print += topic_to_name [ parent ] else : if int ( parent ) <= max_original_topic : # Do not append topic ID if they are not merged if distance < max_distance : to_print += \"\u25a0\u2500\u2500\" + topic_to_name [ parent ] + f \" \u2500\u2500 Topic: { parent } \" + \" \\n \" else : to_print += \"O \\n \" else : to_print += topic_to_name [ parent ] + \" \\n \" if parent not in tree : return to_print for child in tree [ parent ][: - 1 ]: to_print += indent + \"\u251c\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \"\u2502\" + \" \" * width ) child = tree [ parent ][ - 1 ] to_print += indent + \"\u2514\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \" \" * ( width + 1 )) return to_print to_print = \".\" + \" \\n \" to_print = _tree ( to_print , start , start , tree ) return to_print start = str ( hier_topics . Parent_ID . astype ( int ) . max ()) return get_tree ( start , tree ) get_topics () \u00b6 Return topics with top n words and their c-TF-IDF score Returns: Type Description Mapping [ str , Tuple [ str , float ]] self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: all_topics = topic_model . get_topics () Source code in bertopic\\_bertopic.py 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topic_representations_ hierarchical_topics ( docs , linkage_function = None , distance_function = None ) \u00b6 Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using scipy.cluster.hierarchy.linkage . Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Parameters: Name Type Description Default docs List [ int ] The documents you used when calling either fit or fit_transform required linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) None Returns: Name Type Description hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) A custom linkage function can be used as follows: from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Hierarchical topics linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) hierarchical_topics = topic_model . hierarchical_topics ( docs , linkage_function = linkage_function ) Source code in bertopic\\_bertopic.py 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 def hierarchical_topics ( self , docs : List [ int ], linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None ) -> pd . DataFrame : \"\"\" Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using `scipy.cluster.hierarchy.linkage`. Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` Returns: hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) ``` A custom linkage function can be used as follows: ```python from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) # Hierarchical topics linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True) hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function) ``` \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Calculate linkage embeddings = self . c_tf_idf_ [ self . _outliers :] X = distance_function ( embeddings ) Z = linkage_function ( X ) # Calculate basic bag-of-words to be iteratively merged later documents = pd . DataFrame ({ \"Document\" : docs , \"ID\" : range ( len ( docs )), \"Topic\" : self . topics_ }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) documents_per_topic = documents_per_topic . loc [ documents_per_topic . Topic != - 1 , :] documents = self . _preprocess_text ( documents_per_topic . Document . values ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () bow = self . vectorizer_model . transform ( documents ) # Extract clusters hier_topics = pd . DataFrame ( columns = [ \"Parent_ID\" , \"Parent_Name\" , \"Topics\" , \"Child_Left_ID\" , \"Child_Left_Name\" , \"Child_Right_ID\" , \"Child_Right_Name\" ]) for index in tqdm ( range ( len ( Z ))): # Find clustered documents clusters = sch . fcluster ( Z , t = Z [ index ][ 2 ], criterion = 'distance' ) - self . _outliers cluster_df = pd . DataFrame ({ \"Topic\" : range ( len ( clusters )), \"Cluster\" : clusters }) cluster_df = cluster_df . groupby ( \"Cluster\" ) . agg ({ 'Topic' : lambda x : list ( x )}) . reset_index () nr_clusters = len ( clusters ) # Extract first topic we find to get the set of topics in a merged topic topic = None val = Z [ index ][ 0 ] while topic is None : if val - len ( clusters ) < 0 : topic = int ( val ) else : val = Z [ int ( val - len ( clusters ))][ 0 ] clustered_topics = [ i for i , x in enumerate ( clusters ) if x == clusters [ topic ]] # Group bow per cluster, calculate c-TF-IDF and extract words grouped = csr_matrix ( bow [ clustered_topics ] . sum ( axis = 0 )) c_tf_idf = self . ctfidf_model . transform ( grouped ) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels = [ 0 ]) # Extract parent's name and ID parent_id = index + len ( clusters ) parent_name = \"_\" . join ([ x [ 0 ] for x in words_per_topic [ 0 ]][: 5 ]) # Extract child's name and ID Z_id = Z [ index ][ 0 ] child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_left_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_left_name = hier_topics . iloc [ int ( child_left_id )] . Parent_Name # Extract child's name and ID Z_id = Z [ index ][ 1 ] child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_right_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_right_name = hier_topics . iloc [ int ( child_right_id )] . Parent_Name # Save results hier_topics . loc [ len ( hier_topics ), :] = [ parent_id , parent_name , clustered_topics , int ( Z [ index ][ 0 ]), child_left_name , int ( Z [ index ][ 1 ]), child_right_name ] hier_topics [ \"Distance\" ] = Z [:, 2 ] hier_topics = hier_topics . sort_values ( \"Parent_ID\" , ascending = False ) hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] = hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] . astype ( str ) return hier_topics load ( path , embedding_model = None ) classmethod \u00b6 Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required embedding_model If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. None Examples: BERTopic . load ( \"my_model\" ) or if you did not save the embedding model: BERTopic . load ( \"my_model\" , embedding_model = \"all-MiniLM-L6-v2\" ) Source code in bertopic\\_bertopic.py 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Examples: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model merge_topics ( docs , topics_to_merge ) \u00b6 Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required topics_to_merge List [ Union [ Iterable [ int ], int ]] Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. required Examples: If you want to merge topics 1, 2, and 3: topics_to_merge = [ 1 , 2 , 3 ] topic_model . merge_topics ( docs , topics_to_merge ) or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: topics_to_merge = [[ 1 , 2 ] [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge ) Source code in bertopic\\_bertopic.py 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 def merge_topics ( self , docs : List [ str ], topics_to_merge : List [ Union [ Iterable [ int ], int ]]) -> None : \"\"\" Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics_to_merge: Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. Examples: If you want to merge topics 1, 2, and 3: ```python topics_to_merge = [1, 2, 3] topic_model.merge_topics(docs, topics_to_merge) ``` or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: ```python topics_to_merge = [[1, 2] [3, 4]] topic_model.merge_topics(docs, topics_to_merge) ``` \"\"\" check_is_fitted ( self ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) mapping = { topic : topic for topic in set ( self . topics_ )} if isinstance ( topics_to_merge [ 0 ], int ): for topic in sorted ( topics_to_merge ): mapping [ topic ] = topics_to_merge [ 0 ] elif isinstance ( topics_to_merge [ 0 ], Iterable ): for topic_group in sorted ( topics_to_merge ): for topic in topic_group : mapping [ topic ] = topic_group [ 0 ] else : raise ValueError ( \"Make sure that `topics_to_merge` is either\" \"a list of topics or a list of list of topics.\" ) documents . Topic = documents . Topic . map ( mapping ) self . topic_mapper_ . add_mappings ( mapping ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) self . _map_representative_docs () self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) partial_fit ( documents , embeddings = None , y = None ) \u00b6 Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a partial_fit method in order to incrementally train the topic model. Likewise, the bertopic.vectorizers.OnlineCountVectorizer is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: Generate embeddings with a pre-traing language model Incrementally update the dimensionality reduction algorithm with partial_fit Incrementally update the cluster algorithm with partial_fit Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use partial_fit with batches and not single documents for the best performance. Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Examples: from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for index in range ( 0 , len ( docs ), 1000 ): topic_model . partial_fit ( docs [ index : index + 1000 ]) Source code in bertopic\\_bertopic.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def partial_fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a `partial_fit` method in order to incrementally train the topic model. Likewise, the `bertopic.vectorizers.OnlineCountVectorizer` is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: 1. Generate embeddings with a pre-traing language model 2. Incrementally update the dimensionality reduction algorithm with `partial_fit` 3. Incrementally update the cluster algorithm with `partial_fit` 4. Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use `partial_fit` with batches and not single documents for the best performance. Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'))[\"data\"] # Prepare sub-models that support online learning umap_model = IncrementalPCA(n_components=5) cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0) vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01) topic_model = BERTopic(umap_model=umap_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model) # Incrementally fit the topic model by training on 1000 documents at a time for index in range(0, len(docs), 1000): topic_model.partial_fit(docs[index: index+1000]) ``` \"\"\" # Checks check_embeddings_shape ( embeddings , documents ) if not hasattr ( self . hdbscan_model , \"partial_fit\" ): raise ValueError ( \"In order to use `.partial_fit`, the cluster model should have \" \"a `.partial_fit` function.\" ) # Prepare documents if isinstance ( documents , str ): documents = [ documents ] documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : if self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) else : if self . embedding_model is not None and self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y , partial_fit = True ) # Cluster reduced embeddings documents , self . probabilities_ = self . _cluster_embeddings ( umap_embeddings , documents , partial_fit = True ) topics = documents . Topic . to_list () # Map and find new topics if not self . topic_mapper_ : self . topic_mapper_ = TopicMapper ( topics ) mappings = self . topic_mapper_ . get_mappings () new_topics = set ( topics ) . difference ( set ( mappings . keys ())) new_topic_ids = { topic : max ( mappings . values ()) + index + 1 for index , topic in enumerate ( new_topics )} self . topic_mapper_ . add_new_topics ( new_topic_ids ) updated_mappings = self . topic_mapper_ . get_mappings () updated_topics = [ updated_mappings [ topic ] for topic in topics ] documents [ \"Topic\" ] = updated_topics # Add missing topics (topics that were originally created but are now missing) if self . topic_representations_ : missing_topics = set ( self . topic_representations_ . keys ()) . difference ( set ( updated_topics )) for missing_topic in missing_topics : documents . loc [ len ( documents ), :] = [ \" \" , len ( documents ), missing_topic ] else : missing_topics = {} # Prepare documents documents_per_topic = documents . sort_values ( \"Topic\" ) . groupby ([ 'Topic' ], as_index = False ) updated_topics = documents_per_topic . first () . Topic . astype ( int ) documents_per_topic = documents_per_topic . agg ({ 'Document' : ' ' . join }) # Update topic representations self . c_tf_idf_ , updated_words = self . _c_tf_idf ( documents_per_topic , partial_fit = True ) self . topic_representations_ = self . _extract_words_per_topic ( updated_words , self . c_tf_idf_ , labels = updated_topics ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} # Update topic sizes if len ( missing_topics ) > 0 : documents = documents . iloc [: - len ( missing_topics )] if self . topic_sizes_ is None : self . _update_topic_size ( documents ) else : sizes = documents . groupby ([ 'Topic' ], as_index = False ) . count () for _ , row in sizes . iterrows (): topic = int ( row . Topic ) if self . topic_sizes_ . get ( topic ) is not None and topic not in missing_topics : self . topic_sizes_ [ topic ] += int ( row . Document ) elif self . topic_sizes_ . get ( topic ) is None : self . topic_sizes_ [ topic ] = int ( row . Document ) self . topics_ = documents . Topic . astype ( int ) . tolist () return self reduce_topics ( docs , nr_topics = 20 ) \u00b6 Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Parameters: Name Type Description Default docs List [ str ] The docs you used when calling either fit or fit_transform required nr_topics int The number of topics you want reduced to 20 Updates topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): topic_model . reduce_topics ( docs , nr_topics = 30 ) You can then access the updated topics and probabilities with: topics = topic_model . topics_ probabilities = topic_model . probabilities_ Source code in bertopic\\_bertopic.py 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 def reduce_topics ( self , docs : List [ str ], nr_topics : int = 20 ) -> None : \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Updates: topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python topic_model.reduce_topics(docs, nr_topics=30) ``` You can then access the updated topics and probabilities with: ```python topics = topic_model.topics_ probabilities = topic_model.probabilities_ ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) # Reduce number of topics documents = self . _reduce_topics ( documents ) self . _merged_topics = None self . _map_representative_docs () # Map probabilities self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) return self save ( path , save_embedding_model = True ) \u00b6 Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required save_embedding_model bool Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. True Examples: topic_model . save ( \"my_model\" ) or if you do not want the embedding_model to be saved locally: topic_model . save ( \"my_model\" , save_embedding_model = False ) Source code in bertopic\\_bertopic.py 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Examples: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : # This prevents the vectorizer from being too large in size if `min_df` was # set to a value higher than 1 self . vectorizer_model . stop_words_ = None if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file ) set_topic_labels ( topic_labels ) \u00b6 Set custom topic labels in your fitted BERTopic model Parameters: Name Type Description Default topic_labels Union [ List [ str ], Mapping [ int , str ]] If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of topic ID : topic_label , it can have any number of topics as it will only map the topics found in the dictionary. required Examples: First, we define our topic labels with .get_topic_labels in which we can customize our topic labels: topic_labels = topic_model . get_topic_labels ( nr_words = 2 , topic_prefix = True , word_length = 10 , separator = \", \" ) Then, we pass these topic_labels to our topic model which can be accessed at any time with .custom_labels_ : topic_model . set_topic_labels ( topic_labels ) topic_model . custom_labels_ You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: topic_model . set_topic_labels ({ 0 : \"Space\" , 1 : \"Sports\" , 2 : \"Medicine\" }) topic_model . custom_labels_ Source code in bertopic\\_bertopic.py 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 def set_topic_labels ( self , topic_labels : Union [ List [ str ], Mapping [ int , str ]]) -> None : \"\"\" Set custom topic labels in your fitted BERTopic model Arguments: topic_labels: If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of `topic ID`: `topic_label`, it can have any number of topics as it will only map the topics found in the dictionary. Examples: First, we define our topic labels with `.get_topic_labels` in which we can customize our topic labels: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, topic_prefix=True, word_length=10, separator=\", \") ``` Then, we pass these `topic_labels` to our topic model which can be accessed at any time with `.custom_labels_`: ```python topic_model.set_topic_labels(topic_labels) topic_model.custom_labels_ ``` You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: ```python topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"}) topic_model.custom_labels_ ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) if isinstance ( topic_labels , dict ): if self . custom_labels_ is not None : original_labels = { topic : label for topic , label in zip ( unique_topics , self . custom_labels_ )} else : info = self . get_topic_info () original_labels = dict ( zip ( info . Topic , info . Name )) custom_labels = [ topic_labels . get ( topic ) if topic_labels . get ( topic ) else original_labels [ topic ] for topic in unique_topics ] elif isinstance ( topic_labels , list ): if len ( topic_labels ) == len ( unique_topics ): custom_labels = topic_labels else : raise ValueError ( \"Make sure that `topic_labels` contains the same number \" \"of labels as that there are topics.\" ) self . custom_labels_ = custom_labels topics_over_time ( docs , timestamps , nr_bins = None , datetime_format = None , evolution_tuning = True , global_tuning = True ) \u00b6 Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required timestamps Union [ List [ str ], List [ int ]] The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. required nr_bins int The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. None datetime_format str The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. None evolution_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1 . This creates evolutionary topic representations. True global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t . True Returns: Name Type Description topics_over_time pd . DataFrame A dataframe that contains the topic, words, and frequency of topic at timestamp t . Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_over_time = topic_model . topics_over_time ( docs , timestamps , nr_bins = 20 ) Source code in bertopic\\_bertopic.py 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def topics_over_time ( self , docs : List [ str ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp *t*. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp *t*. Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ]) topics_per_class ( docs , classes , global_tuning = True ) \u00b6 Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required classes Union [ List [ int ], List [ str ]] The class of each document. This can be either a list of strings or ints. required global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Name Type Description topics_per_class pd . DataFrame A dataframe that contains the topic, words, and frequency of topics for each class. Examples: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , classes ) Source code in bertopic\\_bertopic.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 def topics_per_class ( self , docs : List [ str ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for _ , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + self . _outliers ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class transform ( documents , embeddings = None ) \u00b6 After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union [ str , List [ str ]] A single document or a list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Name Type Description predictions List [ int ] Topic predictions for each documents probabilities np . ndarray The topic probability distribution which is returned by default. If calculate_probabilities in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) topics , probs = topic_model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) topics , probs = topic_model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) # Extract predictions and probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) # Calculate probabilities if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) logger . info ( \"Calculated probabilities with HDBSCAN\" ) else : predictions = self . hdbscan_model . predict ( umap_embeddings ) probabilities = None logger . info ( \"Predicted clusters\" ) # Map probabilities and predictions probabilities = self . _map_probabilities ( probabilities , original_topics = True ) predictions = self . _map_predictions ( predictions ) return predictions , probabilities update_topics ( docs , topics = None , n_gram_range = None , vectorizer_model = None , ctfidf_model = None ) \u00b6 Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A list of topics where each topic is related to a document in docs . Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline None n_gram_range Tuple [ int , int ] The n-gram range for the CountVectorizer. None vectorizer_model CountVectorizer Pass in your own CountVectorizer from scikit-learn None ctfidf_model ClassTfidfTransformer Pass in your own c-TF-IDF model to update the representations None Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: topic_model . update_topics ( docs , n_gram_range = ( 2 , 3 )) You can also use a custom vectorizer to update the representation: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) You can also use this function to change or map the topics to something else. You can update them as follows: topic_model . update_topics ( docs , my_updated_topics ) Source code in bertopic\\_bertopic.py 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 def update_topics ( self , docs : List [ str ], topics : List [ int ] = None , n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None , ctfidf_model : ClassTfidfTransformer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: A list of topics where each topic is related to a document in `docs`. Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn ctfidf_model: Pass in your own c-TF-IDF model to update the representations Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, n_gram_range=(2, 3)) ``` You can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, vectorizer_model=vectorizer_model) ``` You can also use this function to change or map the topics to something else. You can update them as follows: ```python topic_model.update_topics(docs, my_updated_topics) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () if topics is None : topics = self . topics_ labels = None else : labels = sorted ( list ( set ( topics ))) warnings . warn ( \"Using a custom list of topic assignments may lead to errors if \" \"topic reduction techniques are used afterwards. Make sure that \" \"manually assigning topics is the last step in the pipeline.\" ) # Extract words documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words , labels = labels ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} self . _update_topic_size ( documents ) visualize_barchart ( topics = None , top_n_topics = 8 , n_words = 5 , custom_labels = False , title = 'Topic Word Scores' , width = 250 , height = 250 ) \u00b6 Visualize a barchart of selected topics Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 8 n_words int Number of words to show in a topic 5 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False title str Title of the plot. 'Topic Word Scores' width int The width of each figure. 250 height int The height of each figure. 250 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , custom_labels = custom_labels , title = title , width = width , height = height ) visualize_distribution ( probabilities , min_probability = 0.015 , custom_labels = False , width = 800 , height = 600 ) \u00b6 Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities np . ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 600 Examples: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( topic_model . probabilities_ [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( topic_model . probabilities_ [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(topic_model.probabilities_[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(topic_model.probabilities_[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , custom_labels = custom_labels , width = width , height = height ) visualize_documents ( docs , topics = None , embeddings = None , reduced_embeddings = None , sample = None , hide_annotations = False , hide_document_hover = False , custom_labels = False , width = 1200 , height = 750 ) \u00b6 Visualize documents and their topics in 2D Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample float The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_documents ( docs ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 def visualize_documents ( self , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_documents ( self , docs = docs , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , custom_labels = custom_labels , width = width , height = height ) visualize_heatmap ( topics = None , top_n_topics = None , n_clusters = None , custom_labels = False , width = 800 , height = 800 ) \u00b6 Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 800 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , custom_labels = custom_labels , width = width , height = height ) visualize_hierarchical_documents ( docs , hierarchical_topics , topics = None , embeddings = None , reduced_embeddings = None , sample = None , hide_annotations = False , hide_document_hover = True , nr_levels = 10 , custom_labels = False , width = 1200 , height = 750 ) \u00b6 Visualize documents and their topics in 2D at different levels of hierarchy Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample Union [ float , int ] The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. True nr_levels int The number of levels to be visualized in the hierarchy. First, the distances in hierarchical_topics.Distance are split in nr_levels lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that nr_levels is equal to the length of hierarchical_topics . 10 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 def visualize_hierarchical_documents ( self , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchical_documents ( self , docs = docs , hierarchical_topics = hierarchical_topics , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , nr_levels = nr_levels , custom_labels = custom_labels , width = width , height = height ) visualize_hierarchy ( orientation = 'left' , topics = None , top_n_topics = None , custom_labels = False , width = 1000 , height = 600 , hierarchical_topics = None , linkage_function = None , distance_function = None , color_threshold = 1 ) \u00b6 Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. Only works if orientation is set to 'left' 1000 height int The height of the figure. Only works if orientation is set to 'bottom' 600 hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both topics and top_n_topics are not set. None linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) NOTE: Make sure to use the same linkage_function as used in topic_model.hierarchical_topics . None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) NOTE: Make sure to use the same distance_function as used in topic_model.hierarchical_topics . None color_threshold int Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. 1 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () If you also want the labels visualized of hierarchical topics, run the following: # Extract hierarchical topics and their representations hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Visualize these representations topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , custom_labels = custom_labels , width = width , height = height , hierarchical_topics = hierarchical_topics , linkage_function = linkage_function , distance_function = distance_function , color_threshold = color_threshold ) visualize_term_rank ( topics = None , log_scale = False , custom_labels = False , width = 800 , height = 500 ) \u00b6 Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the ranks of all words across all topics simply run: topic_model . visualize_term_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_term_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\_bertopic.py 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , custom_labels = custom_labels , width = width , height = height ) visualize_topics ( topics = None , top_n_topics = None , width = 650 , height = 650 ) \u00b6 Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Examples: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height ) visualize_topics_over_time ( topics_over_time , top_n_topics = None , topics = None , normalize_frequency = False , custom_labels = False , width = 1250 , height = 450 ) \u00b6 Visualize topics over time Parameters: Name Type Description Default topics_over_time pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height ) visualize_topics_per_class ( topics_per_class , top_n_topics = 10 , topics = None , normalize_frequency = False , custom_labels = False , width = 1250 , height = 900 ) \u00b6 Visualize topics per class Parameters: Name Type Description Default topics_per_class pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height )","title":"BERTopic"},{"location":"api/bertopic.html#bertopic","text":"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. The default embedding model is all-MiniLM-L6-v2 when selecting language=\"english\" and paraphrase-multilingual-MiniLM-L12-v2 when selecting language=\"multilingual\" . Attributes: Name Type Description topics_ List[int]) The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ List [ float ] The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When calculate_probabilities=True , then it is the probabilities of all topics per document. topic_sizes_ Mapping[int, int]) The size of each topic topic_mapper_ TopicMapper) A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ Mapping[int, Tuple[int, float]]) The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ csr_matrix) The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run .vectorizer_model.get_feature_names() or .vectorizer_model.get_feature_names_out() topic_labels_ Mapping[int, str]) The default labels for each topic. custom_labels_ List[str]) Custom labels for each topic. topic_embeddings_ np.ndarray) The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ Mapping[int, str]) The representative documents for each topic. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probabilities = topic_model . fit_transform ( docs ) If you want to use your own embedding model, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. Source code in bertopic\\_bertopic.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 1743 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 1800 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 1850 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 1941 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 2131 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 2168 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 2199 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 2222 2223 2224 2225 2226 2227 2228 2229 2230 2231 2232 2233 2234 2235 2236 2237 2238 2239 2240 2241 2242 2243 2244 2245 2246 2247 2248 2249 2250 2251 2252 2253 2254 2255 2256 2257 2258 2259 2260 2261 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2276 2277 2278 2279 2280 2281 2282 2283 2284 2285 2286 2287 2288 2289 2290 2291 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2317 2318 2319 2320 2321 2322 2323 2324 2325 2326 2327 2328 2329 2330 2331 2332 2333 2334 2335 2336 2337 2338 2339 2340 2341 2342 2343 2344 2345 2346 2347 2348 2349 2350 2351 2352 2353 2354 2355 2356 2357 2358 2359 2360 2361 2362 2363 2364 2365 2366 2367 2368 2369 2370 2371 2372 2373 2374 2375 2376 2377 2378 2379 2380 2381 2382 2383 2384 2385 2386 2387 2388 2389 2390 2391 2392 2393 2394 2395 2396 2397 2398 2399 2400 2401 2402 2403 2404 2405 2406 2407 2408 2409 2410 2411 2412 2413 2414 2415 2416 2417 2418 2419 2420 2421 2422 2423 2424 2425 2426 2427 2428 2429 2430 2431 2432 2433 2434 2435 2436 2437 2438 2439 2440 2441 2442 2443 2444 2445 2446 2447 2448 2449 2450 2451 2452 2453 2454 2455 2456 2457 2458 2459 2460 2461 2462 2463 2464 2465 2466 2467 2468 2469 2470 2471 2472 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 2483 2484 2485 2486 2487 2488 2489 2490 2491 2492 2493 2494 2495 2496 2497 2498 2499 2500 2501 2502 2503 2504 2505 2506 2507 2508 2509 2510 2511 2512 2513 2514 2515 2516 2517 2518 2519 2520 2521 2522 2523 2524 2525 2526 2527 2528 2529 2530 2531 2532 2533 2534 2535 2536 2537 2538 2539 2540 2541 2542 2543 2544 2545 2546 2547 2548 2549 2550 2551 2552 2553 2554 2555 2556 2557 2558 2559 2560 2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574 2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588 2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602 2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616 2617 2618 2619 2620 2621 2622 2623 2624 2625 2626 2627 2628 2629 2630 2631 2632 2633 2634 2635 2636 2637 2638 2639 2640 2641 2642 2643 2644 2645 2646 2647 2648 2649 2650 2651 2652 2653 2654 2655 2656 2657 2658 2659 2660 2661 2662 2663 2664 2665 2666 2667 2668 2669 2670 2671 2672 2673 2674 2675 2676 2677 2678 2679 2680 2681 2682 2683 2684 2685 2686 2687 2688 2689 2690 2691 2692 2693 2694 2695 2696 2697 2698 2699 2700 2701 2702 2703 2704 2705 2706 2707 2708 2709 2710 2711 2712 2713 2714 2715 2716 2717 2718 2719 2720 2721 2722 2723 2724 2725 2726 2727 2728 2729 2730 2731 2732 2733 2734 2735 2736 2737 2738 2739 2740 2741 2742 2743 2744 2745 2746 2747 2748 2749 2750 2751 2752 2753 2754 2755 2756 2757 2758 2759 2760 2761 2762 2763 2764 2765 2766 2767 2768 2769 2770 2771 2772 2773 2774 2775 2776 2777 2778 2779 2780 2781 2782 2783 2784 2785 2786 2787 2788 2789 2790 2791 2792 2793 2794 2795 2796 2797 2798 2799 2800 2801 2802 2803 2804 2805 2806 2807 2808 2809 2810 2811 2812 2813 2814 2815 2816 2817 2818 2819 2820 2821 2822 2823 2824 2825 2826 2827 2828 2829 2830 2831 2832 2833 2834 2835 2836 2837 2838 2839 2840 2841 2842 2843 2844 2845 2846 2847 2848 2849 2850 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 class BERTopic : \"\"\"BERTopic is a topic modeling technique that leverages BERT embeddings and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. The default embedding model is `all-MiniLM-L6-v2` when selecting `language=\"english\"` and `paraphrase-multilingual-MiniLM-L12-v2` when selecting `language=\"multilingual\"`. Attributes: topics_ (List[int]) : The topics that are generated for each document after training or updating the topic model. The most recent topics are tracked. probabilities_ (List[float]): The probability of the assigned topic per document. These are only calculated if a HDBSCAN model is used for the clustering step. When `calculate_probabilities=True`, then it is the probabilities of all topics per document. topic_sizes_ (Mapping[int, int]) : The size of each topic topic_mapper_ (TopicMapper) : A class for tracking topics and their mappings anytime they are merged, reduced, added, or removed. topic_representations_ (Mapping[int, Tuple[int, float]]) : The top n terms per topic and their respective c-TF-IDF values. c_tf_idf_ (csr_matrix) : The topic-term matrix as calculated through c-TF-IDF. To access its respective words, run `.vectorizer_model.get_feature_names()` or `.vectorizer_model.get_feature_names_out()` topic_labels_ (Mapping[int, str]) : The default labels for each topic. custom_labels_ (List[str]) : Custom labels for each topic. topic_embeddings_ (np.ndarray) : The embeddings for each topic. It is calculated by taking the weighted average of word embeddings in a topic based on their c-TF-IDF values. representative_docs_ (Mapping[int, str]) : The representative documents for each topic. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probabilities = topic_model.fit_transform(docs) ``` If you want to use your own embedding model, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") topic_model = BERTopic(embedding_model=sentence_model) ``` Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. Using your own embeddings allows you to try out BERTopic several times until you find the topics that suit you best. \"\"\" def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , diversity : float = None , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , ctfidf_model : TfidfTransformer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. The default sentence-transformers model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2` sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. low_memory: Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. diversity: Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has `.fit` and `.transform` functions. hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has `.fit` and `.predict` functions along with the `.labels_` variable. vectorizer_model: Pass in a custom `CountVectorizer` instead of the default model. ctfidf_model: Pass in a custom ClassTfidfTransformer instead of the default model. \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . diversity = diversity self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () # UMAP or another algorithm that has .fit and .transform functions self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN or another clustering algorithm that has .fit and .predict functions and # the .labels_ variable to extract the labels self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Public attributes self . topics_ = None self . probabilities_ = None self . topic_sizes_ = None self . topic_mapper_ = None self . topic_representations_ = None self . topic_embeddings_ = None self . topic_labels_ = None self . custom_labels_ = None self . representative_docs_ = None self . c_tf_idf_ = None # Private attributes for internal tracking purposes self . _outliers = 1 self . _merged_topics = None if verbose : logger . set_level ( \"DEBUG\" ) def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster reduced embeddings documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs ( original_topics = True ) self . probabilities_ = self . _map_probabilities ( probabilities , original_topics = True ) predictions = documents . Topic . to_list () return predictions , self . probabilities_ def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) # Extract predictions and probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) # Calculate probabilities if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) logger . info ( \"Calculated probabilities with HDBSCAN\" ) else : predictions = self . hdbscan_model . predict ( umap_embeddings ) probabilities = None logger . info ( \"Predicted clusters\" ) # Map probabilities and predictions probabilities = self . _map_probabilities ( probabilities , original_topics = True ) predictions = self . _map_predictions ( predictions ) return predictions , probabilities def partial_fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a `partial_fit` method in order to incrementally train the topic model. Likewise, the `bertopic.vectorizers.OnlineCountVectorizer` is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: 1. Generate embeddings with a pre-traing language model 2. Incrementally update the dimensionality reduction algorithm with `partial_fit` 3. Incrementally update the cluster algorithm with `partial_fit` 4. Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use `partial_fit` with batches and not single documents for the best performance. Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'))[\"data\"] # Prepare sub-models that support online learning umap_model = IncrementalPCA(n_components=5) cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0) vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01) topic_model = BERTopic(umap_model=umap_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model) # Incrementally fit the topic model by training on 1000 documents at a time for index in range(0, len(docs), 1000): topic_model.partial_fit(docs[index: index+1000]) ``` \"\"\" # Checks check_embeddings_shape ( embeddings , documents ) if not hasattr ( self . hdbscan_model , \"partial_fit\" ): raise ValueError ( \"In order to use `.partial_fit`, the cluster model should have \" \"a `.partial_fit` function.\" ) # Prepare documents if isinstance ( documents , str ): documents = [ documents ] documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : if self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) else : if self . embedding_model is not None and self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y , partial_fit = True ) # Cluster reduced embeddings documents , self . probabilities_ = self . _cluster_embeddings ( umap_embeddings , documents , partial_fit = True ) topics = documents . Topic . to_list () # Map and find new topics if not self . topic_mapper_ : self . topic_mapper_ = TopicMapper ( topics ) mappings = self . topic_mapper_ . get_mappings () new_topics = set ( topics ) . difference ( set ( mappings . keys ())) new_topic_ids = { topic : max ( mappings . values ()) + index + 1 for index , topic in enumerate ( new_topics )} self . topic_mapper_ . add_new_topics ( new_topic_ids ) updated_mappings = self . topic_mapper_ . get_mappings () updated_topics = [ updated_mappings [ topic ] for topic in topics ] documents [ \"Topic\" ] = updated_topics # Add missing topics (topics that were originally created but are now missing) if self . topic_representations_ : missing_topics = set ( self . topic_representations_ . keys ()) . difference ( set ( updated_topics )) for missing_topic in missing_topics : documents . loc [ len ( documents ), :] = [ \" \" , len ( documents ), missing_topic ] else : missing_topics = {} # Prepare documents documents_per_topic = documents . sort_values ( \"Topic\" ) . groupby ([ 'Topic' ], as_index = False ) updated_topics = documents_per_topic . first () . Topic . astype ( int ) documents_per_topic = documents_per_topic . agg ({ 'Document' : ' ' . join }) # Update topic representations self . c_tf_idf_ , updated_words = self . _c_tf_idf ( documents_per_topic , partial_fit = True ) self . topic_representations_ = self . _extract_words_per_topic ( updated_words , self . c_tf_idf_ , labels = updated_topics ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} # Update topic sizes if len ( missing_topics ) > 0 : documents = documents . iloc [: - len ( missing_topics )] if self . topic_sizes_ is None : self . _update_topic_size ( documents ) else : sizes = documents . groupby ([ 'Topic' ], as_index = False ) . count () for _ , row in sizes . iterrows (): topic = int ( row . Topic ) if self . topic_sizes_ . get ( topic ) is not None and topic not in missing_topics : self . topic_sizes_ [ topic ] += int ( row . Document ) elif self . topic_sizes_ . get ( topic ) is None : self . topic_sizes_ [ topic ] = int ( row . Document ) self . topics_ = documents . Topic . astype ( int ) . tolist () return self def topics_over_time ( self , docs : List [ str ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp *t*. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp *t*. Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ]) def topics_per_class ( self , docs : List [ str ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for _ , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + self . _outliers ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class def hierarchical_topics ( self , docs : List [ int ], linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None ) -> pd . DataFrame : \"\"\" Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using `scipy.cluster.hierarchy.linkage`. Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` Returns: hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) ``` A custom linkage function can be used as follows: ```python from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) # Hierarchical topics linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True) hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function) ``` \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Calculate linkage embeddings = self . c_tf_idf_ [ self . _outliers :] X = distance_function ( embeddings ) Z = linkage_function ( X ) # Calculate basic bag-of-words to be iteratively merged later documents = pd . DataFrame ({ \"Document\" : docs , \"ID\" : range ( len ( docs )), \"Topic\" : self . topics_ }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) documents_per_topic = documents_per_topic . loc [ documents_per_topic . Topic != - 1 , :] documents = self . _preprocess_text ( documents_per_topic . Document . values ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () bow = self . vectorizer_model . transform ( documents ) # Extract clusters hier_topics = pd . DataFrame ( columns = [ \"Parent_ID\" , \"Parent_Name\" , \"Topics\" , \"Child_Left_ID\" , \"Child_Left_Name\" , \"Child_Right_ID\" , \"Child_Right_Name\" ]) for index in tqdm ( range ( len ( Z ))): # Find clustered documents clusters = sch . fcluster ( Z , t = Z [ index ][ 2 ], criterion = 'distance' ) - self . _outliers cluster_df = pd . DataFrame ({ \"Topic\" : range ( len ( clusters )), \"Cluster\" : clusters }) cluster_df = cluster_df . groupby ( \"Cluster\" ) . agg ({ 'Topic' : lambda x : list ( x )}) . reset_index () nr_clusters = len ( clusters ) # Extract first topic we find to get the set of topics in a merged topic topic = None val = Z [ index ][ 0 ] while topic is None : if val - len ( clusters ) < 0 : topic = int ( val ) else : val = Z [ int ( val - len ( clusters ))][ 0 ] clustered_topics = [ i for i , x in enumerate ( clusters ) if x == clusters [ topic ]] # Group bow per cluster, calculate c-TF-IDF and extract words grouped = csr_matrix ( bow [ clustered_topics ] . sum ( axis = 0 )) c_tf_idf = self . ctfidf_model . transform ( grouped ) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels = [ 0 ]) # Extract parent's name and ID parent_id = index + len ( clusters ) parent_name = \"_\" . join ([ x [ 0 ] for x in words_per_topic [ 0 ]][: 5 ]) # Extract child's name and ID Z_id = Z [ index ][ 0 ] child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_left_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_left_name = hier_topics . iloc [ int ( child_left_id )] . Parent_Name # Extract child's name and ID Z_id = Z [ index ][ 1 ] child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_right_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_right_name = hier_topics . iloc [ int ( child_right_id )] . Parent_Name # Save results hier_topics . loc [ len ( hier_topics ), :] = [ parent_id , parent_name , clustered_topics , int ( Z [ index ][ 0 ]), child_left_name , int ( Z [ index ][ 1 ]), child_right_name ] hier_topics [ \"Distance\" ] = Z [:, 2 ] hier_topics = hier_topics . sort_values ( \"Parent_ID\" , ascending = False ) hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] = hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] . astype ( str ) return hier_topics def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings_ ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity def update_topics ( self , docs : List [ str ], topics : List [ int ] = None , n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None , ctfidf_model : ClassTfidfTransformer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: A list of topics where each topic is related to a document in `docs`. Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn ctfidf_model: Pass in your own c-TF-IDF model to update the representations Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, n_gram_range=(2, 3)) ``` You can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, vectorizer_model=vectorizer_model) ``` You can also use this function to change or map the topics to something else. You can update them as follows: ```python topic_model.update_topics(docs, my_updated_topics) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () if topics is None : topics = self . topics_ labels = None else : labels = sorted ( list ( set ( topics ))) warnings . warn ( \"Using a custom list of topic assignments may lead to errors if \" \"topic reduction techniques are used afterwards. Make sure that \" \"manually assigning topics is the last step in the pipeline.\" ) # Extract words documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words , labels = labels ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} self . _update_topic_size ( documents ) def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topic_representations_ def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Examples: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topic_representations_ : return self . topic_representations_ [ topic ] else : return False def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its ID, frequency, and name. Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Examples: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Topic\" , \"Count\" ]) . sort_values ( \"Topic\" ) info [ \"Name\" ] = info . Topic . map ( self . topic_labels_ ) if self . custom_labels_ is not None : if len ( self . custom_labels_ ) == len ( info ): labels = { topic - self . _outliers : label for topic , label in enumerate ( self . custom_labels_ )} info [ \"CustomName\" ] = info [ \"Topic\" ] . map ( labels ) if topic is not None : info = info . loc [ info . Topic == topic , :] return info . reset_index ( drop = True ) def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Examples: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes_ [ topic ] else : return pd . DataFrame ( self . topic_sizes_ . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False ) def get_representative_docs ( self , topic : int = None ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Examples: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs_ [ topic ] else : return self . representative_docs_ @staticmethod def get_topic_tree ( hier_topics : pd . DataFrame , max_distance : float = None , tight_layout : bool = False ) -> str : \"\"\" Extract the topic tree such that it can be printed Arguments: hier_topics: A dataframe containing the structure of the topic tree. This is the output of `topic_model.hierachical_topics()` max_distance: The maximum distance between two topics. This value is based on the Distance column in `hier_topics`. tight_layout: Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. Returns: A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 The blocks (\u25a0) indicate that the topic is one you can directly access from `topic_model.get_topic`. In other words, they are the original un-grouped topics. Examples: ```python # Train model from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) # Print topic tree tree = topic_model.get_topic_tree(hierarchical_topics) print(tree) ``` \"\"\" width = 1 if tight_layout else 4 if max_distance is None : max_distance = hier_topics . Distance . max () + 1 max_original_topic = hier_topics . Parent_ID . astype ( int ) . min () - 1 # Extract mapping from ID to name topic_to_name = dict ( zip ( hier_topics . Child_Left_ID , hier_topics . Child_Left_Name )) topic_to_name . update ( dict ( zip ( hier_topics . Child_Right_ID , hier_topics . Child_Right_Name ))) topic_to_name = { topic : name [: 100 ] for topic , name in topic_to_name . items ()} # Create tree tree = { str ( row [ 1 ] . Parent_ID ): [ str ( row [ 1 ] . Child_Left_ID ), str ( row [ 1 ] . Child_Right_ID )] for row in hier_topics . iterrows ()} def get_tree ( start , tree ): \"\"\" Based on: https://stackoverflow.com/a/51920869/10532563 \"\"\" def _tree ( to_print , start , parent , tree , grandpa = None , indent = \"\" ): # Get distance between merged topics distance = hier_topics . loc [( hier_topics . Child_Left_ID == parent ) | ( hier_topics . Child_Right_ID == parent ), \"Distance\" ] distance = distance . values [ 0 ] if len ( distance ) > 0 else 10 if parent != start : if grandpa is None : to_print += topic_to_name [ parent ] else : if int ( parent ) <= max_original_topic : # Do not append topic ID if they are not merged if distance < max_distance : to_print += \"\u25a0\u2500\u2500\" + topic_to_name [ parent ] + f \" \u2500\u2500 Topic: { parent } \" + \" \\n \" else : to_print += \"O \\n \" else : to_print += topic_to_name [ parent ] + \" \\n \" if parent not in tree : return to_print for child in tree [ parent ][: - 1 ]: to_print += indent + \"\u251c\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \"\u2502\" + \" \" * width ) child = tree [ parent ][ - 1 ] to_print += indent + \"\u2514\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \" \" * ( width + 1 )) return to_print to_print = \".\" + \" \\n \" to_print = _tree ( to_print , start , start , tree ) return to_print start = str ( hier_topics . Parent_ID . astype ( int ) . max ()) return get_tree ( start , tree ) def set_topic_labels ( self , topic_labels : Union [ List [ str ], Mapping [ int , str ]]) -> None : \"\"\" Set custom topic labels in your fitted BERTopic model Arguments: topic_labels: If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of `topic ID`: `topic_label`, it can have any number of topics as it will only map the topics found in the dictionary. Examples: First, we define our topic labels with `.get_topic_labels` in which we can customize our topic labels: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, topic_prefix=True, word_length=10, separator=\", \") ``` Then, we pass these `topic_labels` to our topic model which can be accessed at any time with `.custom_labels_`: ```python topic_model.set_topic_labels(topic_labels) topic_model.custom_labels_ ``` You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: ```python topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"}) topic_model.custom_labels_ ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) if isinstance ( topic_labels , dict ): if self . custom_labels_ is not None : original_labels = { topic : label for topic , label in zip ( unique_topics , self . custom_labels_ )} else : info = self . get_topic_info () original_labels = dict ( zip ( info . Topic , info . Name )) custom_labels = [ topic_labels . get ( topic ) if topic_labels . get ( topic ) else original_labels [ topic ] for topic in unique_topics ] elif isinstance ( topic_labels , list ): if len ( topic_labels ) == len ( unique_topics ): custom_labels = topic_labels else : raise ValueError ( \"Make sure that `topic_labels` contains the same number \" \"of labels as that there are topics.\" ) self . custom_labels_ = custom_labels def generate_topic_labels ( self , nr_words : int = 3 , topic_prefix : bool = True , word_length : int = None , separator : str = \"_\" ) -> List [ str ]: \"\"\" Get labels for each topic in a user-defined format Arguments: original_labels: nr_words: Top `n` words per topic to use topic_prefix: Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the `separator` word_length: The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. separator: The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is `\", \"`. Returns: topic_labels: A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \") ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) topic_labels = [] for topic in unique_topics : words , _ = zip ( * self . get_topic ( topic )) if word_length : words = [ word [: word_length ] for word in words ][: nr_words ] else : words = list ( words )[: nr_words ] if topic_prefix : topic_label = f \" { topic }{ separator } \" + separator . join ( words ) else : topic_label = separator . join ( words ) topic_labels . append ( topic_label ) return topic_labels def merge_topics ( self , docs : List [ str ], topics_to_merge : List [ Union [ Iterable [ int ], int ]]) -> None : \"\"\" Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics_to_merge: Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. Examples: If you want to merge topics 1, 2, and 3: ```python topics_to_merge = [1, 2, 3] topic_model.merge_topics(docs, topics_to_merge) ``` or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: ```python topics_to_merge = [[1, 2] [3, 4]] topic_model.merge_topics(docs, topics_to_merge) ``` \"\"\" check_is_fitted ( self ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) mapping = { topic : topic for topic in set ( self . topics_ )} if isinstance ( topics_to_merge [ 0 ], int ): for topic in sorted ( topics_to_merge ): mapping [ topic ] = topics_to_merge [ 0 ] elif isinstance ( topics_to_merge [ 0 ], Iterable ): for topic_group in sorted ( topics_to_merge ): for topic in topic_group : mapping [ topic ] = topic_group [ 0 ] else : raise ValueError ( \"Make sure that `topics_to_merge` is either\" \"a list of topics or a list of list of topics.\" ) documents . Topic = documents . Topic . map ( mapping ) self . topic_mapper_ . add_mappings ( mapping ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) self . _map_representative_docs () self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) def reduce_topics ( self , docs : List [ str ], nr_topics : int = 20 ) -> None : \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Updates: topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python topic_model.reduce_topics(docs, nr_topics=30) ``` You can then access the updated topics and probabilities with: ```python topics = topic_model.topics_ probabilities = topic_model.probabilities_ ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) # Reduce number of topics documents = self . _reduce_topics ( documents ) self . _merged_topics = None self . _map_representative_docs () # Map probabilities self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) return self def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height ) def visualize_documents ( self , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_documents ( self , docs = docs , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , custom_labels = custom_labels , width = width , height = height ) def visualize_hierarchical_documents ( self , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchical_documents ( self , docs = docs , hierarchical_topics = hierarchical_topics , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , nr_levels = nr_levels , custom_labels = custom_labels , width = width , height = height ) def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , custom_labels = custom_labels , width = width , height = height ) def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height ) def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height ) def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(topic_model.probabilities_[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(topic_model.probabilities_[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , custom_labels = custom_labels , width = width , height = height ) def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , custom_labels = custom_labels , width = width , height = height , hierarchical_topics = hierarchical_topics , linkage_function = linkage_function , distance_function = distance_function , color_threshold = color_threshold ) def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , custom_labels = custom_labels , width = width , height = height ) def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , custom_labels = custom_labels , title = title , width = width , height = height ) def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Examples: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : # This prevents the vectorizer from being too large in size if `min_df` was # set to a value higher than 1 self . vectorizer_model . stop_words_ = None if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file ) @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Examples: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out def _extract_embeddings ( self , documents : Union [ List [ str ], str ], method : str = \"document\" , verbose : bool = None ) -> np . ndarray : \"\"\" Extract sentence/document embeddings through pre-trained embeddings For an overview of pre-trained models: https://www.sbert.net/docs/pretrained_models.html Arguments: documents: Dataframe with documents and their corresponding IDs method: Whether to extract document or word-embeddings, options are \"document\" and \"word\" verbose: Whether to show a progressbar demonstrating the time to extract embeddings Returns: embeddings: The extracted embeddings. \"\"\" if isinstance ( documents , str ): documents = [ documents ] if method == \"word\" : embeddings = self . embedding_model . embed_words ( documents , verbose ) elif method == \"document\" : embeddings = self . embedding_model . embed_documents ( documents , verbose ) else : raise ValueError ( \"Wrong method for extracting document/word embeddings. \" \"Either choose 'word' or 'document' as the method. \" ) return embeddings def _map_predictions ( self , predictions : List [ int ]) -> List [ int ]: \"\"\" Map predictions to the correct topics if topics were reduced \"\"\" mappings = self . topic_mapper_ . get_mappings ( original_topics = True ) mapped_predictions = [ mappings [ prediction ] if prediction in mappings else - 1 for prediction in predictions ] return mapped_predictions def _reduce_dimensionality ( self , embeddings : Union [ np . ndarray , csr_matrix ], y : Union [ List [ int ], np . ndarray ] = None , partial_fit : bool = False ) -> np . ndarray : \"\"\" Reduce dimensionality of embeddings using UMAP and train a UMAP model Arguments: embeddings: The extracted embeddings using the sentence transformer module. y: The target class for (semi)-supervised dimensionality reduction partial_fit: Whether to run `partial_fit` for online learning Returns: umap_embeddings: The reduced embeddings \"\"\" # Partial fit if partial_fit : if hasattr ( self . umap_model , \"partial_fit\" ): self . umap_model = self . umap_model . partial_fit ( embeddings ) elif self . topic_representations_ is None : self . umap_model . fit ( embeddings ) # Regular fit else : try : self . umap_model . fit ( embeddings , y = y ) except TypeError : logger . info ( \"The dimensionality reduction algorithm did not contain the `y` parameter and\" \" therefore the `y` parameter was not used\" ) self . umap_model . fit ( embeddings ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) return np . nan_to_num ( umap_embeddings ) def _cluster_embeddings ( self , umap_embeddings : np . ndarray , documents : pd . DataFrame , partial_fit : bool = False ) -> Tuple [ pd . DataFrame , np . ndarray ]: \"\"\" Cluster UMAP embeddings with HDBSCAN Arguments: umap_embeddings: The reduced sentence embeddings with UMAP documents: Dataframe with documents and their corresponding IDs partial_fit: Whether to run `partial_fit` for online learning Returns: documents: Updated dataframe with documents and their corresponding IDs and newly added Topics probabilities: The distribution of probabilities \"\"\" if partial_fit : self . hdbscan_model = self . hdbscan_model . partial_fit ( umap_embeddings ) labels = self . hdbscan_model . labels_ documents [ 'Topic' ] = labels self . topics_ = labels else : self . hdbscan_model . fit ( umap_embeddings ) labels = self . hdbscan_model . labels_ documents [ 'Topic' ] = labels self . _update_topic_size ( documents ) # Some algorithms have outlier labels (-1) that can be tricky to work # with if you are slicing data based on that labels. Therefore, we # track if there are outlier labels and act accordingly when slicing. self . _outliers = 1 if - 1 in set ( labels ) else 0 # Save representative docs and calculate probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): probabilities = self . hdbscan_model . probabilities_ self . _save_representative_docs ( documents ) if self . calculate_probabilities : probabilities = hdbscan . all_points_membership_vectors ( self . hdbscan_model ) else : probabilities = None if not partial_fit : self . topic_mapper_ = TopicMapper ( self . topics_ ) logger . info ( \"Clustered reduced embeddings\" ) return documents , probabilities def _guided_topic_modeling ( self , embeddings : np . ndarray ) -> Tuple [ List [ int ], np . array ]: \"\"\" Apply Guided Topic Modeling We transform the seeded topics to embeddings using the same embedder as used for generating document embeddings. Then, we apply cosine similarity between the embeddings and set labels for documents that are more similar to one of the topics, then the average document. If a document is more similar to the average document than any of the topics, it gets the -1 label and is thereby not included in UMAP. Arguments: embeddings: The document embeddings Returns y: The labels for each seeded topic embeddings: Updated embeddings \"\"\" # Create embeddings from the seeded topics seed_topic_list = [ \" \" . join ( seed_topic ) for seed_topic in self . seed_topic_list ] seed_topic_embeddings = self . _extract_embeddings ( seed_topic_list , verbose = self . verbose ) seed_topic_embeddings = np . vstack ([ seed_topic_embeddings , embeddings . mean ( axis = 0 )]) # Label documents that are most similar to one of the seeded topics sim_matrix = cosine_similarity ( embeddings , seed_topic_embeddings ) y = [ np . argmax ( sim_matrix [ index ]) for index in range ( sim_matrix . shape [ 0 ])] y = [ val if val != len ( seed_topic_list ) else - 1 for val in y ] # Average the document embeddings related to the seeded topics with the # embedding of the seeded topic to force the documents in a cluster for seed_topic in range ( len ( seed_topic_list )): indices = [ index for index , topic in enumerate ( y ) if topic == seed_topic ] embeddings [ indices ] = np . average ([ embeddings [ indices ], seed_topic_embeddings [ seed_topic ]], weights = [ 3 , 1 ]) return y , embeddings def _extract_topics ( self , documents : pd . DataFrame ): \"\"\" Extract topics from the clusters using a class-based TF-IDF Arguments: documents: Dataframe with documents and their corresponding IDs Returns: c_tf_idf: The resulting matrix giving a value (importance score) for each word per topic \"\"\" documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} def _save_representative_docs ( self , documents : pd . DataFrame ): \"\"\" Save the most representative docs (3) per topic The most representative docs are extracted by taking the exemplars from the HDBSCAN-generated clusters. Full instructions can be found here: https://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html Arguments: documents: Dataframe with documents and their corresponding IDs \"\"\" # Prepare the condensed tree and luf clusters beneath a given cluster condensed_tree = self . hdbscan_model . condensed_tree_ raw_tree = condensed_tree . _raw_tree clusters = sorted ( condensed_tree . _select_clusters ()) cluster_tree = raw_tree [ raw_tree [ 'child_size' ] > 1 ] # Find the points with maximum lambda value in each leaf representative_docs = {} for topic in documents [ 'Topic' ] . unique (): if topic != - 1 : leaves = hdbscan . plots . _recurse_leaf_dfs ( cluster_tree , clusters [ topic ]) result = np . array ([]) for leaf in leaves : max_lambda = raw_tree [ 'lambda_val' ][ raw_tree [ 'parent' ] == leaf ] . max () points = raw_tree [ 'child' ][( raw_tree [ 'parent' ] == leaf ) & ( raw_tree [ 'lambda_val' ] == max_lambda )] result = np . hstack (( result , points )) representative_docs [ topic ] = list ( np . random . choice ( result , 3 , replace = False ) . astype ( int )) # Convert indices to documents self . representative_docs_ = { topic : [ documents . iloc [ doc_id ] . Document for doc_id in doc_ids ] for topic , doc_ids in representative_docs . items ()} def _map_representative_docs ( self , original_topics : bool = False ): \"\"\" Map the representative docs per topic to the correct topics If topics were reduced, remove documents from topics that were merged into larger topics as we assume that the documents from larger topics are better representative of the entire merged topic. Arguments: original_topics: Whether we want to map from the original topics to the most recent topics or from the second-most recent topics. \"\"\" if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): mappings = self . topic_mapper_ . get_mappings ( original_topics ) representative_docs = self . representative_docs_ . copy () # Update the representative documents updated_representative_docs = { mappings [ old_topic ]: [] for old_topic , _ in representative_docs . items ()} for old_topic , docs in representative_docs . items (): new_topic = mappings [ old_topic ] updated_representative_docs [ new_topic ] . extend ( docs ) self . representative_docs_ = updated_representative_docs self . representative_docs_ . pop ( - 1 , None ) def _create_topic_vectors ( self ): \"\"\" Creates embeddings per topics based on their topic representation We start by creating embeddings out of the topic representation. This results in a number of embeddings per topic. Then, we take the weighted average of embeddings in a topic by their c-TF-IDF score. This will put more emphasis to words that represent a topic best. Only allow topic vectors to be created if there are no custom embeddings and therefore a sentence-transformer model to be used or there are custom embeddings but it is allowed to use a different multi-lingual sentence-transformer model \"\"\" if self . embedding_model is not None : topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () n = self . top_n_words # Extract embeddings for all words in all topics topic_words = [ self . get_topic ( topic ) for topic in topic_list ] topic_words = [ word [ 0 ] for topic in topic_words for word in topic ] embeddings = self . _extract_embeddings ( topic_words , method = \"word\" , verbose = False ) # Take the weighted average of word embeddings in a topic based on their c-TF-IDF value # The embeddings var is a single numpy matrix and therefore slicing is necessary to # access the words per topic topic_embeddings = [] for i , topic in enumerate ( topic_list ): word_importance = [ val [ 1 ] for val in self . get_topic ( topic )] if sum ( word_importance ) == 0 : word_importance = [ 1 for _ in range ( len ( self . get_topic ( topic )))] topic_embedding = np . average ( embeddings [ i * n : n + ( i * n )], weights = word_importance , axis = 0 ) topic_embeddings . append ( topic_embedding ) self . topic_embeddings_ = topic_embeddings def _c_tf_idf ( self , documents_per_topic : pd . DataFrame , fit : bool = True , partial_fit : bool = False ) -> Tuple [ csr_matrix , List [ str ]]: \"\"\" Calculate a class-based TF-IDF where m is the number of total documents. Arguments: documents_per_topic: The joined documents per topic such that each topic has a single string made out of multiple documents m: The total number of documents (unjoined) fit: Whether to fit a new vectorizer or use the fitted self.vectorizer_model partial_fit: Whether to run `partial_fit` for online learning Returns: tf_idf: The resulting matrix giving a value (importance score) for each word per topic words: The names of the words to which values were given \"\"\" documents = self . _preprocess_text ( documents_per_topic . Document . values ) if partial_fit : X = self . vectorizer_model . partial_fit ( documents ) . update_bow ( documents ) elif fit : self . vectorizer_model . fit ( documents ) X = self . vectorizer_model . transform ( documents ) else : X = self . vectorizer_model . transform ( documents ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () if self . seed_topic_list : seed_topic_list = [ seed for seeds in self . seed_topic_list for seed in seeds ] multiplier = np . array ([ 1.2 if word in seed_topic_list else 1 for word in words ]) else : multiplier = None if fit : self . ctfidf_model = self . ctfidf_model . fit ( X , multiplier = multiplier ) c_tf_idf = self . ctfidf_model . transform ( X ) return c_tf_idf , words def _update_topic_size ( self , documents : pd . DataFrame ): \"\"\" Calculate the topic sizes Arguments: documents: Updated dataframe with documents and their corresponding IDs and newly added Topics \"\"\" sizes = documents . groupby ([ 'Topic' ]) . count () . sort_values ( \"Document\" , ascending = False ) . reset_index () self . topic_sizes_ = dict ( zip ( sizes . Topic , sizes . Document )) self . topics_ = documents . Topic . astype ( int ) . tolist () def _extract_words_per_topic ( self , words : List [ str ], c_tf_idf : csr_matrix = None , labels : List [ int ] = None ) -> Mapping [ str , List [ Tuple [ str , float ]]]: \"\"\" Based on tf_idf scores per topic, extract the top n words per topic If the top words per topic need to be extracted, then only the `words` parameter needs to be passed. If the top words per topic in a specific timestamp, then it is important to pass the timestamp-based c-TF-IDF matrix and its corresponding labels. Arguments: words: List of all words (sorted according to tf_idf matrix position) c_tf_idf: A c-TF-IDF matrix from which to calculate the top words labels: A list of topic labels Returns: topics: The top words per topic \"\"\" if c_tf_idf is None : c_tf_idf = self . c_tf_idf_ if labels is None : labels = sorted ( list ( self . topic_sizes_ . keys ())) # Get the top 30 indices and values per row in a sparse c-TF-IDF matrix indices = self . _top_n_idx_sparse ( c_tf_idf , 30 ) scores = self . _top_n_values_sparse ( c_tf_idf , indices ) sorted_indices = np . argsort ( scores , 1 ) indices = np . take_along_axis ( indices , sorted_indices , axis = 1 ) scores = np . take_along_axis ( scores , sorted_indices , axis = 1 ) # Get top 30 words per topic based on c-TF-IDF score topics = { label : [( words [ word_index ], score ) if word_index is not None and score > 0 else ( \"\" , 0.00001 ) for word_index , score in zip ( indices [ index ][:: - 1 ], scores [ index ][:: - 1 ]) ] for index , label in enumerate ( labels )} # Extract word embeddings for the top 30 words per topic and compare it # with the topic embedding to keep only the words most similar to the topic embedding if self . diversity is not None : if self . embedding_model is not None : for topic , topic_words in topics . items (): words = [ word [ 0 ] for word in topic_words ] word_embeddings = self . _extract_embeddings ( words , method = \"word\" , verbose = False ) topic_embedding = self . _extract_embeddings ( \" \" . join ( words ), method = \"word\" , verbose = False ) . reshape ( 1 , - 1 ) topic_words = mmr ( topic_embedding , word_embeddings , words , top_n = self . top_n_words , diversity = self . diversity ) topics [ topic ] = [( word , value ) for word , value in topics [ topic ] if word in topic_words ] topics = { label : values [: self . top_n_words ] for label , values in topics . items ()} return topics def _reduce_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce topics to self.nr_topics Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" initial_nr_topics = len ( self . get_topics ()) if isinstance ( self . nr_topics , int ): if self . nr_topics < initial_nr_topics : documents = self . _reduce_to_n_topics ( documents ) elif isinstance ( self . nr_topics , str ): documents = self . _auto_reduce_topics ( documents ) else : raise ValueError ( \"nr_topics needs to be an int or 'auto'! \" ) logger . info ( f \"Reduced number of topics from { initial_nr_topics } to { len ( self . get_topic_freq ()) } \" ) return documents def _reduce_to_n_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce topics to self.nr_topics Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" # Track which topics where originally merged if not self . _merged_topics : self . _merged_topics = [] # Create topic similarity matrix similarities = cosine_similarity ( self . c_tf_idf_ ) np . fill_diagonal ( similarities , 0 ) # Find most similar topic to least common topic topics = documents . Topic . tolist () . copy () mapped_topics = {} while len ( self . get_topic_freq ()) > self . nr_topics + self . _outliers : topic_to_merge = self . get_topic_freq () . iloc [ - 1 ] . Topic topic_to_merge_into = np . argmax ( similarities [ topic_to_merge + self . _outliers ]) - self . _outliers similarities [:, topic_to_merge + self . _outliers ] = - self . _outliers self . _merged_topics . append ( topic_to_merge ) # Update Topic labels documents . loc [ documents . Topic == topic_to_merge , \"Topic\" ] = topic_to_merge_into mapped_topics [ topic_to_merge ] = topic_to_merge_into self . _update_topic_size ( documents ) # Map topics mapped_topics = { from_topic : to_topic for from_topic , to_topic in zip ( topics , documents . Topic . tolist ())} self . topic_mapper_ . add_mappings ( mapped_topics ) # Update representations documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) return documents def _auto_reduce_topics ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reduce the number of topics automatically using HDBSCAN Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the reduced number of Topics \"\"\" topics = documents . Topic . tolist () . copy () unique_topics = sorted ( list ( documents . Topic . unique ()))[ self . _outliers :] max_topic = unique_topics [ - 1 ] # Find similar topics if self . topic_embeddings_ is not None : embeddings = np . array ( self . topic_embeddings_ ) else : embeddings = self . c_tf_idf_ . toarray () norm_data = normalize ( embeddings , norm = 'l2' ) predictions = hdbscan . HDBSCAN ( min_cluster_size = 2 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) . fit_predict ( norm_data [ self . _outliers :]) # Map similar topics mapped_topics = { unique_topics [ index ]: prediction + max_topic for index , prediction in enumerate ( predictions ) if prediction != - 1 } documents . Topic = documents . Topic . map ( mapped_topics ) . fillna ( documents . Topic ) . astype ( int ) mapped_topics = { from_topic : to_topic for from_topic , to_topic in zip ( topics , documents . Topic . tolist ())} # Update documents and topics self . topic_mapper_ . add_mappings ( mapped_topics ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) return documents def _sort_mappings_by_frequency ( self , documents : pd . DataFrame ) -> pd . DataFrame : \"\"\" Reorder mappings by their frequency. For example, if topic 88 was mapped to topic 5 and topic 5 turns out to be the largest topic, then topic 5 will be topic 0. The second largest, will be topic 1, etc. If there are no mappings since no reduction of topics took place, then the topics will simply be ordered by their frequency and will get the topic ids based on that order. This means that -1 will remain the outlier class, and that the rest of the topics will be in descending order of ids and frequency. Arguments: documents: Dataframe with documents and their corresponding IDs and Topics Returns: documents: Updated dataframe with documents and the mapped and re-ordered topic ids \"\"\" self . _update_topic_size ( documents ) # Map topics based on frequency df = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Old_Topic\" , \"Size\" ]) . sort_values ( \"Size\" , ascending = False ) df = df [ df . Old_Topic != - 1 ] sorted_topics = { ** { - 1 : - 1 }, ** dict ( zip ( df . Old_Topic , range ( len ( df ))))} self . topic_mapper_ . add_mappings ( sorted_topics ) # Map documents documents . Topic = documents . Topic . map ( sorted_topics ) . fillna ( documents . Topic ) . astype ( int ) self . _update_topic_size ( documents ) return documents def _map_probabilities ( self , probabilities : Union [ np . ndarray , None ], original_topics : bool = False ) -> Union [ np . ndarray , None ]: \"\"\" Map the probabilities to the reduced topics. This is achieved by adding the probabilities together of all topics that were mapped to the same topic. Then, the topics that were mapped from were set to 0 as they were reduced. Arguments: probabilities: An array containing probabilities original_topics: Whether we want to map from the original topics to the most recent topics or from the second-most recent topics. Returns: mapped_probabilities: Updated probabilities \"\"\" mappings = self . topic_mapper_ . get_mappings ( original_topics ) # Map array of probabilities (probability for assigned topic per document) if probabilities is not None : if len ( probabilities . shape ) == 2 and self . get_topic ( - 1 ): mapped_probabilities = np . zeros (( probabilities . shape [ 0 ], len ( set ( mappings . values ())) - 1 )) for from_topic , to_topic in mappings . items (): if to_topic != - 1 and from_topic != - 1 : mapped_probabilities [:, to_topic ] += probabilities [:, from_topic ] return mapped_probabilities return probabilities def _preprocess_text ( self , documents : np . ndarray ) -> List [ str ]: \"\"\" Basic preprocessing of text Steps: * Replace \\n and \\t with whitespace * Only keep alpha-numerical characters \"\"\" cleaned_documents = [ doc . replace ( \" \\n \" , \" \" ) for doc in documents ] cleaned_documents = [ doc . replace ( \" \\t \" , \" \" ) for doc in cleaned_documents ] if self . language == \"english\" : cleaned_documents = [ re . sub ( r '[^A-Za-z0-9 ]+' , '' , doc ) for doc in cleaned_documents ] cleaned_documents = [ doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents ] return cleaned_documents @staticmethod def _top_n_idx_sparse ( matrix : csr_matrix , n : int ) -> np . ndarray : \"\"\" Return indices of top n values in each row of a sparse matrix Retrieved from: https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix Arguments: matrix: The sparse matrix from which to get the top n indices per row n: The number of highest values to extract from each row Returns: indices: The top n indices per row \"\"\" indices = [] for le , ri in zip ( matrix . indptr [: - 1 ], matrix . indptr [ 1 :]): n_row_pick = min ( n , ri - le ) values = matrix . indices [ le + np . argpartition ( matrix . data [ le : ri ], - n_row_pick )[ - n_row_pick :]] values = [ values [ index ] if len ( values ) >= index + 1 else None for index in range ( n )] indices . append ( values ) return np . array ( indices ) @staticmethod def _top_n_values_sparse ( matrix : csr_matrix , indices : np . ndarray ) -> np . ndarray : \"\"\" Return the top n values for each row in a sparse matrix Arguments: matrix: The sparse matrix from which to get the top n indices per row indices: The top n indices per row Returns: top_values: The top n scores per row \"\"\" top_values = [] for row , values in enumerate ( indices ): scores = np . array ([ matrix [ row , value ] if value is not None else 0 for value in values ]) top_values . append ( scores ) return np . array ( top_values ) @classmethod def _get_param_names ( cls ): \"\"\"Get parameter names for the estimator Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 \"\"\" init_signature = inspect . signature ( cls . __init__ ) parameters = sorted ([ p . name for p in init_signature . parameters . values () if p . name != 'self' and p . kind != p . VAR_KEYWORD ]) return parameters def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\"","title":"BERTopic"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.__init__","text":"BERTopic initialization Parameters: Name Type Description Default language str The main language used in your documents. The default sentence-transformers model for \"english\" is all-MiniLM-L6-v2 . For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the paraphrase-multilingual-MiniLM-L12-v2 sentence-tranformers model that supports 50+ languages. 'english' top_n_words int The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. 10 n_gram_range Tuple [ int , int ] The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. (1, 1) min_topic_size int The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. 10 nr_topics Union [ int , str ] Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. None low_memory bool Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. False calculate_probabilities bool Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method visualize_probabilities . False diversity float Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. None seed_topic_list List [ List [ str ]] A list of seed words per topic to converge around None verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False embedding_model Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html None umap_model UMAP Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has .fit and .transform functions. None hdbscan_model hdbscan . HDBSCAN Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has .fit and .predict functions along with the .labels_ variable. None vectorizer_model CountVectorizer Pass in a custom CountVectorizer instead of the default model. None ctfidf_model TfidfTransformer Pass in a custom ClassTfidfTransformer instead of the default model. None Source code in bertopic\\_bertopic.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def __init__ ( self , language : str = \"english\" , top_n_words : int = 10 , n_gram_range : Tuple [ int , int ] = ( 1 , 1 ), min_topic_size : int = 10 , nr_topics : Union [ int , str ] = None , low_memory : bool = False , calculate_probabilities : bool = False , diversity : float = None , seed_topic_list : List [ List [ str ]] = None , embedding_model = None , umap_model : UMAP = None , hdbscan_model : hdbscan . HDBSCAN = None , vectorizer_model : CountVectorizer = None , ctfidf_model : TfidfTransformer = None , verbose : bool = False , ): \"\"\"BERTopic initialization Arguments: language: The main language used in your documents. The default sentence-transformers model for \"english\" is `all-MiniLM-L6-v2`. For a full overview of supported languages see bertopic.backend.languages. Select \"multilingual\" to load in the `paraphrase-multilingual-MiniLM-L12-v2` sentence-tranformers model that supports 50+ languages. top_n_words: The number of words per topic to extract. Setting this too high can negatively impact topic embeddings as topics are typically best represented by at most 10 words. n_gram_range: The n-gram range for the CountVectorizer. Advised to keep high values between 1 and 3. More would likely lead to memory issues. NOTE: This param will not be used if you pass in your own CountVectorizer. min_topic_size: The minimum size of the topic. Increasing this value will lead to a lower number of clusters/topics. nr_topics: Specifying the number of topics will reduce the initial number of topics to the value specified. This reduction can take a while as each reduction in topics (-1) activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics using HDBSCAN. low_memory: Sets UMAP low memory to True to make sure less memory is used. NOTE: This is only used in UMAP. For example, if you use PCA instead of UMAP this parameter will not be used. calculate_probabilities: Whether to calculate the probabilities of all topics per document instead of the probability of the assigned topic per document. This could slow down the extraction of topics if you have many documents (> 100_000). Set this only to True if you have a low amount of documents or if you do not mind more computation time. NOTE: If false you cannot use the corresponding visualization method `visualize_probabilities`. diversity: Whether to use MMR to diversify the resulting topic representations. If set to None, MMR will not be used. Accepted values lie between 0 and 1 with 0 being not at all diverse and 1 being very diverse. seed_topic_list: A list of seed words per topic to converge around verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. embedding_model: Use a custom embedding model. The following backends are currently supported * SentenceTransformers * Flair * Spacy * Gensim * USE (TF-Hub) You can also pass in a string that points to one of the following sentence-transformers models: * https://www.sbert.net/docs/pretrained_models.html umap_model: Pass in a UMAP model to be used instead of the default. NOTE: You can also pass in any dimensionality reduction algorithm as long as it has `.fit` and `.transform` functions. hdbscan_model: Pass in a hdbscan.HDBSCAN model to be used instead of the default NOTE: You can also pass in any clustering algorithm as long as it has `.fit` and `.predict` functions along with the `.labels_` variable. vectorizer_model: Pass in a custom `CountVectorizer` instead of the default model. ctfidf_model: Pass in a custom ClassTfidfTransformer instead of the default model. \"\"\" # Topic-based parameters if top_n_words > 30 : raise ValueError ( \"top_n_words should be lower or equal to 30. The preferred value is 10.\" ) self . top_n_words = top_n_words self . min_topic_size = min_topic_size self . nr_topics = nr_topics self . low_memory = low_memory self . calculate_probabilities = calculate_probabilities self . diversity = diversity self . verbose = verbose self . seed_topic_list = seed_topic_list # Embedding model self . language = language if not embedding_model else None self . embedding_model = embedding_model # Vectorizer self . n_gram_range = n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = self . n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () # UMAP or another algorithm that has .fit and .transform functions self . umap_model = umap_model or UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , low_memory = self . low_memory ) # HDBSCAN or another clustering algorithm that has .fit and .predict functions and # the .labels_ variable to extract the labels self . hdbscan_model = hdbscan_model or hdbscan . HDBSCAN ( min_cluster_size = self . min_topic_size , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) # Public attributes self . topics_ = None self . probabilities_ = None self . topic_sizes_ = None self . topic_mapper_ = None self . topic_representations_ = None self . topic_embeddings_ = None self . topic_labels_ = None self . custom_labels_ = None self . representative_docs_ = None self . c_tf_idf_ = None # Private attributes for internal tracking purposes self . _outliers = 1 self . _merged_topics = None if verbose : logger . set_level ( \"DEBUG\" )","title":"__init__()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.__str__","text":"Get a string representation of the current object. Returns: Name Type Description str Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. Source code in bertopic\\_bertopic.py 2851 2852 2853 2854 2855 2856 2857 2858 2859 2860 2861 2862 2863 2864 2865 def __str__ ( self ): \"\"\"Get a string representation of the current object. Returns: str: Human readable representation of the most important model parameters. The parameters that represent models are ignored due to their length. \"\"\" parameters = \"\" for parameter , value in self . get_params () . items (): value = str ( value ) if \"(\" in value and value [ 0 ] != \"(\" : value = value . split ( \"(\" )[ 0 ] + \"(...)\" parameters += f \" { parameter } = { value } , \" return f \"BERTopic( { parameters [: - 2 ] } )\"","title":"__str__()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.find_topics","text":"Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Parameters: Name Type Description Default search_term str the term you want to use to search for topics required top_n int the number of topics to return 5 Returns: Name Type Description similar_topics List [ int ] the most similar topics from high to low similarity List [ float ] the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: topics , similarity = topic_model . find_topics ( \"sports\" , top_n = 5 ) Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. Source code in bertopic\\_bertopic.py 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 def find_topics ( self , search_term : str , top_n : int = 5 ) -> Tuple [ List [ int ], List [ float ]]: \"\"\" Find topics most similar to a search_term Creates an embedding for search_term and compares that with the topic embeddings. The most similar topics are returned along with their similarity values. The search_term can be of any size but since it compares with the topic representation it is advised to keep it below 5 words. Arguments: search_term: the term you want to use to search for topics top_n: the number of topics to return Returns: similar_topics: the most similar topics from high to low similarity: the similarity scores from high to low Examples: You can use the underlying embedding model to find topics that best represent the search term: ```python topics, similarity = topic_model.find_topics(\"sports\", top_n=5) ``` Note that the search query is typically more accurate if the search_term consists of a phrase or multiple words. \"\"\" if self . embedding_model is None : raise Exception ( \"This method can only be used if you did not use custom embeddings.\" ) topic_list = list ( self . topic_representations_ . keys ()) topic_list . sort () # Extract search_term embeddings and compare with topic embeddings search_embedding = self . _extract_embeddings ([ search_term ], method = \"word\" , verbose = False ) . flatten () sims = cosine_similarity ( search_embedding . reshape ( 1 , - 1 ), self . topic_embeddings_ ) . flatten () # Extract topics most similar to search_term ids = np . argsort ( sims )[ - top_n :] similarity = [ sims [ i ] for i in ids ][:: - 1 ] similar_topics = [ topic_list [ index ] for index in ids ][:: - 1 ] return similar_topics , similarity","title":"find_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit","text":"Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) Source code in bertopic\\_bertopic.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit the models (Bert, UMAP, and, HDBSCAN) on a collection of documents and generate topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) ``` \"\"\" self . fit_transform ( documents , embeddings , y ) return self","title":"fit()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.fit_transform","text":"Fit the models on a collection of documents, generate topics, and return the docs with topics Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Returns: Name Type Description predictions List [ int ] Topic predictions for each documents probabilities Union [ np . ndarray , None] The probability of the assigned topic per document. If calculate_probabilities in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) If you want to use your own embeddings, use it as follows: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def fit_transform ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ) -> Tuple [ List [ int ], Union [ np . ndarray , None ]]: \"\"\" Fit the models on a collection of documents, generate topics, and return the docs with topics Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Returns: predictions: Topic predictions for each documents probabilities: The probability of the assigned topic per document. If `calculate_probabilities` in BERTopic is set to True, then it calculates the probabilities of all topics across all documents instead of only the assigned topic. This, however, slows down computation and may increase memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) ``` If you want to use your own embeddings, use it as follows: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs, embeddings) ``` \"\"\" check_documents_type ( documents ) check_embeddings_shape ( embeddings , documents ) documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) logger . info ( \"Transformed documents to Embeddings\" ) else : if self . embedding_model is not None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y ) # Cluster reduced embeddings documents , probabilities = self . _cluster_embeddings ( umap_embeddings , documents ) # Sort and Map Topic IDs by their frequency if not self . nr_topics : documents = self . _sort_mappings_by_frequency ( documents ) # Extract topics by calculating c-TF-IDF self . _extract_topics ( documents ) # Reduce topics if self . nr_topics : documents = self . _reduce_topics ( documents ) self . _map_representative_docs ( original_topics = True ) self . probabilities_ = self . _map_probabilities ( probabilities , original_topics = True ) predictions = documents . Topic . to_list () return predictions , self . probabilities_","title":"fit_transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.generate_topic_labels","text":"Get labels for each topic in a user-defined format Parameters: Name Type Description Default original_labels required nr_words int Top n words per topic to use 3 topic_prefix bool Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the separator True word_length int The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. None separator str The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is \", \" . '_' Returns: Name Type Description topic_labels List [ str ] A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: topic_labels = topic_model . get_topic_labels ( nr_words = 2 , separator = \", \" ) Source code in bertopic\\_bertopic.py 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 def generate_topic_labels ( self , nr_words : int = 3 , topic_prefix : bool = True , word_length : int = None , separator : str = \"_\" ) -> List [ str ]: \"\"\" Get labels for each topic in a user-defined format Arguments: original_labels: nr_words: Top `n` words per topic to use topic_prefix: Whether to use the topic ID as a prefix. If set to True, the topic ID will be separated using the `separator` word_length: The maximum length of each word in the topic label. Some words might be relatively long and setting this value helps to make sure that all labels have relatively similar lengths. separator: The string with which the words and topic prefix will be separated. Underscores are the default but a nice alternative is `\", \"`. Returns: topic_labels: A list of topic labels sorted from the lowest topic ID to the highest. If the topic model was trained using HDBSCAN, the lowest topic ID is -1, otherwise it is 0. Examples: To create our custom topic labels, usage is rather straightforward: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, separator=\", \") ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) topic_labels = [] for topic in unique_topics : words , _ = zip ( * self . get_topic ( topic )) if word_length : words = [ word [: word_length ] for word in words ][: nr_words ] else : words = list ( words )[: nr_words ] if topic_prefix : topic_label = f \" { topic }{ separator } \" + separator . join ( words ) else : topic_label = separator . join ( words ) topic_labels . append ( topic_label ) return topic_labels","title":"generate_topic_labels()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_params","text":"Get parameters for this estimator. Adapted from https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Parameters: Name Type Description Default deep bool bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. False Returns: Name Type Description out Mapping [ str , Any ] Parameter names mapped to their values. Source code in bertopic\\_bertopic.py 2200 2201 2202 2203 2204 2205 2206 2207 2208 2209 2210 2211 2212 2213 2214 2215 2216 2217 2218 2219 2220 2221 def get_params ( self , deep : bool = False ) -> Mapping [ str , Any ]: \"\"\" Get parameters for this estimator. Adapted from: https://github.com/scikit-learn/scikit-learn/blob/b3ea3ed6a/sklearn/base.py#L178 Arguments: deep: bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns: out: Parameter names mapped to their values. \"\"\" out = dict () for key in self . _get_param_names (): value = getattr ( self , key ) if deep and hasattr ( value , 'get_params' ): deep_items = value . get_params () . items () out . update (( key + '__' + k , val ) for k , val in deep_items ) out [ key ] = value return out","title":"get_params()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_representative_docs","text":"Extract representative documents per topic Parameters: Name Type Description Default topic int A specific topic for which you want the representative documents None Returns: Type Description List [ str ] Representative documents of the chosen topic Examples: To extract the representative docs of all topics: representative_docs = topic_model . get_representative_docs () To get the representative docs of a single topic: representative_docs = topic_model . get_representative_docs ( 12 ) Source code in bertopic\\_bertopic.py 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 def get_representative_docs ( self , topic : int = None ) -> List [ str ]: \"\"\" Extract representative documents per topic Arguments: topic: A specific topic for which you want the representative documents Returns: Representative documents of the chosen topic Examples: To extract the representative docs of all topics: ```python representative_docs = topic_model.get_representative_docs() ``` To get the representative docs of a single topic: ```python representative_docs = topic_model.get_representative_docs(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . representative_docs_ [ topic ] else : return self . representative_docs_","title":"get_representative_docs()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic","text":"Return top n words for a specific topic and their c-TF-IDF scores Parameters: Name Type Description Default topic int A specific topic for which you want its representation required Returns: Type Description Union [ Mapping [ str , Tuple [ str , float ]], bool ] The top n words for a specific word and its respective c-TF-IDF scores Examples: topic = topic_model . get_topic ( 12 ) Source code in bertopic\\_bertopic.py 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 def get_topic ( self , topic : int ) -> Union [ Mapping [ str , Tuple [ str , float ]], bool ]: \"\"\" Return top n words for a specific topic and their c-TF-IDF scores Arguments: topic: A specific topic for which you want its representation Returns: The top n words for a specific word and its respective c-TF-IDF scores Examples: ```python topic = topic_model.get_topic(12) ``` \"\"\" check_is_fitted ( self ) if topic in self . topic_representations_ : return self . topic_representations_ [ topic ] else : return False","title":"get_topic()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_freq","text":"Return the the size of topics (descending order) Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Type Description Union [ pd . DataFrame , int ] Either the frequency of a single topic or dataframe with Union [ pd . DataFrame , int ] the frequencies of all topics Examples: To extract the frequency of all topics: frequency = topic_model . get_topic_freq () To get the frequency of a single topic: frequency = topic_model . get_topic_freq ( 12 ) Source code in bertopic\\_bertopic.py 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 def get_topic_freq ( self , topic : int = None ) -> Union [ pd . DataFrame , int ]: \"\"\" Return the the size of topics (descending order) Arguments: topic: A specific topic for which you want the frequency Returns: Either the frequency of a single topic or dataframe with the frequencies of all topics Examples: To extract the frequency of all topics: ```python frequency = topic_model.get_topic_freq() ``` To get the frequency of a single topic: ```python frequency = topic_model.get_topic_freq(12) ``` \"\"\" check_is_fitted ( self ) if isinstance ( topic , int ): return self . topic_sizes_ [ topic ] else : return pd . DataFrame ( self . topic_sizes_ . items (), columns = [ 'Topic' , 'Count' ]) . sort_values ( \"Count\" , ascending = False )","title":"get_topic_freq()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_info","text":"Get information about each topic including its ID, frequency, and name. Parameters: Name Type Description Default topic int A specific topic for which you want the frequency None Returns: Name Type Description info pd . DataFrame The information relating to either a single topic or all topics Examples: info_df = topic_model . get_topic_info () Source code in bertopic\\_bertopic.py 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 def get_topic_info ( self , topic : int = None ) -> pd . DataFrame : \"\"\" Get information about each topic including its ID, frequency, and name. Arguments: topic: A specific topic for which you want the frequency Returns: info: The information relating to either a single topic or all topics Examples: ```python info_df = topic_model.get_topic_info() ``` \"\"\" check_is_fitted ( self ) info = pd . DataFrame ( self . topic_sizes_ . items (), columns = [ \"Topic\" , \"Count\" ]) . sort_values ( \"Topic\" ) info [ \"Name\" ] = info . Topic . map ( self . topic_labels_ ) if self . custom_labels_ is not None : if len ( self . custom_labels_ ) == len ( info ): labels = { topic - self . _outliers : label for topic , label in enumerate ( self . custom_labels_ )} info [ \"CustomName\" ] = info [ \"Topic\" ] . map ( labels ) if topic is not None : info = info . loc [ info . Topic == topic , :] return info . reset_index ( drop = True )","title":"get_topic_info()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topic_tree","text":"Extract the topic tree such that it can be printed Parameters: Name Type Description Default hier_topics pd . DataFrame A dataframe containing the structure of the topic tree. This is the output of topic_model.hierachical_topics() required max_distance float The maximum distance between two topics. This value is based on the Distance column in hier_topics . None tight_layout bool Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. False Returns: Type Description str A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 str The blocks (\u25a0) indicate that the topic is one you can directly access str from topic_model.get_topic . In other words, they are the original un-grouped topics. Examples: # Train model from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Print topic tree tree = topic_model . get_topic_tree ( hierarchical_topics ) print ( tree ) Source code in bertopic\\_bertopic.py 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 @staticmethod def get_topic_tree ( hier_topics : pd . DataFrame , max_distance : float = None , tight_layout : bool = False ) -> str : \"\"\" Extract the topic tree such that it can be printed Arguments: hier_topics: A dataframe containing the structure of the topic tree. This is the output of `topic_model.hierachical_topics()` max_distance: The maximum distance between two topics. This value is based on the Distance column in `hier_topics`. tight_layout: Whether to use a tight layout (narrow width) for easier readability if you have hundreds of topics. Returns: A tree that has the following structure when printed: . . \u2514\u2500health_medical_disease_patients_hiv \u251c\u2500patients_medical_disease_candida_health \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 The blocks (\u25a0) indicate that the topic is one you can directly access from `topic_model.get_topic`. In other words, they are the original un-grouped topics. Examples: ```python # Train model from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) # Print topic tree tree = topic_model.get_topic_tree(hierarchical_topics) print(tree) ``` \"\"\" width = 1 if tight_layout else 4 if max_distance is None : max_distance = hier_topics . Distance . max () + 1 max_original_topic = hier_topics . Parent_ID . astype ( int ) . min () - 1 # Extract mapping from ID to name topic_to_name = dict ( zip ( hier_topics . Child_Left_ID , hier_topics . Child_Left_Name )) topic_to_name . update ( dict ( zip ( hier_topics . Child_Right_ID , hier_topics . Child_Right_Name ))) topic_to_name = { topic : name [: 100 ] for topic , name in topic_to_name . items ()} # Create tree tree = { str ( row [ 1 ] . Parent_ID ): [ str ( row [ 1 ] . Child_Left_ID ), str ( row [ 1 ] . Child_Right_ID )] for row in hier_topics . iterrows ()} def get_tree ( start , tree ): \"\"\" Based on: https://stackoverflow.com/a/51920869/10532563 \"\"\" def _tree ( to_print , start , parent , tree , grandpa = None , indent = \"\" ): # Get distance between merged topics distance = hier_topics . loc [( hier_topics . Child_Left_ID == parent ) | ( hier_topics . Child_Right_ID == parent ), \"Distance\" ] distance = distance . values [ 0 ] if len ( distance ) > 0 else 10 if parent != start : if grandpa is None : to_print += topic_to_name [ parent ] else : if int ( parent ) <= max_original_topic : # Do not append topic ID if they are not merged if distance < max_distance : to_print += \"\u25a0\u2500\u2500\" + topic_to_name [ parent ] + f \" \u2500\u2500 Topic: { parent } \" + \" \\n \" else : to_print += \"O \\n \" else : to_print += topic_to_name [ parent ] + \" \\n \" if parent not in tree : return to_print for child in tree [ parent ][: - 1 ]: to_print += indent + \"\u251c\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \"\u2502\" + \" \" * width ) child = tree [ parent ][ - 1 ] to_print += indent + \"\u2514\" + \"\u2500\" to_print = _tree ( to_print , start , child , tree , parent , indent + \" \" * ( width + 1 )) return to_print to_print = \".\" + \" \\n \" to_print = _tree ( to_print , start , start , tree ) return to_print start = str ( hier_topics . Parent_ID . astype ( int ) . max ()) return get_tree ( start , tree )","title":"get_topic_tree()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.get_topics","text":"Return topics with top n words and their c-TF-IDF score Returns: Type Description Mapping [ str , Tuple [ str , float ]] self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: all_topics = topic_model . get_topics () Source code in bertopic\\_bertopic.py 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 def get_topics ( self ) -> Mapping [ str , Tuple [ str , float ]]: \"\"\" Return topics with top n words and their c-TF-IDF score Returns: self.topic_representations_: The top n words per topic and the corresponding c-TF-IDF score Examples: ```python all_topics = topic_model.get_topics() ``` \"\"\" check_is_fitted ( self ) return self . topic_representations_","title":"get_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.hierarchical_topics","text":"Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using scipy.cluster.hierarchy.linkage . Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Parameters: Name Type Description Default docs List [ int ] The documents you used when calling either fit or fit_transform required linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) None Returns: Name Type Description hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) A custom linkage function can be used as follows: from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Hierarchical topics linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) hierarchical_topics = topic_model . hierarchical_topics ( docs , linkage_function = linkage_function ) Source code in bertopic\\_bertopic.py 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 def hierarchical_topics ( self , docs : List [ int ], linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None ) -> pd . DataFrame : \"\"\" Create a hierarchy of topics To create this hierarchy, BERTopic needs to be already fitted once. Then, a hierarchy is calculated on the distance matrix of the c-TF-IDF representation using `scipy.cluster.hierarchy.linkage`. Based on that hierarchy, we calculate the topic representation at each merged step. This is a local representation, as we only assume that the chosen step is merged and not all others which typically improves the topic representation. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` Returns: hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) hierarchical_topics = topic_model.hierarchical_topics(docs) ``` A custom linkage function can be used as follows: ```python from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) # Hierarchical topics linkage_function = lambda x: sch.linkage(x, 'ward', optimal_ordering=True) hierarchical_topics = topic_model.hierarchical_topics(docs, linkage_function=linkage_function) ``` \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Calculate linkage embeddings = self . c_tf_idf_ [ self . _outliers :] X = distance_function ( embeddings ) Z = linkage_function ( X ) # Calculate basic bag-of-words to be iteratively merged later documents = pd . DataFrame ({ \"Document\" : docs , \"ID\" : range ( len ( docs )), \"Topic\" : self . topics_ }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) documents_per_topic = documents_per_topic . loc [ documents_per_topic . Topic != - 1 , :] documents = self . _preprocess_text ( documents_per_topic . Document . values ) # Scikit-Learn Deprecation: get_feature_names is deprecated in 1.0 # and will be removed in 1.2. Please use get_feature_names_out instead. if version . parse ( sklearn_version ) >= version . parse ( \"1.0.0\" ): words = self . vectorizer_model . get_feature_names_out () else : words = self . vectorizer_model . get_feature_names () bow = self . vectorizer_model . transform ( documents ) # Extract clusters hier_topics = pd . DataFrame ( columns = [ \"Parent_ID\" , \"Parent_Name\" , \"Topics\" , \"Child_Left_ID\" , \"Child_Left_Name\" , \"Child_Right_ID\" , \"Child_Right_Name\" ]) for index in tqdm ( range ( len ( Z ))): # Find clustered documents clusters = sch . fcluster ( Z , t = Z [ index ][ 2 ], criterion = 'distance' ) - self . _outliers cluster_df = pd . DataFrame ({ \"Topic\" : range ( len ( clusters )), \"Cluster\" : clusters }) cluster_df = cluster_df . groupby ( \"Cluster\" ) . agg ({ 'Topic' : lambda x : list ( x )}) . reset_index () nr_clusters = len ( clusters ) # Extract first topic we find to get the set of topics in a merged topic topic = None val = Z [ index ][ 0 ] while topic is None : if val - len ( clusters ) < 0 : topic = int ( val ) else : val = Z [ int ( val - len ( clusters ))][ 0 ] clustered_topics = [ i for i , x in enumerate ( clusters ) if x == clusters [ topic ]] # Group bow per cluster, calculate c-TF-IDF and extract words grouped = csr_matrix ( bow [ clustered_topics ] . sum ( axis = 0 )) c_tf_idf = self . ctfidf_model . transform ( grouped ) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels = [ 0 ]) # Extract parent's name and ID parent_id = index + len ( clusters ) parent_name = \"_\" . join ([ x [ 0 ] for x in words_per_topic [ 0 ]][: 5 ]) # Extract child's name and ID Z_id = Z [ index ][ 0 ] child_left_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_left_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_left_name = hier_topics . iloc [ int ( child_left_id )] . Parent_Name # Extract child's name and ID Z_id = Z [ index ][ 1 ] child_right_id = Z_id if Z_id - nr_clusters < 0 else Z_id - nr_clusters if Z_id - nr_clusters < 0 : child_right_name = \"_\" . join ([ x [ 0 ] for x in self . get_topic ( Z_id )][: 5 ]) else : child_right_name = hier_topics . iloc [ int ( child_right_id )] . Parent_Name # Save results hier_topics . loc [ len ( hier_topics ), :] = [ parent_id , parent_name , clustered_topics , int ( Z [ index ][ 0 ]), child_left_name , int ( Z [ index ][ 1 ]), child_right_name ] hier_topics [ \"Distance\" ] = Z [:, 2 ] hier_topics = hier_topics . sort_values ( \"Parent_ID\" , ascending = False ) hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] = hier_topics [[ \"Parent_ID\" , \"Child_Left_ID\" , \"Child_Right_ID\" ]] . astype ( str ) return hier_topics","title":"hierarchical_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.load","text":"Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the BERTopic file you want to load required embedding_model If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. None Examples: BERTopic . load ( \"my_model\" ) or if you did not save the embedding model: BERTopic . load ( \"my_model\" , embedding_model = \"all-MiniLM-L6-v2\" ) Source code in bertopic\\_bertopic.py 2169 2170 2171 2172 2173 2174 2175 2176 2177 2178 2179 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 2190 2191 2192 2193 2194 2195 2196 2197 2198 @classmethod def load ( cls , path : str , embedding_model = None ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the BERTopic file you want to load embedding_model: If the embedding_model was not saved to save space or to load it in from the cloud, you can load it in by specifying it here. Examples: ```python BERTopic.load(\"my_model\") ``` or if you did not save the embedding model: ```python BERTopic.load(\"my_model\", embedding_model=\"all-MiniLM-L6-v2\") ``` \"\"\" with open ( path , 'rb' ) as file : if embedding_model : topic_model = joblib . load ( file ) topic_model . embedding_model = select_backend ( embedding_model ) else : topic_model = joblib . load ( file ) return topic_model","title":"load()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.merge_topics","text":"Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required topics_to_merge List [ Union [ Iterable [ int ], int ]] Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. required Examples: If you want to merge topics 1, 2, and 3: topics_to_merge = [ 1 , 2 , 3 ] topic_model . merge_topics ( docs , topics_to_merge ) or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: topics_to_merge = [[ 1 , 2 ] [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge ) Source code in bertopic\\_bertopic.py 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 def merge_topics ( self , docs : List [ str ], topics_to_merge : List [ Union [ Iterable [ int ], int ]]) -> None : \"\"\" Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics_to_merge: Either a list of topics or a list of list of topics to merge. For example: [1, 2, 3] will merge topics 1, 2 and 3 [[1, 2], [3, 4]] will merge topics 1 and 2, and separately merge topics 3 and 4. Examples: If you want to merge topics 1, 2, and 3: ```python topics_to_merge = [1, 2, 3] topic_model.merge_topics(docs, topics_to_merge) ``` or if you want to merge topics 1 and 2, and separately merge topics 3 and 4: ```python topics_to_merge = [[1, 2] [3, 4]] topic_model.merge_topics(docs, topics_to_merge) ``` \"\"\" check_is_fitted ( self ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) mapping = { topic : topic for topic in set ( self . topics_ )} if isinstance ( topics_to_merge [ 0 ], int ): for topic in sorted ( topics_to_merge ): mapping [ topic ] = topics_to_merge [ 0 ] elif isinstance ( topics_to_merge [ 0 ], Iterable ): for topic_group in sorted ( topics_to_merge ): for topic in topic_group : mapping [ topic ] = topic_group [ 0 ] else : raise ValueError ( \"Make sure that `topics_to_merge` is either\" \"a list of topics or a list of list of topics.\" ) documents . Topic = documents . Topic . map ( mapping ) self . topic_mapper_ . add_mappings ( mapping ) documents = self . _sort_mappings_by_frequency ( documents ) self . _extract_topics ( documents ) self . _update_topic_size ( documents ) self . _map_representative_docs () self . probabilities_ = self . _map_probabilities ( self . probabilities_ )","title":"merge_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.partial_fit","text":"Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a partial_fit method in order to incrementally train the topic model. Likewise, the bertopic.vectorizers.OnlineCountVectorizer is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: Generate embeddings with a pre-traing language model Incrementally update the dimensionality reduction algorithm with partial_fit Incrementally update the cluster algorithm with partial_fit Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use partial_fit with batches and not single documents for the best performance. Parameters: Name Type Description Default documents List [ str ] A list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model None y Union [ List [ int ], np . ndarray ] The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. None Examples: from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for index in range ( 0 , len ( docs ), 1000 ): topic_model . partial_fit ( docs [ index : index + 1000 ]) Source code in bertopic\\_bertopic.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 def partial_fit ( self , documents : List [ str ], embeddings : np . ndarray = None , y : Union [ List [ int ], np . ndarray ] = None ): \"\"\" Fit BERTopic on a subset of the data and perform online learning with batch-like data. Online topic modeling in BERTopic is performed by using dimensionality reduction and cluster algorithms that support a `partial_fit` method in order to incrementally train the topic model. Likewise, the `bertopic.vectorizers.OnlineCountVectorizer` is used to dynamically update its vocabulary when presented with new data. It has several parameters for modeling decay and updating the representations. In other words, although the main algorithm stays the same, the training procedure now works as follows: For each subset of the data: 1. Generate embeddings with a pre-traing language model 2. Incrementally update the dimensionality reduction algorithm with `partial_fit` 3. Incrementally update the cluster algorithm with `partial_fit` 4. Incrementally update the OnlineCountVectorizer and apply some form of decay Note that it is advised to use `partial_fit` with batches and not single documents for the best performance. Arguments: documents: A list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model y: The target class for (semi)-supervised modeling. Use -1 if no class for a specific instance is specified. Examples: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer from bertopic import BERTopic # Prepare documents docs = fetch_20newsgroups(subset=subset, remove=('headers', 'footers', 'quotes'))[\"data\"] # Prepare sub-models that support online learning umap_model = IncrementalPCA(n_components=5) cluster_model = MiniBatchKMeans(n_clusters=50, random_state=0) vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=.01) topic_model = BERTopic(umap_model=umap_model, hdbscan_model=cluster_model, vectorizer_model=vectorizer_model) # Incrementally fit the topic model by training on 1000 documents at a time for index in range(0, len(docs), 1000): topic_model.partial_fit(docs[index: index+1000]) ``` \"\"\" # Checks check_embeddings_shape ( embeddings , documents ) if not hasattr ( self . hdbscan_model , \"partial_fit\" ): raise ValueError ( \"In order to use `.partial_fit`, the cluster model should have \" \"a `.partial_fit` function.\" ) # Prepare documents if isinstance ( documents , str ): documents = [ documents ] documents = pd . DataFrame ({ \"Document\" : documents , \"ID\" : range ( len ( documents )), \"Topic\" : None }) # Extract embeddings if embeddings is None : if self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) embeddings = self . _extract_embeddings ( documents . Document , method = \"document\" , verbose = self . verbose ) else : if self . embedding_model is not None and self . topic_representations_ is None : self . embedding_model = select_backend ( self . embedding_model , language = self . language ) # Reduce dimensionality if self . seed_topic_list is not None and self . embedding_model is not None : y , embeddings = self . _guided_topic_modeling ( embeddings ) umap_embeddings = self . _reduce_dimensionality ( embeddings , y , partial_fit = True ) # Cluster reduced embeddings documents , self . probabilities_ = self . _cluster_embeddings ( umap_embeddings , documents , partial_fit = True ) topics = documents . Topic . to_list () # Map and find new topics if not self . topic_mapper_ : self . topic_mapper_ = TopicMapper ( topics ) mappings = self . topic_mapper_ . get_mappings () new_topics = set ( topics ) . difference ( set ( mappings . keys ())) new_topic_ids = { topic : max ( mappings . values ()) + index + 1 for index , topic in enumerate ( new_topics )} self . topic_mapper_ . add_new_topics ( new_topic_ids ) updated_mappings = self . topic_mapper_ . get_mappings () updated_topics = [ updated_mappings [ topic ] for topic in topics ] documents [ \"Topic\" ] = updated_topics # Add missing topics (topics that were originally created but are now missing) if self . topic_representations_ : missing_topics = set ( self . topic_representations_ . keys ()) . difference ( set ( updated_topics )) for missing_topic in missing_topics : documents . loc [ len ( documents ), :] = [ \" \" , len ( documents ), missing_topic ] else : missing_topics = {} # Prepare documents documents_per_topic = documents . sort_values ( \"Topic\" ) . groupby ([ 'Topic' ], as_index = False ) updated_topics = documents_per_topic . first () . Topic . astype ( int ) documents_per_topic = documents_per_topic . agg ({ 'Document' : ' ' . join }) # Update topic representations self . c_tf_idf_ , updated_words = self . _c_tf_idf ( documents_per_topic , partial_fit = True ) self . topic_representations_ = self . _extract_words_per_topic ( updated_words , self . c_tf_idf_ , labels = updated_topics ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} # Update topic sizes if len ( missing_topics ) > 0 : documents = documents . iloc [: - len ( missing_topics )] if self . topic_sizes_ is None : self . _update_topic_size ( documents ) else : sizes = documents . groupby ([ 'Topic' ], as_index = False ) . count () for _ , row in sizes . iterrows (): topic = int ( row . Topic ) if self . topic_sizes_ . get ( topic ) is not None and topic not in missing_topics : self . topic_sizes_ [ topic ] += int ( row . Document ) elif self . topic_sizes_ . get ( topic ) is None : self . topic_sizes_ [ topic ] = int ( row . Document ) self . topics_ = documents . Topic . astype ( int ) . tolist () return self","title":"partial_fit()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.reduce_topics","text":"Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Parameters: Name Type Description Default docs List [ str ] The docs you used when calling either fit or fit_transform required nr_topics int The number of topics you want reduced to 20 Updates topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): topic_model . reduce_topics ( docs , nr_topics = 30 ) You can then access the updated topics and probabilities with: topics = topic_model . topics_ probabilities = topic_model . probabilities_ Source code in bertopic\\_bertopic.py 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 def reduce_topics ( self , docs : List [ str ], nr_topics : int = 20 ) -> None : \"\"\" Further reduce the number of topics to nr_topics. The number of topics is further reduced by calculating the c-TF-IDF matrix of the documents and then reducing them by iteratively merging the least frequent topic with the most similar one based on their c-TF-IDF matrices. The topics, their sizes, and representations are updated. Arguments: docs: The docs you used when calling either `fit` or `fit_transform` nr_topics: The number of topics you want reduced to Updates: topics_ : Assigns topics to their merged representations. probabilities_ : Assigns probabilities to their merged representations. Examples: You can further reduce the topics by passing the documents with its topics and probabilities (if they were calculated): ```python topic_model.reduce_topics(docs, nr_topics=30) ``` You can then access the updated topics and probabilities with: ```python topics = topic_model.topics_ probabilities = topic_model.probabilities_ ``` \"\"\" check_is_fitted ( self ) self . nr_topics = nr_topics documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ }) # Reduce number of topics documents = self . _reduce_topics ( documents ) self . _merged_topics = None self . _map_representative_docs () # Map probabilities self . probabilities_ = self . _map_probabilities ( self . probabilities_ ) return self","title":"reduce_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.save","text":"Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required save_embedding_model bool Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. True Examples: topic_model . save ( \"my_model\" ) or if you do not want the embedding_model to be saved locally: topic_model . save ( \"my_model\" , save_embedding_model = False ) Source code in bertopic\\_bertopic.py 2132 2133 2134 2135 2136 2137 2138 2139 2140 2141 2142 2143 2144 2145 2146 2147 2148 2149 2150 2151 2152 2153 2154 2155 2156 2157 2158 2159 2160 2161 2162 2163 2164 2165 2166 2167 def save ( self , path : str , save_embedding_model : bool = True ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save save_embedding_model: Whether to save the embedding model in this class as you might have selected a local model or one that is downloaded automatically from the cloud. Examples: ```python topic_model.save(\"my_model\") ``` or if you do not want the embedding_model to be saved locally: ```python topic_model.save(\"my_model\", save_embedding_model=False) ``` \"\"\" with open ( path , 'wb' ) as file : # This prevents the vectorizer from being too large in size if `min_df` was # set to a value higher than 1 self . vectorizer_model . stop_words_ = None if not save_embedding_model : embedding_model = self . embedding_model self . embedding_model = None joblib . dump ( self , file ) self . embedding_model = embedding_model else : joblib . dump ( self , file )","title":"save()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.set_topic_labels","text":"Set custom topic labels in your fitted BERTopic model Parameters: Name Type Description Default topic_labels Union [ List [ str ], Mapping [ int , str ]] If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of topic ID : topic_label , it can have any number of topics as it will only map the topics found in the dictionary. required Examples: First, we define our topic labels with .get_topic_labels in which we can customize our topic labels: topic_labels = topic_model . get_topic_labels ( nr_words = 2 , topic_prefix = True , word_length = 10 , separator = \", \" ) Then, we pass these topic_labels to our topic model which can be accessed at any time with .custom_labels_ : topic_model . set_topic_labels ( topic_labels ) topic_model . custom_labels_ You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: topic_model . set_topic_labels ({ 0 : \"Space\" , 1 : \"Sports\" , 2 : \"Medicine\" }) topic_model . custom_labels_ Source code in bertopic\\_bertopic.py 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 def set_topic_labels ( self , topic_labels : Union [ List [ str ], Mapping [ int , str ]]) -> None : \"\"\" Set custom topic labels in your fitted BERTopic model Arguments: topic_labels: If a list of topic labels, it should contain the same number of labels as there are topics. This must be ordered from the topic with the lowest ID to the highest ID, including topic -1 if it exists. If a dictionary of `topic ID`: `topic_label`, it can have any number of topics as it will only map the topics found in the dictionary. Examples: First, we define our topic labels with `.get_topic_labels` in which we can customize our topic labels: ```python topic_labels = topic_model.get_topic_labels(nr_words=2, topic_prefix=True, word_length=10, separator=\", \") ``` Then, we pass these `topic_labels` to our topic model which can be accessed at any time with `.custom_labels_`: ```python topic_model.set_topic_labels(topic_labels) topic_model.custom_labels_ ``` You might want to change only a few topic labels instead of all of them. To do so, you can pass a dictionary where the keys are the topic IDs and its keys the topic labels: ```python topic_model.set_topic_labels({0: \"Space\", 1: \"Sports\", 2: \"Medicine\"}) topic_model.custom_labels_ ``` \"\"\" unique_topics = sorted ( set ( self . topics_ )) if isinstance ( topic_labels , dict ): if self . custom_labels_ is not None : original_labels = { topic : label for topic , label in zip ( unique_topics , self . custom_labels_ )} else : info = self . get_topic_info () original_labels = dict ( zip ( info . Topic , info . Name )) custom_labels = [ topic_labels . get ( topic ) if topic_labels . get ( topic ) else original_labels [ topic ] for topic in unique_topics ] elif isinstance ( topic_labels , list ): if len ( topic_labels ) == len ( unique_topics ): custom_labels = topic_labels else : raise ValueError ( \"Make sure that `topic_labels` contains the same number \" \"of labels as that there are topics.\" ) self . custom_labels_ = custom_labels","title":"set_topic_labels()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.topics_over_time","text":"Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required timestamps Union [ List [ str ], List [ int ]] The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. required nr_bins int The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. None datetime_format str The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. None evolution_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp t-1 . This creates evolutionary topic representations. True global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t . True Returns: Name Type Description topics_over_time pd . DataFrame A dataframe that contains the topic, words, and frequency of topic at timestamp t . Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_over_time = topic_model . topics_over_time ( docs , timestamps , nr_bins = 20 ) Source code in bertopic\\_bertopic.py 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 def topics_over_time ( self , docs : List [ str ], timestamps : Union [ List [ str ], List [ int ]], nr_bins : int = None , datetime_format : str = None , evolution_tuning : bool = True , global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics over time To create the topics over time, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each timestamp t. Then, the c-TF-IDF representations at timestamp t are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. NOTE: Make sure to use a limited number of unique timestamps (<100) as the c-TF-IDF representation will be calculated at each single unique timestamp. Having a large number of unique timestamps can take some time to be calculated. Moreover, there aren't many use-cased where you would like to see the difference in topic representations over more than 100 different timestamps. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` timestamps: The timestamp of each document. This can be either a list of strings or ints. If it is a list of strings, then the datetime format will be automatically inferred. If it is a list of ints, then the documents will be ordered by ascending order. nr_bins: The number of bins you want to create for the timestamps. The left interval will be chosen as the timestamp. An additional column will be created with the entire interval. datetime_format: The datetime format of the timestamps if they are strings, eg \u201c%d/%m/%Y\u201d. Set this to None if you want to have it automatically detect the format. See strftime documentation for more information on choices: https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior. evolution_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the c-TF-IDF matrix at timestamp *t-1*. This creates evolutionary topic representations. global_tuning: Fine-tune each topic representation at timestamp *t* by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp *t*. Returns: topics_over_time: A dataframe that contains the topic, words, and frequency of topic at timestamp *t*. Examples: The timestamps variable represent the timestamp of each document. If you have over 100 unique timestamps, it is advised to bin the timestamps as shown below: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_over_time = topic_model.topics_over_time(docs, timestamps, nr_bins=20) ``` \"\"\" check_is_fitted ( self ) check_documents_type ( docs ) documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Timestamps\" : timestamps }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) all_topics = sorted ( list ( documents . Topic . unique ())) all_topics_indices = { topic : index for index , topic in enumerate ( all_topics )} if isinstance ( timestamps [ 0 ], str ): infer_datetime_format = True if not datetime_format else False documents [ \"Timestamps\" ] = pd . to_datetime ( documents [ \"Timestamps\" ], infer_datetime_format = infer_datetime_format , format = datetime_format ) if nr_bins : documents [ \"Bins\" ] = pd . cut ( documents . Timestamps , bins = nr_bins ) documents [ \"Timestamps\" ] = documents . apply ( lambda row : row . Bins . left , 1 ) # Sort documents in chronological order documents = documents . sort_values ( \"Timestamps\" ) timestamps = documents . Timestamps . unique () if len ( timestamps ) > 100 : warnings . warn ( f \"There are more than 100 unique timestamps (i.e., { len ( timestamps ) } ) \" \"which significantly slows down the application. Consider setting `nr_bins` \" \"to a value lower than 100 to speed up calculation. \" ) # For each unique timestamp, create topic representations topics_over_time = [] for index , timestamp in tqdm ( enumerate ( timestamps ), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Timestamps == timestamp , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Timestamps\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) if global_tuning or evolution_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF # matrix at timestamp t-1 if evolution_tuning and index != 0 : current_topics = sorted ( list ( documents_per_topic . Topic . values )) overlapping_topics = sorted ( list ( set ( previous_topics ) . intersection ( set ( current_topics )))) current_overlap_idx = [ current_topics . index ( topic ) for topic in overlapping_topics ] previous_overlap_idx = [ previous_topics . index ( topic ) for topic in overlapping_topics ] c_tf_idf . tolil ()[ current_overlap_idx ] = (( c_tf_idf [ current_overlap_idx ] + previous_c_tf_idf [ previous_overlap_idx ]) / 2.0 ) . tolil () # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : selected_topics = [ all_topics_indices [ topic ] for topic in documents_per_topic . Topic . values ] c_tf_idf = ( global_c_tf_idf [ selected_topics ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Timestamps . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_timestamp = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], timestamp ) for topic , values in words_per_topic . items ()] topics_over_time . extend ( topics_at_timestamp ) if evolution_tuning : previous_topics = sorted ( list ( documents_per_topic . Topic . values )) previous_c_tf_idf = c_tf_idf . copy () return pd . DataFrame ( topics_over_time , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Timestamp\" ])","title":"topics_over_time()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.topics_per_class","text":"Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required classes Union [ List [ int ], List [ str ]] The class of each document. This can be either a list of strings or ints. required global_tuning bool Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. True Returns: Name Type Description topics_per_class pd . DataFrame A dataframe that contains the topic, words, and frequency of topics for each class. Examples: from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , classes ) Source code in bertopic\\_bertopic.py 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 def topics_per_class ( self , docs : List [ str ], classes : Union [ List [ int ], List [ str ]], global_tuning : bool = True ) -> pd . DataFrame : \"\"\" Create topics per class To create the topics per class, BERTopic needs to be already fitted once. From the fitted models, the c-TF-IDF representations are calculate at each class c. Then, the c-TF-IDF representations at class c are averaged with the global c-TF-IDF representations in order to fine-tune the local representations. This can be turned off if the pure representation is needed. NOTE: Make sure to use a limited number of unique classes (<100) as the c-TF-IDF representation will be calculated at each single unique class. Having a large number of unique classes can take some time to be calculated. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` classes: The class of each document. This can be either a list of strings or ints. global_tuning: Fine-tune each topic representation at timestamp t by averaging its c-TF-IDF matrix with the global c-TF-IDF matrix. Turn this off if you want to prevent words in topic representations that could not be found in the documents at timestamp t. Returns: topics_per_class: A dataframe that contains the topic, words, and frequency of topics for each class. Examples: ```python from bertopic import BERTopic topic_model = BERTopic() topics, probs = topic_model.fit_transform(docs) topics_per_class = topic_model.topics_per_class(docs, classes) ``` \"\"\" documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : self . topics_ , \"Class\" : classes }) global_c_tf_idf = normalize ( self . c_tf_idf_ , axis = 1 , norm = 'l1' , copy = False ) # For each unique timestamp, create topic representations topics_per_class = [] for _ , class_ in tqdm ( enumerate ( set ( classes )), disable = not self . verbose ): # Calculate c-TF-IDF representation for a specific timestamp selection = documents . loc [ documents . Class == class_ , :] documents_per_topic = selection . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join , \"Class\" : \"count\" }) c_tf_idf , words = self . _c_tf_idf ( documents_per_topic , fit = False ) # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation # by simply taking the average of the two if global_tuning : c_tf_idf = normalize ( c_tf_idf , axis = 1 , norm = 'l1' , copy = False ) c_tf_idf = ( global_c_tf_idf [ documents_per_topic . Topic . values + self . _outliers ] + c_tf_idf ) / 2.0 # Extract the words per topic labels = sorted ( list ( documents_per_topic . Topic . unique ())) words_per_topic = self . _extract_words_per_topic ( words , c_tf_idf , labels ) topic_frequency = pd . Series ( documents_per_topic . Class . values , index = documents_per_topic . Topic ) . to_dict () # Fill dataframe with results topics_at_class = [( topic , \", \" . join ([ words [ 0 ] for words in values ][: 5 ]), topic_frequency [ topic ], class_ ) for topic , values in words_per_topic . items ()] topics_per_class . extend ( topics_at_class ) topics_per_class = pd . DataFrame ( topics_per_class , columns = [ \"Topic\" , \"Words\" , \"Frequency\" , \"Class\" ]) return topics_per_class","title":"topics_per_class()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.transform","text":"After having fit a model, use transform to predict new instances Parameters: Name Type Description Default documents Union [ str , List [ str ]] A single document or a list of documents to fit on required embeddings np . ndarray Pre-trained document embeddings. These can be used instead of the sentence-transformer model. None Returns: Name Type Description predictions List [ int ] Topic predictions for each documents probabilities np . ndarray The topic probability distribution which is returned by default. If calculate_probabilities in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] topic_model = BERTopic () . fit ( docs ) topics , probs = topic_model . transform ( docs ) If you want to use your own embeddings: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups ( subset = 'all' )[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = True ) # Create topic model topic_model = BERTopic () . fit ( docs , embeddings ) topics , probs = topic_model . transform ( docs , embeddings ) Source code in bertopic\\_bertopic.py 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 def transform ( self , documents : Union [ str , List [ str ]], embeddings : np . ndarray = None ) -> Tuple [ List [ int ], np . ndarray ]: \"\"\" After having fit a model, use transform to predict new instances Arguments: documents: A single document or a list of documents to fit on embeddings: Pre-trained document embeddings. These can be used instead of the sentence-transformer model. Returns: predictions: Topic predictions for each documents probabilities: The topic probability distribution which is returned by default. If `calculate_probabilities` in BERTopic is set to False, then the probabilities are not calculated to speed up computation and decrease memory usage. Examples: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups(subset='all')['data'] topic_model = BERTopic().fit(docs) topics, probs = topic_model.transform(docs) ``` If you want to use your own embeddings: ```python from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Create embeddings docs = fetch_20newsgroups(subset='all')['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=True) # Create topic model topic_model = BERTopic().fit(docs, embeddings) topics, probs = topic_model.transform(docs, embeddings) ``` \"\"\" check_is_fitted ( self ) check_embeddings_shape ( embeddings , documents ) if isinstance ( documents , str ): documents = [ documents ] if embeddings is None : embeddings = self . _extract_embeddings ( documents , method = \"document\" , verbose = self . verbose ) umap_embeddings = self . umap_model . transform ( embeddings ) logger . info ( \"Reduced dimensionality\" ) # Extract predictions and probabilities if it is a HDBSCAN model if isinstance ( self . hdbscan_model , hdbscan . HDBSCAN ): predictions , probabilities = hdbscan . approximate_predict ( self . hdbscan_model , umap_embeddings ) # Calculate probabilities if self . calculate_probabilities : probabilities = hdbscan . membership_vector ( self . hdbscan_model , umap_embeddings ) logger . info ( \"Calculated probabilities with HDBSCAN\" ) else : predictions = self . hdbscan_model . predict ( umap_embeddings ) probabilities = None logger . info ( \"Predicted clusters\" ) # Map probabilities and predictions probabilities = self . _map_probabilities ( probabilities , original_topics = True ) predictions = self . _map_predictions ( predictions ) return predictions , probabilities","title":"transform()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.update_topics","text":"Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A list of topics where each topic is related to a document in docs . Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline None n_gram_range Tuple [ int , int ] The n-gram range for the CountVectorizer. None vectorizer_model CountVectorizer Pass in your own CountVectorizer from scikit-learn None ctfidf_model ClassTfidfTransformer Pass in your own c-TF-IDF model to update the representations None Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: topic_model . update_topics ( docs , n_gram_range = ( 2 , 3 )) You can also use a custom vectorizer to update the representation: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) You can also use this function to change or map the topics to something else. You can update them as follows: topic_model . update_topics ( docs , my_updated_topics ) Source code in bertopic\\_bertopic.py 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 def update_topics ( self , docs : List [ str ], topics : List [ int ] = None , n_gram_range : Tuple [ int , int ] = None , vectorizer_model : CountVectorizer = None , ctfidf_model : ClassTfidfTransformer = None ): \"\"\" Updates the topic representation by recalculating c-TF-IDF with the new parameters as defined in this function. When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. This function allows you to update the topic representation after they have been formed. Arguments: docs: The documents you used when calling either `fit` or `fit_transform` topics: A list of topics where each topic is related to a document in `docs`. Use this variable to change or map the topics. NOTE: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline n_gram_range: The n-gram range for the CountVectorizer. vectorizer_model: Pass in your own CountVectorizer from scikit-learn ctfidf_model: Pass in your own c-TF-IDF model to update the representations Examples: In order to update the topic representation, you will need to first fit the topic model and extract topics from them. Based on these, you can update the representation: ```python topic_model.update_topics(docs, n_gram_range=(2, 3)) ``` You can also use a custom vectorizer to update the representation: ```python from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\") topic_model.update_topics(docs, vectorizer_model=vectorizer_model) ``` You can also use this function to change or map the topics to something else. You can update them as follows: ```python topic_model.update_topics(docs, my_updated_topics) ``` \"\"\" check_is_fitted ( self ) if not n_gram_range : n_gram_range = self . n_gram_range self . vectorizer_model = vectorizer_model or CountVectorizer ( ngram_range = n_gram_range ) self . ctfidf_model = ctfidf_model or ClassTfidfTransformer () if topics is None : topics = self . topics_ labels = None else : labels = sorted ( list ( set ( topics ))) warnings . warn ( \"Using a custom list of topic assignments may lead to errors if \" \"topic reduction techniques are used afterwards. Make sure that \" \"manually assigning topics is the last step in the pipeline.\" ) # Extract words documents = pd . DataFrame ({ \"Document\" : docs , \"Topic\" : topics }) documents_per_topic = documents . groupby ([ 'Topic' ], as_index = False ) . agg ({ 'Document' : ' ' . join }) self . c_tf_idf_ , words = self . _c_tf_idf ( documents_per_topic ) self . topic_representations_ = self . _extract_words_per_topic ( words , labels = labels ) self . _create_topic_vectors () self . topic_labels_ = { key : f \" { key } _\" + \"_\" . join ([ word [ 0 ] for word in values [: 4 ]]) for key , values in self . topic_representations_ . items ()} self . _update_topic_size ( documents )","title":"update_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_barchart","text":"Visualize a barchart of selected topics Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 8 n_words int Number of words to show in a topic 5 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False title str Title of the plot. 'Topic Word Scores' width int The width of each figure. 250 height int The height of each figure. 250 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 2083 2084 2085 2086 2087 2088 2089 2090 2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104 2105 2106 2107 2108 2109 2110 2111 2112 2113 2114 2115 2116 2117 2118 2119 2120 2121 2122 2123 2124 2125 2126 2127 2128 2129 2130 def visualize_barchart ( self , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_barchart ( self , topics = topics , top_n_topics = top_n_topics , n_words = n_words , custom_labels = custom_labels , title = title , width = width , height = height )","title":"visualize_barchart()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_distribution","text":"Visualize the distribution of topic probabilities Parameters: Name Type Description Default probabilities np . ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 600 Examples: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( topic_model . probabilities_ [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( topic_model . probabilities_ [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1901 1902 1903 1904 1905 1906 1907 1908 1909 1910 1911 1912 1913 1914 1915 1916 1917 1918 1919 1920 1921 1922 1923 1924 1925 1926 1927 1928 1929 1930 1931 1932 1933 1934 1935 1936 1937 1938 1939 1940 def visualize_distribution ( self , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(topic_model.probabilities_[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(topic_model.probabilities_[0]) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_distribution ( self , probabilities = probabilities , min_probability = min_probability , custom_labels = custom_labels , width = width , height = height )","title":"visualize_distribution()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_documents","text":"Visualize documents and their topics in 2D Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample float The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_documents ( docs ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 def visualize_documents ( self , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_documents ( self , docs = docs , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , custom_labels = custom_labels , width = width , height = height )","title":"visualize_documents()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_heatmap","text":"Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 800 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 2033 2034 2035 2036 2037 2038 2039 2040 2041 2042 2043 2044 2045 2046 2047 2048 2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062 2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076 2077 2078 2079 2080 2081 def visualize_heatmap ( self , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_heatmap ( self , topics = topics , top_n_topics = top_n_topics , n_clusters = n_clusters , custom_labels = custom_labels , width = width , height = height )","title":"visualize_heatmap()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_hierarchical_documents","text":"Visualize documents and their topics in 2D at different levels of hierarchy Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample Union [ float , int ] The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. True nr_levels int The number of levels to be visualized in the hierarchy. First, the distances in hierarchical_topics.Distance are split in nr_levels lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that nr_levels is equal to the length of hierarchical_topics . 10 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 1681 1682 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 1704 1705 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 1720 1721 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 1736 1737 1738 1739 1740 1741 1742 def visualize_hierarchical_documents ( self , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchical_documents ( self , docs = docs , hierarchical_topics = hierarchical_topics , topics = topics , embeddings = embeddings , reduced_embeddings = reduced_embeddings , sample = sample , hide_annotations = hide_annotations , hide_document_hover = hide_document_hover , nr_levels = nr_levels , custom_labels = custom_labels , width = width , height = height )","title":"visualize_hierarchical_documents()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_hierarchy","text":"Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. Only works if orientation is set to 'left' 1000 height int The height of the figure. Only works if orientation is set to 'bottom' 600 hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both topics and top_n_topics are not set. None linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) NOTE: Make sure to use the same linkage_function as used in topic_model.hierarchical_topics . None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) NOTE: Make sure to use the same distance_function as used in topic_model.hierarchical_topics . None color_threshold int Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. 1 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () If you also want the labels visualized of hierarchical topics, run the following: # Extract hierarchical topics and their representations hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Visualize these representations topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1942 1943 1944 1945 1946 1947 1948 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 def visualize_hierarchy ( self , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" check_is_fitted ( self ) return plotting . visualize_hierarchy ( self , orientation = orientation , topics = topics , top_n_topics = top_n_topics , custom_labels = custom_labels , width = width , height = height , hierarchical_topics = hierarchical_topics , linkage_function = linkage_function , distance_function = distance_function , color_threshold = color_threshold )","title":"visualize_hierarchy()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_term_rank","text":"Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the ranks of all words across all topics simply run: topic_model . visualize_term_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_term_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\_bertopic.py 1744 1745 1746 1747 1748 1749 1750 1751 1752 1753 1754 1755 1756 1757 1758 1759 1760 1761 1762 1763 1764 1765 1766 1767 1768 1769 1770 1771 1772 1773 1774 1775 1776 1777 1778 1779 1780 1781 1782 1783 1784 1785 1786 1787 1788 1789 1790 1791 1792 1793 1794 1795 1796 1797 1798 1799 def visualize_term_rank ( self , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" check_is_fitted ( self ) return plotting . visualize_term_rank ( self , topics = topics , log_scale = log_scale , custom_labels = custom_labels , width = width , height = height )","title":"visualize_term_rank()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics","text":"Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Examples: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 def visualize_topics ( self , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics ( self , topics = topics , top_n_topics = top_n_topics , width = width , height = height )","title":"visualize_topics()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics_over_time","text":"Visualize topics over time Parameters: Name Type Description Default topics_over_time pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1801 1802 1803 1804 1805 1806 1807 1808 1809 1810 1811 1812 1813 1814 1815 1816 1817 1818 1819 1820 1821 1822 1823 1824 1825 1826 1827 1828 1829 1830 1831 1832 1833 1834 1835 1836 1837 1838 1839 1840 1841 1842 1843 1844 1845 1846 1847 1848 1849 def visualize_topics_over_time ( self , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_over_time ( self , topics_over_time = topics_over_time , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height )","title":"visualize_topics_over_time()"},{"location":"api/bertopic.html#bertopic._bertopic.BERTopic.visualize_topics_per_class","text":"Visualize topics per class Parameters: Name Type Description Default topics_per_class pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\_bertopic.py 1851 1852 1853 1854 1855 1856 1857 1858 1859 1860 1861 1862 1863 1864 1865 1866 1867 1868 1869 1870 1871 1872 1873 1874 1875 1876 1877 1878 1879 1880 1881 1882 1883 1884 1885 1886 1887 1888 1889 1890 1891 1892 1893 1894 1895 1896 1897 1898 1899 def visualize_topics_per_class ( self , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` \"\"\" check_is_fitted ( self ) return plotting . visualize_topics_per_class ( self , topics_per_class = topics_per_class , top_n_topics = top_n_topics , topics = topics , normalize_frequency = normalize_frequency , custom_labels = custom_labels , width = width , height = height )","title":"visualize_topics_per_class()"},{"location":"api/ctfidf.html","text":"c-TF-IDF \u00b6 Bases: TfidfTransformer A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. c-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. The frequency of each word x is extracted for each class c and is l1 normalized. This constitutes the term frequency. Then, the term frequency is multiplied with IDF which is the logarithm of 1 plus the average number of words per class A divided by the frequency of word x across all classes. Parameters: Name Type Description Default bm25_weighting bool Uses BM25-inspired idf-weighting procedure instead of the procedure as defined in the c-TF-IDF formula. It uses the following weighting scheme: log(1+((avg_nr_samples - df + 0.5) / (df+0.5))) False reduce_frequent_words bool Takes the square root of the bag-of-words after normalizing the matrix. Helps to reduce the impact of words that appear too frequently. False Examples: transformer = ClassTfidfTransformer () Source code in bertopic\\vectorizers\\_ctfidf.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class ClassTfidfTransformer ( TfidfTransformer ): \"\"\" A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. ![](../algorithm/c-TF-IDF.svg) c-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. The frequency of each word **x** is extracted for each class **c** and is **l1** normalized. This constitutes the term frequency. Then, the term frequency is multiplied with IDF which is the logarithm of 1 plus the average number of words per class **A** divided by the frequency of word **x** across all classes. Arguments: bm25_weighting: Uses BM25-inspired idf-weighting procedure instead of the procedure as defined in the c-TF-IDF formula. It uses the following weighting scheme: `log(1+((avg_nr_samples - df + 0.5) / (df+0.5)))` reduce_frequent_words: Takes the square root of the bag-of-words after normalizing the matrix. Helps to reduce the impact of words that appear too frequently. Examples: ```python transformer = ClassTfidfTransformer() ``` \"\"\" def __init__ ( self , bm25_weighting : bool = False , reduce_frequent_words : bool = False ): self . bm25_weighting = bm25_weighting self . reduce_frequent_words = reduce_frequent_words super ( ClassTfidfTransformer , self ) . __init__ () def fit ( self , X : sp . csr_matrix , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. multiplier: A multiplier for increasing/decreasing certain IDF scores \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape # Calculate the frequency of words across all classes df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) # Calculate the average number of samples as regularization avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) # BM25-inspired weighting procedure if self . bm25_weighting : idf = np . log ( 1 + (( avg_nr_samples - df + 0.5 ) / ( df + 0.5 ))) # Divide the average number of samples by the word frequency # +1 is added to force values to be positive else : idf = np . log (( avg_nr_samples / df ) + 1 ) # Multiplier to increase/decrease certain idf scores if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self def transform ( self , X : sp . csr_matrix ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) if self . reduce_frequent_words : X . data = np . sqrt ( X . data ) X = X * self . _idf_diag return X fit ( X , multiplier = None ) \u00b6 Learn the idf vector (global term weights). Parameters: Name Type Description Default X sp . csr_matrix A matrix of term/token counts. required multiplier np . ndarray A multiplier for increasing/decreasing certain IDF scores None Source code in bertopic\\vectorizers\\_ctfidf.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def fit ( self , X : sp . csr_matrix , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. multiplier: A multiplier for increasing/decreasing certain IDF scores \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape # Calculate the frequency of words across all classes df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) # Calculate the average number of samples as regularization avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) # BM25-inspired weighting procedure if self . bm25_weighting : idf = np . log ( 1 + (( avg_nr_samples - df + 0.5 ) / ( df + 0.5 ))) # Divide the average number of samples by the word frequency # +1 is added to force values to be positive else : idf = np . log (( avg_nr_samples / df ) + 1 ) # Multiplier to increase/decrease certain idf scores if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self transform ( X ) \u00b6 Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X sparse matrix A matrix of term/token counts. required Returns: Name Type Description X sparse matrix A c-TF-IDF matrix Source code in bertopic\\vectorizers\\_ctfidf.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def transform ( self , X : sp . csr_matrix ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) if self . reduce_frequent_words : X . data = np . sqrt ( X . data ) X = X * self . _idf_diag return X","title":"cTFIDF"},{"location":"api/ctfidf.html#c-tf-idf","text":"Bases: TfidfTransformer A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. c-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. The frequency of each word x is extracted for each class c and is l1 normalized. This constitutes the term frequency. Then, the term frequency is multiplied with IDF which is the logarithm of 1 plus the average number of words per class A divided by the frequency of word x across all classes. Parameters: Name Type Description Default bm25_weighting bool Uses BM25-inspired idf-weighting procedure instead of the procedure as defined in the c-TF-IDF formula. It uses the following weighting scheme: log(1+((avg_nr_samples - df + 0.5) / (df+0.5))) False reduce_frequent_words bool Takes the square root of the bag-of-words after normalizing the matrix. Helps to reduce the impact of words that appear too frequently. False Examples: transformer = ClassTfidfTransformer () Source code in bertopic\\vectorizers\\_ctfidf.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 class ClassTfidfTransformer ( TfidfTransformer ): \"\"\" A Class-based TF-IDF procedure using scikit-learns TfidfTransformer as a base. ![](../algorithm/c-TF-IDF.svg) c-TF-IDF can best be explained as a TF-IDF formula adopted for multiple classes by joining all documents per class. Thus, each class is converted to a single document instead of set of documents. The frequency of each word **x** is extracted for each class **c** and is **l1** normalized. This constitutes the term frequency. Then, the term frequency is multiplied with IDF which is the logarithm of 1 plus the average number of words per class **A** divided by the frequency of word **x** across all classes. Arguments: bm25_weighting: Uses BM25-inspired idf-weighting procedure instead of the procedure as defined in the c-TF-IDF formula. It uses the following weighting scheme: `log(1+((avg_nr_samples - df + 0.5) / (df+0.5)))` reduce_frequent_words: Takes the square root of the bag-of-words after normalizing the matrix. Helps to reduce the impact of words that appear too frequently. Examples: ```python transformer = ClassTfidfTransformer() ``` \"\"\" def __init__ ( self , bm25_weighting : bool = False , reduce_frequent_words : bool = False ): self . bm25_weighting = bm25_weighting self . reduce_frequent_words = reduce_frequent_words super ( ClassTfidfTransformer , self ) . __init__ () def fit ( self , X : sp . csr_matrix , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. multiplier: A multiplier for increasing/decreasing certain IDF scores \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape # Calculate the frequency of words across all classes df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) # Calculate the average number of samples as regularization avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) # BM25-inspired weighting procedure if self . bm25_weighting : idf = np . log ( 1 + (( avg_nr_samples - df + 0.5 ) / ( df + 0.5 ))) # Divide the average number of samples by the word frequency # +1 is added to force values to be positive else : idf = np . log (( avg_nr_samples / df ) + 1 ) # Multiplier to increase/decrease certain idf scores if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self def transform ( self , X : sp . csr_matrix ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) if self . reduce_frequent_words : X . data = np . sqrt ( X . data ) X = X * self . _idf_diag return X","title":"c-TF-IDF"},{"location":"api/ctfidf.html#bertopic.vectorizers._ctfidf.ClassTfidfTransformer.fit","text":"Learn the idf vector (global term weights). Parameters: Name Type Description Default X sp . csr_matrix A matrix of term/token counts. required multiplier np . ndarray A multiplier for increasing/decreasing certain IDF scores None Source code in bertopic\\vectorizers\\_ctfidf.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def fit ( self , X : sp . csr_matrix , multiplier : np . ndarray = None ): \"\"\"Learn the idf vector (global term weights). Arguments: X: A matrix of term/token counts. multiplier: A multiplier for increasing/decreasing certain IDF scores \"\"\" X = check_array ( X , accept_sparse = ( 'csr' , 'csc' )) if not sp . issparse ( X ): X = sp . csr_matrix ( X ) dtype = np . float64 if self . use_idf : _ , n_features = X . shape # Calculate the frequency of words across all classes df = np . squeeze ( np . asarray ( X . sum ( axis = 0 ))) # Calculate the average number of samples as regularization avg_nr_samples = int ( X . sum ( axis = 1 ) . mean ()) # BM25-inspired weighting procedure if self . bm25_weighting : idf = np . log ( 1 + (( avg_nr_samples - df + 0.5 ) / ( df + 0.5 ))) # Divide the average number of samples by the word frequency # +1 is added to force values to be positive else : idf = np . log (( avg_nr_samples / df ) + 1 ) # Multiplier to increase/decrease certain idf scores if multiplier is not None : idf = idf * multiplier self . _idf_diag = sp . diags ( idf , offsets = 0 , shape = ( n_features , n_features ), format = 'csr' , dtype = dtype ) return self","title":"fit()"},{"location":"api/ctfidf.html#bertopic.vectorizers._ctfidf.ClassTfidfTransformer.transform","text":"Transform a count-based matrix to c-TF-IDF Parameters: Name Type Description Default X sparse matrix A matrix of term/token counts. required Returns: Name Type Description X sparse matrix A c-TF-IDF matrix Source code in bertopic\\vectorizers\\_ctfidf.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def transform ( self , X : sp . csr_matrix ): \"\"\"Transform a count-based matrix to c-TF-IDF Arguments: X (sparse matrix): A matrix of term/token counts. Returns: X (sparse matrix): A c-TF-IDF matrix \"\"\" if self . use_idf : X = normalize ( X , axis = 1 , norm = 'l1' , copy = False ) if self . reduce_frequent_words : X . data = np . sqrt ( X . data ) X = X * self . _idf_diag return X","title":"transform()"},{"location":"api/mmr.html","text":"Maximal Marginal Relevance \u00b6 Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding np . ndarray The document embeddings required word_embeddings np . ndarray The embeddings of the selected candidate keywords/phrases required words List [ str ] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List [ str ] List[str]: The selected keywords/keyphrases Source code in bertopic\\_mmr.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ str ]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[str]: The selected keywords/keyphrases \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [ words [ idx ] for idx in keywords_idx ]","title":"MMR"},{"location":"api/mmr.html#maximal-marginal-relevance","text":"Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Parameters: Name Type Description Default doc_embedding np . ndarray The document embeddings required word_embeddings np . ndarray The embeddings of the selected candidate keywords/phrases required words List [ str ] The selected candidate keywords/keyphrases required top_n int The number of keywords/keyhprases to return 5 diversity float How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. 0.8 Returns: Type Description List [ str ] List[str]: The selected keywords/keyphrases Source code in bertopic\\_mmr.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def mmr ( doc_embedding : np . ndarray , word_embeddings : np . ndarray , words : List [ str ], top_n : int = 5 , diversity : float = 0.8 ) -> List [ str ]: \"\"\" Calculate Maximal Marginal Relevance (MMR) between candidate keywords and the document. MMR considers the similarity of keywords/keyphrases with the document, along with the similarity of already selected keywords and keyphrases. This results in a selection of keywords that maximize their within diversity with respect to the document. Arguments: doc_embedding: The document embeddings word_embeddings: The embeddings of the selected candidate keywords/phrases words: The selected candidate keywords/keyphrases top_n: The number of keywords/keyhprases to return diversity: How diverse the select keywords/keyphrases are. Values between 0 and 1 with 0 being not diverse at all and 1 being most diverse. Returns: List[str]: The selected keywords/keyphrases \"\"\" # Extract similarity within words, and between words and the document word_doc_similarity = cosine_similarity ( word_embeddings , doc_embedding ) word_similarity = cosine_similarity ( word_embeddings ) # Initialize candidates and already choose best keyword/keyphras keywords_idx = [ np . argmax ( word_doc_similarity )] candidates_idx = [ i for i in range ( len ( words )) if i != keywords_idx [ 0 ]] for _ in range ( top_n - 1 ): # Extract similarities within candidates and # between candidates and selected keywords/phrases candidate_similarities = word_doc_similarity [ candidates_idx , :] target_similarities = np . max ( word_similarity [ candidates_idx ][:, keywords_idx ], axis = 1 ) # Calculate MMR mmr = ( 1 - diversity ) * candidate_similarities - diversity * target_similarities . reshape ( - 1 , 1 ) mmr_idx = candidates_idx [ np . argmax ( mmr )] # Update keywords & candidates keywords_idx . append ( mmr_idx ) candidates_idx . remove ( mmr_idx ) return [ words [ idx ] for idx in keywords_idx ]","title":"Maximal Marginal Relevance"},{"location":"api/onlinecv.html","text":"OnlineCountVectorizer \u00b6 Bases: CountVectorizer An online variant of the CountVectorizer with updating vocabulary. At each .partial_fit , its vocabulary is updated based on any OOV words it might find. Then, .update_bow can be used to track and update the Bag-of-Words representation. These functions are seperated such that the vectorizer can be used in iteration without updating the Bag-of-Words representation can might speed up the fitting process. However, the .update_bow function is used in BERTopic to track changes in the topic representations and allow for decay. This class inherits its parameters and attributes from sklearn.feature_extraction.text.CountVectorizer Parameters: Name Type Description Default decay float A value between [0, 1] to weight the percentage of frequencies the previous bag-of-words should be decreased. For example, a value of .1 will decrease the frequencies in the bag-of-words matrix with 10% at each iteration. None delete_min_df float Delete words eat each iteration from its vocabulary that do not exceed a minimum frequency. This will keep the resulting bag-of-words matrix small such that it does not explode in size with increasing vocabulary. If decay is None then this equals min_df . None **kwargs Set of parameters inherited from: sklearn.feature_extraction.text.CountVectorizer In practice, this means that you can still use parameters from the original CountVectorizer, like stop_words and ngram_range . {} Attributes: Name Type Description X_ scipy.sparse.csr_matrix) The Bag-of-Words representation Examples: from bertopic.vectorizers import OnlineCountVectorizer vectorizer = OnlineCountVectorizer ( stop_words = \"english\" ) for index , doc in enumerate ( my_docs ): vectorizer . partial_fit ( doc ) # Update and clean the bow every 100 iterations: if index % 100 == 0 : X = vectorizer . update_bow () To use the model in BERTopic: from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) References Adapted from: https://github.com/idoshlomo/online_vectorizers Source code in bertopic\\vectorizers\\_online_cv.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class OnlineCountVectorizer ( CountVectorizer ): \"\"\" An online variant of the CountVectorizer with updating vocabulary. At each `.partial_fit`, its vocabulary is updated based on any OOV words it might find. Then, `.update_bow` can be used to track and update the Bag-of-Words representation. These functions are seperated such that the vectorizer can be used in iteration without updating the Bag-of-Words representation can might speed up the fitting process. However, the `.update_bow` function is used in BERTopic to track changes in the topic representations and allow for decay. This class inherits its parameters and attributes from: `sklearn.feature_extraction.text.CountVectorizer` Arguments: decay: A value between [0, 1] to weight the percentage of frequencies the previous bag-of-words should be decreased. For example, a value of `.1` will decrease the frequencies in the bag-of-words matrix with 10% at each iteration. delete_min_df: Delete words eat each iteration from its vocabulary that do not exceed a minimum frequency. This will keep the resulting bag-of-words matrix small such that it does not explode in size with increasing vocabulary. If `decay` is None then this equals `min_df`. **kwargs: Set of parameters inherited from: `sklearn.feature_extraction.text.CountVectorizer` In practice, this means that you can still use parameters from the original CountVectorizer, like `stop_words` and `ngram_range`. Attributes: X_ (scipy.sparse.csr_matrix) : The Bag-of-Words representation Examples: ```python from bertopic.vectorizers import OnlineCountVectorizer vectorizer = OnlineCountVectorizer(stop_words=\"english\") for index, doc in enumerate(my_docs): vectorizer.partial_fit(doc) # Update and clean the bow every 100 iterations: if index % 100 == 0: X = vectorizer.update_bow() ``` To use the model in BERTopic: ```python from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer(stop_words=\"english\") topic_model = BERTopic(vectorizer_model=vectorizer_model) ``` References: Adapted from: https://github.com/idoshlomo/online_vectorizers \"\"\" def __init__ ( self , decay : float = None , delete_min_df : float = None , ** kwargs ): self . decay = decay self . delete_min_df = delete_min_df super ( OnlineCountVectorizer , self ) . __init__ ( ** kwargs ) def partial_fit ( self , raw_documents : List [ str ]) -> None : \"\"\" Perform a partial fit and update vocabulary with OOV tokens Arguments: raw_documents: A list of documents \"\"\" if not hasattr ( self , 'vocabulary_' ): return self . fit ( raw_documents ) analyzer = self . build_analyzer () analyzed_documents = [ analyzer ( doc ) for doc in raw_documents ] new_tokens = set ( chain . from_iterable ( analyzed_documents )) oov_tokens = new_tokens . difference ( set ( self . vocabulary_ . keys ())) if oov_tokens : max_index = max ( self . vocabulary_ . values ()) oov_vocabulary = dict ( zip ( oov_tokens , list ( range ( max_index + 1 , max_index + 1 + len ( oov_tokens ), 1 )))) self . vocabulary_ . update ( oov_vocabulary ) return self def update_bow ( self , raw_documents : List [ str ]) -> csr_matrix : \"\"\" Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if `self.decay` has been set during init. Similarly, words that do not exceed `self.delete_min_df` are removed from its vocabulary and bag-of-words matrix. Arguments: raw_documents: A list of documents Returns: X_: Bag-of-words matrix \"\"\" if hasattr ( self , \"X_\" ): X = self . transform ( raw_documents ) # Add empty columns if new words are found columns = csr_matrix (( self . X_ . shape [ 0 ], X . shape [ 1 ] - self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . hstack ([ self . X_ , columns ]) # Add empty rows if new topics are found rows = csr_matrix (( X . shape [ 0 ] - self . X_ . shape [ 0 ], self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . vstack ([ self . X_ , rows ]) # Decay of BoW matrix if self . decay is not None : self . X_ = self . X_ * ( 1 - self . decay ) self . X_ += X else : self . X_ = self . transform ( raw_documents ) if self . delete_min_df is not None : self . _clean_bow () return self . X_ def _clean_bow ( self ) -> None : \"\"\" Remove words that do not exceed `self.delete_min_df` \"\"\" # Only keep words with a minimum frequency indices = np . where ( self . X_ . sum ( 0 ) >= self . delete_min_df )[ 1 ] indices_dict = { index : index for index in indices } self . X_ = self . X_ [:, indices ] # Update vocabulary with new words new_vocab = {} vocabulary_dict = { v : k for k , v in self . vocabulary_ . items ()} for i , index in enumerate ( indices ): if indices_dict . get ( index ) is not None : new_vocab [ vocabulary_dict [ index ]] = i self . vocabulary_ = new_vocab partial_fit ( raw_documents ) \u00b6 Perform a partial fit and update vocabulary with OOV tokens Parameters: Name Type Description Default raw_documents List [ str ] A list of documents required Source code in bertopic\\vectorizers\\_online_cv.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def partial_fit ( self , raw_documents : List [ str ]) -> None : \"\"\" Perform a partial fit and update vocabulary with OOV tokens Arguments: raw_documents: A list of documents \"\"\" if not hasattr ( self , 'vocabulary_' ): return self . fit ( raw_documents ) analyzer = self . build_analyzer () analyzed_documents = [ analyzer ( doc ) for doc in raw_documents ] new_tokens = set ( chain . from_iterable ( analyzed_documents )) oov_tokens = new_tokens . difference ( set ( self . vocabulary_ . keys ())) if oov_tokens : max_index = max ( self . vocabulary_ . values ()) oov_vocabulary = dict ( zip ( oov_tokens , list ( range ( max_index + 1 , max_index + 1 + len ( oov_tokens ), 1 )))) self . vocabulary_ . update ( oov_vocabulary ) return self update_bow ( raw_documents ) \u00b6 Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if self.decay has been set during init. Similarly, words that do not exceed self.delete_min_df are removed from its vocabulary and bag-of-words matrix. Parameters: Name Type Description Default raw_documents List [ str ] A list of documents required Returns: Name Type Description X_ csr_matrix Bag-of-words matrix Source code in bertopic\\vectorizers\\_online_cv.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def update_bow ( self , raw_documents : List [ str ]) -> csr_matrix : \"\"\" Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if `self.decay` has been set during init. Similarly, words that do not exceed `self.delete_min_df` are removed from its vocabulary and bag-of-words matrix. Arguments: raw_documents: A list of documents Returns: X_: Bag-of-words matrix \"\"\" if hasattr ( self , \"X_\" ): X = self . transform ( raw_documents ) # Add empty columns if new words are found columns = csr_matrix (( self . X_ . shape [ 0 ], X . shape [ 1 ] - self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . hstack ([ self . X_ , columns ]) # Add empty rows if new topics are found rows = csr_matrix (( X . shape [ 0 ] - self . X_ . shape [ 0 ], self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . vstack ([ self . X_ , rows ]) # Decay of BoW matrix if self . decay is not None : self . X_ = self . X_ * ( 1 - self . decay ) self . X_ += X else : self . X_ = self . transform ( raw_documents ) if self . delete_min_df is not None : self . _clean_bow () return self . X_","title":"OnlineCountVectorizer"},{"location":"api/onlinecv.html#onlinecountvectorizer","text":"Bases: CountVectorizer An online variant of the CountVectorizer with updating vocabulary. At each .partial_fit , its vocabulary is updated based on any OOV words it might find. Then, .update_bow can be used to track and update the Bag-of-Words representation. These functions are seperated such that the vectorizer can be used in iteration without updating the Bag-of-Words representation can might speed up the fitting process. However, the .update_bow function is used in BERTopic to track changes in the topic representations and allow for decay. This class inherits its parameters and attributes from sklearn.feature_extraction.text.CountVectorizer Parameters: Name Type Description Default decay float A value between [0, 1] to weight the percentage of frequencies the previous bag-of-words should be decreased. For example, a value of .1 will decrease the frequencies in the bag-of-words matrix with 10% at each iteration. None delete_min_df float Delete words eat each iteration from its vocabulary that do not exceed a minimum frequency. This will keep the resulting bag-of-words matrix small such that it does not explode in size with increasing vocabulary. If decay is None then this equals min_df . None **kwargs Set of parameters inherited from: sklearn.feature_extraction.text.CountVectorizer In practice, this means that you can still use parameters from the original CountVectorizer, like stop_words and ngram_range . {} Attributes: Name Type Description X_ scipy.sparse.csr_matrix) The Bag-of-Words representation Examples: from bertopic.vectorizers import OnlineCountVectorizer vectorizer = OnlineCountVectorizer ( stop_words = \"english\" ) for index , doc in enumerate ( my_docs ): vectorizer . partial_fit ( doc ) # Update and clean the bow every 100 iterations: if index % 100 == 0 : X = vectorizer . update_bow () To use the model in BERTopic: from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) References Adapted from: https://github.com/idoshlomo/online_vectorizers Source code in bertopic\\vectorizers\\_online_cv.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class OnlineCountVectorizer ( CountVectorizer ): \"\"\" An online variant of the CountVectorizer with updating vocabulary. At each `.partial_fit`, its vocabulary is updated based on any OOV words it might find. Then, `.update_bow` can be used to track and update the Bag-of-Words representation. These functions are seperated such that the vectorizer can be used in iteration without updating the Bag-of-Words representation can might speed up the fitting process. However, the `.update_bow` function is used in BERTopic to track changes in the topic representations and allow for decay. This class inherits its parameters and attributes from: `sklearn.feature_extraction.text.CountVectorizer` Arguments: decay: A value between [0, 1] to weight the percentage of frequencies the previous bag-of-words should be decreased. For example, a value of `.1` will decrease the frequencies in the bag-of-words matrix with 10% at each iteration. delete_min_df: Delete words eat each iteration from its vocabulary that do not exceed a minimum frequency. This will keep the resulting bag-of-words matrix small such that it does not explode in size with increasing vocabulary. If `decay` is None then this equals `min_df`. **kwargs: Set of parameters inherited from: `sklearn.feature_extraction.text.CountVectorizer` In practice, this means that you can still use parameters from the original CountVectorizer, like `stop_words` and `ngram_range`. Attributes: X_ (scipy.sparse.csr_matrix) : The Bag-of-Words representation Examples: ```python from bertopic.vectorizers import OnlineCountVectorizer vectorizer = OnlineCountVectorizer(stop_words=\"english\") for index, doc in enumerate(my_docs): vectorizer.partial_fit(doc) # Update and clean the bow every 100 iterations: if index % 100 == 0: X = vectorizer.update_bow() ``` To use the model in BERTopic: ```python from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer(stop_words=\"english\") topic_model = BERTopic(vectorizer_model=vectorizer_model) ``` References: Adapted from: https://github.com/idoshlomo/online_vectorizers \"\"\" def __init__ ( self , decay : float = None , delete_min_df : float = None , ** kwargs ): self . decay = decay self . delete_min_df = delete_min_df super ( OnlineCountVectorizer , self ) . __init__ ( ** kwargs ) def partial_fit ( self , raw_documents : List [ str ]) -> None : \"\"\" Perform a partial fit and update vocabulary with OOV tokens Arguments: raw_documents: A list of documents \"\"\" if not hasattr ( self , 'vocabulary_' ): return self . fit ( raw_documents ) analyzer = self . build_analyzer () analyzed_documents = [ analyzer ( doc ) for doc in raw_documents ] new_tokens = set ( chain . from_iterable ( analyzed_documents )) oov_tokens = new_tokens . difference ( set ( self . vocabulary_ . keys ())) if oov_tokens : max_index = max ( self . vocabulary_ . values ()) oov_vocabulary = dict ( zip ( oov_tokens , list ( range ( max_index + 1 , max_index + 1 + len ( oov_tokens ), 1 )))) self . vocabulary_ . update ( oov_vocabulary ) return self def update_bow ( self , raw_documents : List [ str ]) -> csr_matrix : \"\"\" Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if `self.decay` has been set during init. Similarly, words that do not exceed `self.delete_min_df` are removed from its vocabulary and bag-of-words matrix. Arguments: raw_documents: A list of documents Returns: X_: Bag-of-words matrix \"\"\" if hasattr ( self , \"X_\" ): X = self . transform ( raw_documents ) # Add empty columns if new words are found columns = csr_matrix (( self . X_ . shape [ 0 ], X . shape [ 1 ] - self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . hstack ([ self . X_ , columns ]) # Add empty rows if new topics are found rows = csr_matrix (( X . shape [ 0 ] - self . X_ . shape [ 0 ], self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . vstack ([ self . X_ , rows ]) # Decay of BoW matrix if self . decay is not None : self . X_ = self . X_ * ( 1 - self . decay ) self . X_ += X else : self . X_ = self . transform ( raw_documents ) if self . delete_min_df is not None : self . _clean_bow () return self . X_ def _clean_bow ( self ) -> None : \"\"\" Remove words that do not exceed `self.delete_min_df` \"\"\" # Only keep words with a minimum frequency indices = np . where ( self . X_ . sum ( 0 ) >= self . delete_min_df )[ 1 ] indices_dict = { index : index for index in indices } self . X_ = self . X_ [:, indices ] # Update vocabulary with new words new_vocab = {} vocabulary_dict = { v : k for k , v in self . vocabulary_ . items ()} for i , index in enumerate ( indices ): if indices_dict . get ( index ) is not None : new_vocab [ vocabulary_dict [ index ]] = i self . vocabulary_ = new_vocab","title":"OnlineCountVectorizer"},{"location":"api/onlinecv.html#bertopic.vectorizers._online_cv.OnlineCountVectorizer.partial_fit","text":"Perform a partial fit and update vocabulary with OOV tokens Parameters: Name Type Description Default raw_documents List [ str ] A list of documents required Source code in bertopic\\vectorizers\\_online_cv.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def partial_fit ( self , raw_documents : List [ str ]) -> None : \"\"\" Perform a partial fit and update vocabulary with OOV tokens Arguments: raw_documents: A list of documents \"\"\" if not hasattr ( self , 'vocabulary_' ): return self . fit ( raw_documents ) analyzer = self . build_analyzer () analyzed_documents = [ analyzer ( doc ) for doc in raw_documents ] new_tokens = set ( chain . from_iterable ( analyzed_documents )) oov_tokens = new_tokens . difference ( set ( self . vocabulary_ . keys ())) if oov_tokens : max_index = max ( self . vocabulary_ . values ()) oov_vocabulary = dict ( zip ( oov_tokens , list ( range ( max_index + 1 , max_index + 1 + len ( oov_tokens ), 1 )))) self . vocabulary_ . update ( oov_vocabulary ) return self","title":"partial_fit()"},{"location":"api/onlinecv.html#bertopic.vectorizers._online_cv.OnlineCountVectorizer.update_bow","text":"Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if self.decay has been set during init. Similarly, words that do not exceed self.delete_min_df are removed from its vocabulary and bag-of-words matrix. Parameters: Name Type Description Default raw_documents List [ str ] A list of documents required Returns: Name Type Description X_ csr_matrix Bag-of-words matrix Source code in bertopic\\vectorizers\\_online_cv.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def update_bow ( self , raw_documents : List [ str ]) -> csr_matrix : \"\"\" Create or update the bag-of-words matrix Update the bag-of-words matrix by adding the newly transformed documents. This may add empty columns if new words are found and/or add empty rows if new topics are found. During this process, the previous bag-of-words matrix might be decayed if `self.decay` has been set during init. Similarly, words that do not exceed `self.delete_min_df` are removed from its vocabulary and bag-of-words matrix. Arguments: raw_documents: A list of documents Returns: X_: Bag-of-words matrix \"\"\" if hasattr ( self , \"X_\" ): X = self . transform ( raw_documents ) # Add empty columns if new words are found columns = csr_matrix (( self . X_ . shape [ 0 ], X . shape [ 1 ] - self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . hstack ([ self . X_ , columns ]) # Add empty rows if new topics are found rows = csr_matrix (( X . shape [ 0 ] - self . X_ . shape [ 0 ], self . X_ . shape [ 1 ]), dtype = int ) self . X_ = sparse . vstack ([ self . X_ , rows ]) # Decay of BoW matrix if self . decay is not None : self . X_ = self . X_ * ( 1 - self . decay ) self . X_ += X else : self . X_ = self . transform ( raw_documents ) if self . delete_min_df is not None : self . _clean_bow () return self . X_","title":"update_bow()"},{"location":"api/backends/base.html","text":"BaseEmbedder \u00b6 The Base Embedder used for creating embedding models Parameters: Name Type Description Default embedding_model The main embedding model to be used for extracting document and word embedding None word_embedding_model The embedding model used for extracting word embeddings only. If this model is selected, then the embedding_model is purely used for creating document embeddings. None Source code in bertopic\\backend\\_base.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class BaseEmbedder : \"\"\" The Base Embedder used for creating embedding models Arguments: embedding_model: The main embedding model to be used for extracting document and word embedding word_embedding_model: The embedding model used for extracting word embeddings only. If this model is selected, then the `embedding_model` is purely used for creating document embeddings. \"\"\" def __init__ ( self , embedding_model = None , word_embedding_model = None ): self . embedding_model = embedding_model self . word_embedding_model = word_embedding_model def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose ) def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose ) embed ( documents , verbose = False ) \u00b6 Embed a list of n documents/words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default documents List [ str ] A list of documents or words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document/words embeddings with shape (n, m) with n documents/words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass embed_documents ( document , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List [ str ] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document embeddings with shape (n, m) with n documents np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose ) embed_words ( words , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List [ str ] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Word embeddings with shape (n, m) with n words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose )","title":"Base"},{"location":"api/backends/base.html#baseembedder","text":"The Base Embedder used for creating embedding models Parameters: Name Type Description Default embedding_model The main embedding model to be used for extracting document and word embedding None word_embedding_model The embedding model used for extracting word embeddings only. If this model is selected, then the embedding_model is purely used for creating document embeddings. None Source code in bertopic\\backend\\_base.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 class BaseEmbedder : \"\"\" The Base Embedder used for creating embedding models Arguments: embedding_model: The main embedding model to be used for extracting document and word embedding word_embedding_model: The embedding model used for extracting word embeddings only. If this model is selected, then the `embedding_model` is purely used for creating document embeddings. \"\"\" def __init__ ( self , embedding_model = None , word_embedding_model = None ): self . embedding_model = embedding_model self . word_embedding_model = word_embedding_model def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose ) def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose )","title":"BaseEmbedder"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed","text":"Embed a list of n documents/words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default documents List [ str ] A list of documents or words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document/words embeddings with shape (n, m) with n documents/words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def embed ( self , documents : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n documents/words into an n-dimensional matrix of embeddings Arguments: documents: A list of documents or words to be embedded verbose: Controls the verbosity of the process Returns: Document/words embeddings with shape (n, m) with `n` documents/words that each have an embeddings size of `m` \"\"\" pass","title":"embed()"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed_documents","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List [ str ] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document embeddings with shape (n, m) with n documents np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embed ( document , verbose )","title":"embed_documents()"},{"location":"api/backends/base.html#bertopic.backend._base.BaseEmbedder.embed_words","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List [ str ] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Word embeddings with shape (n, m) with n words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . embed ( words , verbose )","title":"embed_words()"},{"location":"api/backends/word_doc.html","text":"WordDocEmbedder \u00b6 Bases: BaseEmbedder Combine a document- and word-level embedder Source code in bertopic\\backend\\_word_doc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class WordDocEmbedder ( BaseEmbedder ): \"\"\" Combine a document- and word-level embedder \"\"\" def __init__ ( self , embedding_model , word_embedding_model ): super () . __init__ () self . embedding_model = select_backend ( embedding_model ) self . word_embedding_model = select_backend ( word_embedding_model ) def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose ) def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose ) embed_documents ( document , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List [ str ] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document embeddings with shape (n, m) with n documents np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose ) embed_words ( words , verbose = False ) \u00b6 Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List [ str ] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Word embeddings with shape (n, m) with n words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose )","title":"Word Doc"},{"location":"api/backends/word_doc.html#worddocembedder","text":"Bases: BaseEmbedder Combine a document- and word-level embedder Source code in bertopic\\backend\\_word_doc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 class WordDocEmbedder ( BaseEmbedder ): \"\"\" Combine a document- and word-level embedder \"\"\" def __init__ ( self , embedding_model , word_embedding_model ): super () . __init__ () self . embedding_model = select_backend ( embedding_model ) self . word_embedding_model = select_backend ( word_embedding_model ) def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose ) def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose )","title":"WordDocEmbedder"},{"location":"api/backends/word_doc.html#bertopic.backend._word_doc.WordDocEmbedder.embed_documents","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default document List [ str ] A list of documents to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Document embeddings with shape (n, m) with n documents np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def embed_documents ( self , document : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: document: A list of documents to be embedded verbose: Controls the verbosity of the process Returns: Document embeddings with shape (n, m) with `n` documents that each have an embeddings size of `m` \"\"\" return self . embedding_model . embed ( document , verbose )","title":"embed_documents()"},{"location":"api/backends/word_doc.html#bertopic.backend._word_doc.WordDocEmbedder.embed_words","text":"Embed a list of n words into an n-dimensional matrix of embeddings Parameters: Name Type Description Default words List [ str ] A list of words to be embedded required verbose bool Controls the verbosity of the process False Returns: Type Description np . ndarray Word embeddings with shape (n, m) with n words np . ndarray that each have an embeddings size of m Source code in bertopic\\backend\\_word_doc.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def embed_words ( self , words : List [ str ], verbose : bool = False ) -> np . ndarray : \"\"\" Embed a list of n words into an n-dimensional matrix of embeddings Arguments: words: A list of words to be embedded verbose: Controls the verbosity of the process Returns: Word embeddings with shape (n, m) with `n` words that each have an embeddings size of `m` \"\"\" return self . word_embedding_model . embed ( words , verbose )","title":"embed_words()"},{"location":"api/plotting/barchart.html","text":"Barchart \u00b6 Visualize a barchart of selected topics Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 8 n_words int Number of words to show in a topic 5 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False title str Title of the plot. 'Topic Word Scores' width int The width of each figure. 250 height int The height of each figure. 250 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_barchart.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def visualize_barchart ( topic_model , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/bar_chart.html\" style=\"width:1100px; height: 660px; border: 0px;\"\"></iframe> \"\"\" colors = itertools . cycle ([ \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" , \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" ]) # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()[ 0 : 6 ]) # Initialize figure if topic_model . custom_labels_ is not None and custom_labels : subplot_titles = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in topics ] else : subplot_titles = [ f \"Topic { topic } \" for topic in topics ] columns = 4 rows = int ( np . ceil ( len ( topics ) / columns )) fig = make_subplots ( rows = rows , cols = columns , shared_xaxes = False , horizontal_spacing = .1 , vertical_spacing = .4 / rows if rows > 1 else 0 , subplot_titles = subplot_titles ) # Add barchart for each topic row = 1 column = 1 for topic in topics : words = [ word + \" \" for word , _ in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] scores = [ score for _ , score in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] fig . add_trace ( go . Bar ( x = scores , y = words , orientation = 'h' , marker_color = next ( colors )), row = row , col = column ) if column == columns : column = 1 row += 1 else : column += 1 # Stylize graph fig . update_layout ( template = \"plotly_white\" , showlegend = False , title = { 'text' : f \"<b> { title } \" , 'x' : .5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width * 4 , height = height * rows if rows > 1 else height * 1.3 , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) return fig","title":"Barchart"},{"location":"api/plotting/barchart.html#barchart","text":"Visualize a barchart of selected topics Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. 8 n_words int Number of words to show in a topic 5 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False title str Title of the plot. 'Topic Word Scores' width int The width of each figure. 250 height int The height of each figure. 250 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the barchart of selected topics simply run: topic_model . visualize_barchart () Or if you want to save the resulting figure: fig = topic_model . visualize_barchart () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_barchart.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def visualize_barchart ( topic_model , topics : List [ int ] = None , top_n_topics : int = 8 , n_words : int = 5 , custom_labels : bool = False , title : str = \"Topic Word Scores\" , width : int = 250 , height : int = 250 ) -> go . Figure : \"\"\" Visualize a barchart of selected topics Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_words: Number of words to show in a topic custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. title: Title of the plot. width: The width of each figure. height: The height of each figure. Returns: fig: A plotly figure Examples: To visualize the barchart of selected topics simply run: ```python topic_model.visualize_barchart() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_barchart() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/bar_chart.html\" style=\"width:1100px; height: 660px; border: 0px;\"\"></iframe> \"\"\" colors = itertools . cycle ([ \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" , \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" ]) # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()[ 0 : 6 ]) # Initialize figure if topic_model . custom_labels_ is not None and custom_labels : subplot_titles = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in topics ] else : subplot_titles = [ f \"Topic { topic } \" for topic in topics ] columns = 4 rows = int ( np . ceil ( len ( topics ) / columns )) fig = make_subplots ( rows = rows , cols = columns , shared_xaxes = False , horizontal_spacing = .1 , vertical_spacing = .4 / rows if rows > 1 else 0 , subplot_titles = subplot_titles ) # Add barchart for each topic row = 1 column = 1 for topic in topics : words = [ word + \" \" for word , _ in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] scores = [ score for _ , score in topic_model . get_topic ( topic )][: n_words ][:: - 1 ] fig . add_trace ( go . Bar ( x = scores , y = words , orientation = 'h' , marker_color = next ( colors )), row = row , col = column ) if column == columns : column = 1 row += 1 else : column += 1 # Stylize graph fig . update_layout ( template = \"plotly_white\" , showlegend = False , title = { 'text' : f \"<b> { title } \" , 'x' : .5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width * 4 , height = height * rows if rows > 1 else height * 1.3 , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) return fig","title":"Barchart"},{"location":"api/plotting/distribution.html","text":"Distribution \u00b6 Visualize the distribution of topic probabilities Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required probabilities np . ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 600 Examples: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_distribution.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def visualize_distribution ( topic_model , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: topic_model: A fitted BERTopic instance. probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/probabilities.html\" style=\"width:1000px; height: 500px; border: 0px;\"\"></iframe> \"\"\" if len ( probabilities . shape ) != 1 : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities of all topics. \" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not topic_model . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels if topic_model . custom_labels_ is not None and custom_labels : labels = [ topic_model . custom_labels_ [ idx + topic_model . _outliers ] for idx in labels_idx ] else : labels = [] for idx in labels_idx : words = topic_model . get_topic ( idx ) if words : label = [ word [ 0 ] for word in words [: 5 ]] label = f \"<b>Topic { idx } </b>: { '_' . join ( label ) } \" label = label [: 40 ] + \"...\" if len ( label ) > 40 else label labels . append ( label ) else : vals . remove ( probabilities [ idx ]) # Create Figure fig = go . Figure ( go . Bar ( x = vals , y = labels , marker = dict ( color = '#C8D2D7' , line = dict ( color = '#6E8484' , width = 1 ), ), orientation = 'h' ) ) fig . update_layout ( xaxis_title = \"Probability\" , title = { 'text' : \"<b>Topic Probability Distribution\" , 'y' : .95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) return fig","title":"Distribution"},{"location":"api/plotting/distribution.html#distribution","text":"Visualize the distribution of topic probabilities Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required probabilities np . ndarray An array of probability scores required min_probability float The minimum probability score to visualize. All others are ignored. 0.015 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 600 Examples: Make sure to fit the model before and only input the probabilities of a single document: topic_model . visualize_distribution ( probabilities [ 0 ]) Or if you want to save the resulting figure: fig = topic_model . visualize_distribution ( probabilities [ 0 ]) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_distribution.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def visualize_distribution ( topic_model , probabilities : np . ndarray , min_probability : float = 0.015 , custom_labels : bool = False , width : int = 800 , height : int = 600 ) -> go . Figure : \"\"\" Visualize the distribution of topic probabilities Arguments: topic_model: A fitted BERTopic instance. probabilities: An array of probability scores min_probability: The minimum probability score to visualize. All others are ignored. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: Make sure to fit the model before and only input the probabilities of a single document: ```python topic_model.visualize_distribution(probabilities[0]) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_distribution(probabilities[0]) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/probabilities.html\" style=\"width:1000px; height: 500px; border: 0px;\"\"></iframe> \"\"\" if len ( probabilities . shape ) != 1 : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities of all topics. \" ) if len ( probabilities [ probabilities > min_probability ]) == 0 : raise ValueError ( \"There are no values where `min_probability` is higher than the \" \"probabilities that were supplied. Lower `min_probability` to prevent this error.\" ) if not topic_model . calculate_probabilities : raise ValueError ( \"This visualization cannot be used if you have set `calculate_probabilities` to False \" \"as it uses the topic probabilities. \" ) # Get values and indices equal or exceed the minimum probability labels_idx = np . argwhere ( probabilities >= min_probability ) . flatten () vals = probabilities [ labels_idx ] . tolist () # Create labels if topic_model . custom_labels_ is not None and custom_labels : labels = [ topic_model . custom_labels_ [ idx + topic_model . _outliers ] for idx in labels_idx ] else : labels = [] for idx in labels_idx : words = topic_model . get_topic ( idx ) if words : label = [ word [ 0 ] for word in words [: 5 ]] label = f \"<b>Topic { idx } </b>: { '_' . join ( label ) } \" label = label [: 40 ] + \"...\" if len ( label ) > 40 else label labels . append ( label ) else : vals . remove ( probabilities [ idx ]) # Create Figure fig = go . Figure ( go . Bar ( x = vals , y = labels , marker = dict ( color = '#C8D2D7' , line = dict ( color = '#6E8484' , width = 1 ), ), orientation = 'h' ) ) fig . update_layout ( xaxis_title = \"Probability\" , title = { 'text' : \"<b>Topic Probability Distribution\" , 'y' : .95 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) return fig","title":"Distribution"},{"location":"api/plotting/documents.html","text":"Documents \u00b6 Visualize documents and their topics in 2D Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample float The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_documents ( docs ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_documents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def visualize_documents ( topic_model , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ): \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" topic_per_doc = topic_model . topics_ # Sample the data to optimize for visualization and dimensionality reduction if sample is None or sample > 1 : sample = 1 indices = [] for topic in set ( topic_per_doc ): s = np . where ( np . array ( topic_per_doc ) == topic )[ 0 ] size = len ( s ) if len ( s ) < 100 else int ( len ( s ) * sample ) indices . extend ( np . random . choice ( s , size = size , replace = False )) indices = np . array ( indices ) df = pd . DataFrame ({ \"topic\" : np . array ( topic_per_doc )[ indices ]}) df [ \"doc\" ] = [ docs [ index ] for index in indices ] df [ \"topic\" ] = [ topic_per_doc [ index ] for index in indices ] # Extract embeddings if not already done if sample is None : if embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) else : embeddings_to_reduce = embeddings else : if embeddings is not None : embeddings_to_reduce = embeddings [ indices ] elif embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) # Reduce input embeddings if reduced_embeddings is None : umap_model = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit ( embeddings_to_reduce ) embeddings_2d = umap_model . embedding_ elif sample is not None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings [ indices ] elif sample is None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings unique_topics = set ( topic_per_doc ) if topics is None : topics = unique_topics # Combine data df [ \"x\" ] = embeddings_2d [:, 0 ] df [ \"y\" ] = embeddings_2d [:, 1 ] # Prepare text and names if topic_model . custom_labels_ is not None and custom_labels : names = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in unique_topics ] else : names = [ f \" { topic } _\" + \"_\" . join ([ word for word , value in topic_model . get_topic ( topic )][: 3 ]) for topic in unique_topics ] # Visualize fig = go . Figure () # Outliers and non-selected topics non_selected_topics = set ( unique_topics ) . difference ( topics ) if len ( non_selected_topics ) == 0 : non_selected_topics = [ - 1 ] selection = df . loc [ df . topic . isin ( non_selected_topics ), :] selection [ \"text\" ] = \"\" selection . loc [ len ( selection ), :] = [ None , None , selection . x . mean (), selection . y . mean (), \"Other documents\" ] fig . add_trace ( go . Scattergl ( x = selection . x , y = selection . y , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , mode = 'markers+text' , name = \"other\" , showlegend = False , marker = dict ( color = '#CFD8DC' , size = 5 , opacity = 0.5 ) ) ) # Selected topics for name , topic in zip ( names , unique_topics ): if topic in topics and topic != - 1 : selection = df . loc [ df . topic == topic , :] selection [ \"text\" ] = \"\" if not hide_annotations : selection . loc [ len ( selection ), :] = [ None , None , selection . x . mean (), selection . y . mean (), name ] fig . add_trace ( go . Scattergl ( x = selection . x , y = selection . y , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , text = selection . text , mode = 'markers+text' , name = name , textfont = dict ( size = 12 , ), marker = dict ( size = 5 , opacity = 0.5 ) ) ) # Add grid in a 'plus' shape x_range = ( df . x . min () - abs (( df . x . min ()) * .15 ), df . x . max () + abs (( df . x . max ()) * .15 )) y_range = ( df . y . min () - abs (( df . y . min ()) * .15 ), df . y . max () + abs (( df . y . max ()) * .15 )) fig . add_shape ( type = \"line\" , x0 = sum ( x_range ) / 2 , y0 = y_range [ 0 ], x1 = sum ( x_range ) / 2 , y1 = y_range [ 1 ], line = dict ( color = \"#CFD8DC\" , width = 2 )) fig . add_shape ( type = \"line\" , x0 = x_range [ 0 ], y0 = sum ( y_range ) / 2 , x1 = x_range [ 1 ], y1 = sum ( y_range ) / 2 , line = dict ( color = \"#9E9E9E\" , width = 2 )) fig . add_annotation ( x = x_range [ 0 ], y = sum ( y_range ) / 2 , text = \"D1\" , showarrow = False , yshift = 10 ) fig . add_annotation ( y = y_range [ 1 ], x = sum ( x_range ) / 2 , text = \"D2\" , showarrow = False , xshift = 10 ) # Stylize layout fig . update_layout ( template = \"simple_white\" , title = { 'text' : \"<b>Documents and Topics\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height ) fig . update_xaxes ( visible = False ) fig . update_yaxes ( visible = False ) return fig","title":"Documents"},{"location":"api/plotting/documents.html#documents","text":"Visualize documents and their topics in 2D Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required docs List [ str ] The documents you used when calling either fit or fit_transform required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample float The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_documents ( docs ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_documents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def visualize_documents ( topic_model , docs : List [ str ], topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : float = None , hide_annotations : bool = False , hide_document_hover : bool = False , custom_labels : bool = False , width : int = 1200 , height : int = 750 ): \"\"\" Visualize documents and their topics in 2D Arguments: topic_model: A fitted BERTopic instance. docs: The documents you used when calling either `fit` or `fit_transform` topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualization. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_documents(docs) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic topic_model = BERTopic().fit(docs, embeddings) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_documents(docs, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/documents.html\" style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe> \"\"\" topic_per_doc = topic_model . topics_ # Sample the data to optimize for visualization and dimensionality reduction if sample is None or sample > 1 : sample = 1 indices = [] for topic in set ( topic_per_doc ): s = np . where ( np . array ( topic_per_doc ) == topic )[ 0 ] size = len ( s ) if len ( s ) < 100 else int ( len ( s ) * sample ) indices . extend ( np . random . choice ( s , size = size , replace = False )) indices = np . array ( indices ) df = pd . DataFrame ({ \"topic\" : np . array ( topic_per_doc )[ indices ]}) df [ \"doc\" ] = [ docs [ index ] for index in indices ] df [ \"topic\" ] = [ topic_per_doc [ index ] for index in indices ] # Extract embeddings if not already done if sample is None : if embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) else : embeddings_to_reduce = embeddings else : if embeddings is not None : embeddings_to_reduce = embeddings [ indices ] elif embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) # Reduce input embeddings if reduced_embeddings is None : umap_model = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit ( embeddings_to_reduce ) embeddings_2d = umap_model . embedding_ elif sample is not None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings [ indices ] elif sample is None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings unique_topics = set ( topic_per_doc ) if topics is None : topics = unique_topics # Combine data df [ \"x\" ] = embeddings_2d [:, 0 ] df [ \"y\" ] = embeddings_2d [:, 1 ] # Prepare text and names if topic_model . custom_labels_ is not None and custom_labels : names = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in unique_topics ] else : names = [ f \" { topic } _\" + \"_\" . join ([ word for word , value in topic_model . get_topic ( topic )][: 3 ]) for topic in unique_topics ] # Visualize fig = go . Figure () # Outliers and non-selected topics non_selected_topics = set ( unique_topics ) . difference ( topics ) if len ( non_selected_topics ) == 0 : non_selected_topics = [ - 1 ] selection = df . loc [ df . topic . isin ( non_selected_topics ), :] selection [ \"text\" ] = \"\" selection . loc [ len ( selection ), :] = [ None , None , selection . x . mean (), selection . y . mean (), \"Other documents\" ] fig . add_trace ( go . Scattergl ( x = selection . x , y = selection . y , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , mode = 'markers+text' , name = \"other\" , showlegend = False , marker = dict ( color = '#CFD8DC' , size = 5 , opacity = 0.5 ) ) ) # Selected topics for name , topic in zip ( names , unique_topics ): if topic in topics and topic != - 1 : selection = df . loc [ df . topic == topic , :] selection [ \"text\" ] = \"\" if not hide_annotations : selection . loc [ len ( selection ), :] = [ None , None , selection . x . mean (), selection . y . mean (), name ] fig . add_trace ( go . Scattergl ( x = selection . x , y = selection . y , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , text = selection . text , mode = 'markers+text' , name = name , textfont = dict ( size = 12 , ), marker = dict ( size = 5 , opacity = 0.5 ) ) ) # Add grid in a 'plus' shape x_range = ( df . x . min () - abs (( df . x . min ()) * .15 ), df . x . max () + abs (( df . x . max ()) * .15 )) y_range = ( df . y . min () - abs (( df . y . min ()) * .15 ), df . y . max () + abs (( df . y . max ()) * .15 )) fig . add_shape ( type = \"line\" , x0 = sum ( x_range ) / 2 , y0 = y_range [ 0 ], x1 = sum ( x_range ) / 2 , y1 = y_range [ 1 ], line = dict ( color = \"#CFD8DC\" , width = 2 )) fig . add_shape ( type = \"line\" , x0 = x_range [ 0 ], y0 = sum ( y_range ) / 2 , x1 = x_range [ 1 ], y1 = sum ( y_range ) / 2 , line = dict ( color = \"#9E9E9E\" , width = 2 )) fig . add_annotation ( x = x_range [ 0 ], y = sum ( y_range ) / 2 , text = \"D1\" , showarrow = False , yshift = 10 ) fig . add_annotation ( y = y_range [ 1 ], x = sum ( x_range ) / 2 , text = \"D2\" , showarrow = False , xshift = 10 ) # Stylize layout fig . update_layout ( template = \"simple_white\" , title = { 'text' : \"<b>Documents and Topics\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height ) fig . update_xaxes ( visible = False ) fig . update_yaxes ( visible = False ) return fig","title":"Documents"},{"location":"api/plotting/dtm.html","text":"DTM \u00b6 Visualize topics over time Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_over_time pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_over_time.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def visualize_topics_over_time ( topic_model , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topic_model: A fitted BERTopic instance. topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/trump.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : selected_topics = list ( topics ) elif top_n_topics is not None : selected_topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : selected_topics = sorted ( freq_df . Topic . to_list ()) # Prepare data if topic_model . custom_labels_ is not None and custom_labels : topic_names = { key : topic_model . custom_labels_ [ key + topic_model . _outliers ] for key , _ in topic_model . topic_labels_ . items ()} else : topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_labels_ . items ()} topics_over_time [ \"Name\" ] = topics_over_time . Topic . map ( topic_names ) data = topics_over_time . loc [ topics_over_time . Topic . isin ( selected_topics ), :] . sort_values ([ \"Topic\" , \"Timestamp\" ]) # Add traces fig = go . Figure () for index , topic in enumerate ( data . Topic . unique ()): trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : y = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : y = trace_data . Frequency fig . add_trace ( go . Scatter ( x = trace_data . Timestamp , y = y , mode = 'lines' , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( yaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , title = { 'text' : \"<b>Topics over Time\" , 'y' : .95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"DTM"},{"location":"api/plotting/dtm.html#dtm","text":"Visualize topics over time Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_over_time pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all None topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 450 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: topics_over_time = topic_model . topics_over_time ( docs , timestamps ) topic_model . visualize_topics_over_time ( topics_over_time ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_over_time ( topics_over_time ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_over_time.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def visualize_topics_over_time ( topic_model , topics_over_time : pd . DataFrame , top_n_topics : int = None , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 450 ) -> go . Figure : \"\"\" Visualize topics over time Arguments: topic_model: A fitted BERTopic instance. topics_over_time: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics over time, simply run: ```python topics_over_time = topic_model.topics_over_time(docs, timestamps) topic_model.visualize_topics_over_time(topics_over_time) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_over_time(topics_over_time) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/trump.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : selected_topics = list ( topics ) elif top_n_topics is not None : selected_topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : selected_topics = sorted ( freq_df . Topic . to_list ()) # Prepare data if topic_model . custom_labels_ is not None and custom_labels : topic_names = { key : topic_model . custom_labels_ [ key + topic_model . _outliers ] for key , _ in topic_model . topic_labels_ . items ()} else : topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_labels_ . items ()} topics_over_time [ \"Name\" ] = topics_over_time . Topic . map ( topic_names ) data = topics_over_time . loc [ topics_over_time . Topic . isin ( selected_topics ), :] . sort_values ([ \"Topic\" , \"Timestamp\" ]) # Add traces fig = go . Figure () for index , topic in enumerate ( data . Topic . unique ()): trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : y = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : y = trace_data . Frequency fig . add_trace ( go . Scatter ( x = trace_data . Timestamp , y = y , mode = 'lines' , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( yaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , title = { 'text' : \"<b>Topics over Time\" , 'y' : .95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"DTM"},{"location":"api/plotting/heatmap.html","text":"Heatmap \u00b6 Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 800 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_heatmap.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def visualize_heatmap ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/heatmap.html\" style=\"width:1000px; height: 720px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings_ is not None : embeddings = np . array ( topic_model . topic_embeddings_ ) else : embeddings = topic_model . c_tf_idf_ # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Order heatmap by similar clusters of topics if n_clusters : if n_clusters >= len ( set ( topics )): raise ValueError ( \"Make sure to set `n_clusters` lower than \" \"the total number of unique topics.\" ) embeddings = embeddings [[ topic + topic_model . _outliers for topic in topics ]] distance_matrix = cosine_similarity ( embeddings ) Z = linkage ( distance_matrix , 'ward' ) clusters = fcluster ( Z , t = n_clusters , criterion = 'maxclust' ) # Extract new order of topics mapping = { cluster : [] for cluster in clusters } for topic , cluster in zip ( topics , clusters ): mapping [ cluster ] . append ( topic ) mapping = [ cluster for cluster in mapping . values ()] sorted_topics = [ topic for cluster in mapping for topic in cluster ] else : sorted_topics = topics # Select embeddings indices = np . array ([ topics . index ( topic ) for topic in sorted_topics ]) embeddings = embeddings [ indices ] distance_matrix = cosine_similarity ( embeddings ) # Create labels if topic_model . custom_labels_ is not None and custom_labels : new_labels = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in sorted_topics ] else : new_labels = [[[ str ( topic ), None ]] + topic_model . get_topic ( topic ) for topic in sorted_topics ] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] fig = px . imshow ( distance_matrix , labels = dict ( color = \"Similarity Score\" ), x = new_labels , y = new_labels , color_continuous_scale = 'GnBu' ) fig . update_layout ( title = { 'text' : \"<b>Similarity Matrix\" , 'y' : .95 , 'x' : 0.55 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_layout ( showlegend = True ) fig . update_layout ( legend_title_text = 'Trend' ) return fig","title":"Heatmap"},{"location":"api/plotting/heatmap.html#heatmap","text":"Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. None top_n_topics int Only select the top n most frequent topics. None n_clusters int Create n clusters and order the similarity matrix by those clusters. None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 800 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the similarity matrix of topics simply run: topic_model . visualize_heatmap () Or if you want to save the resulting figure: fig = topic_model . visualize_heatmap () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_heatmap.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def visualize_heatmap ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , n_clusters : int = None , custom_labels : bool = False , width : int = 800 , height : int = 800 ) -> go . Figure : \"\"\" Visualize a heatmap of the topic's similarity matrix Based on the cosine similarity matrix between topic embeddings, a heatmap is created showing the similarity between topics. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. top_n_topics: Only select the top n most frequent topics. n_clusters: Create n clusters and order the similarity matrix by those clusters. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the similarity matrix of topics simply run: ```python topic_model.visualize_heatmap() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_heatmap() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/heatmap.html\" style=\"width:1000px; height: 720px; border: 0px;\"\"></iframe> \"\"\" # Select topic embeddings if topic_model . topic_embeddings_ is not None : embeddings = np . array ( topic_model . topic_embeddings_ ) else : embeddings = topic_model . c_tf_idf_ # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Order heatmap by similar clusters of topics if n_clusters : if n_clusters >= len ( set ( topics )): raise ValueError ( \"Make sure to set `n_clusters` lower than \" \"the total number of unique topics.\" ) embeddings = embeddings [[ topic + topic_model . _outliers for topic in topics ]] distance_matrix = cosine_similarity ( embeddings ) Z = linkage ( distance_matrix , 'ward' ) clusters = fcluster ( Z , t = n_clusters , criterion = 'maxclust' ) # Extract new order of topics mapping = { cluster : [] for cluster in clusters } for topic , cluster in zip ( topics , clusters ): mapping [ cluster ] . append ( topic ) mapping = [ cluster for cluster in mapping . values ()] sorted_topics = [ topic for cluster in mapping for topic in cluster ] else : sorted_topics = topics # Select embeddings indices = np . array ([ topics . index ( topic ) for topic in sorted_topics ]) embeddings = embeddings [ indices ] distance_matrix = cosine_similarity ( embeddings ) # Create labels if topic_model . custom_labels_ is not None and custom_labels : new_labels = [ topic_model . custom_labels_ [ topic + topic_model . _outliers ] for topic in sorted_topics ] else : new_labels = [[[ str ( topic ), None ]] + topic_model . get_topic ( topic ) for topic in sorted_topics ] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] fig = px . imshow ( distance_matrix , labels = dict ( color = \"Similarity Score\" ), x = new_labels , y = new_labels , color_continuous_scale = 'GnBu' ) fig . update_layout ( title = { 'text' : \"<b>Similarity Matrix\" , 'y' : .95 , 'x' : 0.55 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_layout ( showlegend = True ) fig . update_layout ( legend_title_text = 'Trend' ) return fig","title":"Heatmap"},{"location":"api/plotting/hierarchical_documents.html","text":"Hierarchical Documents \u00b6 Visualize documents and their topics in 2D at different levels of hierarchy Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample Union [ float , int ] The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. True nr_levels int The number of levels to be visualized in the hierarchy. First, the distances in hierarchical_topics.Distance are split in nr_levels lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that nr_levels is equal to the length of hierarchical_topics . 10 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) NOTE This visualization was inspired by the scatter plot representation of Doc2Map: https://github.com/louisgeisler/Doc2Map Source code in bertopic\\plotting\\_hierarchical_documents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def visualize_hierarchical_documents ( topic_model , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` NOTE: This visualization was inspired by the scatter plot representation of Doc2Map: https://github.com/louisgeisler/Doc2Map <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" topic_per_doc = topic_model . topics_ # Sample the data to optimize for visualization and dimensionality reduction if sample is None or sample > 1 : sample = 1 indices = [] for topic in set ( topic_per_doc ): s = np . where ( np . array ( topic_per_doc ) == topic )[ 0 ] size = len ( s ) if len ( s ) < 100 else int ( len ( s ) * sample ) indices . extend ( np . random . choice ( s , size = size , replace = False )) indices = np . array ( indices ) df = pd . DataFrame ({ \"topic\" : np . array ( topic_per_doc )[ indices ]}) df [ \"doc\" ] = [ docs [ index ] for index in indices ] df [ \"topic\" ] = [ topic_per_doc [ index ] for index in indices ] # Extract embeddings if not already done if sample is None : if embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) else : embeddings_to_reduce = embeddings else : if embeddings is not None : embeddings_to_reduce = embeddings [ indices ] elif embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) # Reduce input embeddings if reduced_embeddings is None : umap_model = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit ( embeddings_to_reduce ) embeddings_2d = umap_model . embedding_ elif sample is not None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings [ indices ] elif sample is None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings # Combine data df [ \"x\" ] = embeddings_2d [:, 0 ] df [ \"y\" ] = embeddings_2d [:, 1 ] # Create topic list for each level, levels are created by calculating the distance distances = hierarchical_topics . Distance . to_list () max_distances = [ distances [ indices [ - 1 ]] for indices in np . array_split ( range ( len ( hierarchical_topics )), nr_levels )][:: - 1 ] for index , max_distance in enumerate ( max_distances ): # Get topics below `max_distance` mapping = { topic : topic for topic in df . topic . unique ()} selection = hierarchical_topics . loc [ hierarchical_topics . Distance <= max_distance , :] selection . Parent_ID = selection . Parent_ID . astype ( int ) selection = selection . sort_values ( \"Parent_ID\" ) for row in selection . iterrows (): for topic in row [ 1 ] . Topics : mapping [ topic ] = row [ 1 ] . Parent_ID # Make sure the mappings are mapped 1:1 mappings = [ True for _ in mapping ] while any ( mappings ): for i , ( key , value ) in enumerate ( mapping . items ()): if value in mapping . keys () and key != value : mapping [ key ] = mapping [ value ] else : mappings [ i ] = False # Create new column df [ f \"level_ { index + 1 } \" ] = df . topic . map ( mapping ) df [ f \"level_ { index + 1 } \" ] = df [ f \"level_ { index + 1 } \" ] . astype ( int ) # Prepare topic names of original and merged topics trace_names = [] topic_names = {} for topic in range ( hierarchical_topics . Parent_ID . astype ( int ) . max ()): if topic < hierarchical_topics . Parent_ID . astype ( int ) . min (): if topic_model . get_topic ( topic ): if topic_model . custom_labels_ is not None and custom_labels : trace_name = topic_model . custom_labels_ [ topic + topic_model . _outliers ] else : trace_name = f \" { topic } _\" + \"_\" . join ([ word [: 20 ] for word , _ in topic_model . get_topic ( topic )][: 3 ]) topic_names [ topic ] = { \"trace_name\" : trace_name [: 40 ], \"plot_text\" : trace_name [: 40 ]} trace_names . append ( trace_name ) else : trace_name = f \" { topic } _\" + hierarchical_topics . loc [ hierarchical_topics . Parent_ID == str ( topic ), \"Parent_Name\" ] . values [ 0 ] plot_text = \"_\" . join ([ name [: 20 ] for name in trace_name . split ( \"_\" )[: 3 ]]) topic_names [ topic ] = { \"trace_name\" : trace_name [: 40 ], \"plot_text\" : plot_text [: 40 ]} trace_names . append ( trace_name ) # Prepare traces all_traces = [] for level in range ( len ( max_distances )): traces = [] # Outliers if topic_model . _outliers : traces . append ( go . Scattergl ( x = df . loc [( df [ f \"level_ { level + 1 } \" ] == - 1 ), \"x\" ], y = df . loc [ df [ f \"level_ { level + 1 } \" ] == - 1 , \"y\" ], mode = 'markers+text' , name = \"other\" , hoverinfo = \"text\" , hovertext = df . loc [( df [ f \"level_ { level + 1 } \" ] == - 1 ), \"doc\" ] if not hide_document_hover else None , showlegend = False , marker = dict ( color = '#CFD8DC' , size = 5 , opacity = 0.5 ) ) ) # Selected topics if topics : selection = df . loc [( df . topic . isin ( topics )), :] unique_topics = sorted ([ int ( topic ) for topic in selection [ f \"level_ { level + 1 } \" ] . unique ()]) else : unique_topics = sorted ([ int ( topic ) for topic in df [ f \"level_ { level + 1 } \" ] . unique ()]) for topic in unique_topics : if topic != - 1 : if topics : selection = df . loc [( df [ f \"level_ { level + 1 } \" ] == topic ) & ( df . topic . isin ( topics )), :] else : selection = df . loc [ df [ f \"level_ { level + 1 } \" ] == topic , :] if not hide_annotations : selection . loc [ len ( selection ), :] = None selection [ \"text\" ] = \"\" selection . loc [ len ( selection ) - 1 , \"x\" ] = selection . x . mean () selection . loc [ len ( selection ) - 1 , \"y\" ] = selection . y . mean () selection . loc [ len ( selection ) - 1 , \"text\" ] = topic_names [ int ( topic )][ \"plot_text\" ] traces . append ( go . Scattergl ( x = selection . x , y = selection . y , text = selection . text if not hide_annotations else None , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , name = topic_names [ int ( topic )][ \"trace_name\" ], mode = 'markers+text' , marker = dict ( size = 5 , opacity = 0.5 ) ) ) all_traces . append ( traces ) # Track and count traces nr_traces_per_set = [ len ( traces ) for traces in all_traces ] trace_indices = [( 0 , nr_traces_per_set [ 0 ])] for index , nr_traces in enumerate ( nr_traces_per_set [ 1 :]): start = trace_indices [ index ][ 1 ] end = nr_traces + start trace_indices . append (( start , end )) # Visualization fig = go . Figure () for traces in all_traces : for trace in traces : fig . add_trace ( trace ) for index in range ( len ( fig . data )): if index >= nr_traces_per_set [ 0 ]: fig . data [ index ] . visible = False # Create and add slider steps = [] for index , indices in enumerate ( trace_indices ): step = dict ( method = \"update\" , label = str ( index ), args = [{ \"visible\" : [ False ] * len ( fig . data )}] ) for index in range ( indices [ 1 ] - indices [ 0 ]): step [ \"args\" ][ 0 ][ \"visible\" ][ index + indices [ 0 ]] = True steps . append ( step ) sliders = [ dict ( currentvalue = { \"prefix\" : \"Level: \" }, pad = { \"t\" : 20 }, steps = steps )] # Add grid in a 'plus' shape x_range = ( df . x . min () - abs (( df . x . min ()) * .15 ), df . x . max () + abs (( df . x . max ()) * .15 )) y_range = ( df . y . min () - abs (( df . y . min ()) * .15 ), df . y . max () + abs (( df . y . max ()) * .15 )) fig . add_shape ( type = \"line\" , x0 = sum ( x_range ) / 2 , y0 = y_range [ 0 ], x1 = sum ( x_range ) / 2 , y1 = y_range [ 1 ], line = dict ( color = \"#CFD8DC\" , width = 2 )) fig . add_shape ( type = \"line\" , x0 = x_range [ 0 ], y0 = sum ( y_range ) / 2 , x1 = x_range [ 1 ], y1 = sum ( y_range ) / 2 , line = dict ( color = \"#9E9E9E\" , width = 2 )) fig . add_annotation ( x = x_range [ 0 ], y = sum ( y_range ) / 2 , text = \"D1\" , showarrow = False , yshift = 10 ) fig . add_annotation ( y = y_range [ 1 ], x = sum ( x_range ) / 2 , text = \"D2\" , showarrow = False , xshift = 10 ) # Stylize layout fig . update_layout ( sliders = sliders , template = \"simple_white\" , title = { 'text' : \"<b>Hierarchical Documents and Topics\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , ) fig . update_xaxes ( visible = False ) fig . update_yaxes ( visible = False ) return fig","title":"Hierarchical documents"},{"location":"api/plotting/hierarchical_documents.html#hierarchical-documents","text":"Visualize documents and their topics in 2D at different levels of hierarchy Parameters: Name Type Description Default docs List [ str ] The documents you used when calling either fit or fit_transform required hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children required topics List [ int ] A selection of topics to visualize. Not to be confused with the topics that you get from .fit_transform . For example, if you want to visualize only topics 1 through 5: topics = [1, 2, 3, 4, 5] . None embeddings np . ndarray The embeddings of all documents in docs . None reduced_embeddings np . ndarray The 2D reduced embeddings of all documents in docs . None sample Union [ float , int ] The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. None hide_annotations bool Hide the names of the traces on top of each cluster. False hide_document_hover bool Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. True nr_levels int The number of levels to be visualized in the hierarchy. First, the distances in hierarchical_topics.Distance are split in nr_levels lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that nr_levels is equal to the length of hierarchical_topics . 10 custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. 1200 height int The height of the figure. 750 Examples: To visualize the topics simply run: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics ) Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Or, if you have reduced the original embeddings already: topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Or if you want to save the resulting figure: fig = topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) fig . write_html ( \"path/to/file.html\" ) NOTE This visualization was inspired by the scatter plot representation of Doc2Map: https://github.com/louisgeisler/Doc2Map Source code in bertopic\\plotting\\_hierarchical_documents.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def visualize_hierarchical_documents ( topic_model , docs : List [ str ], hierarchical_topics : pd . DataFrame , topics : List [ int ] = None , embeddings : np . ndarray = None , reduced_embeddings : np . ndarray = None , sample : Union [ float , int ] = None , hide_annotations : bool = False , hide_document_hover : bool = True , nr_levels : int = 10 , custom_labels : bool = False , width : int = 1200 , height : int = 750 ) -> go . Figure : \"\"\" Visualize documents and their topics in 2D at different levels of hierarchy Arguments: docs: The documents you used when calling either `fit` or `fit_transform` hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children topics: A selection of topics to visualize. Not to be confused with the topics that you get from `.fit_transform`. For example, if you want to visualize only topics 1 through 5: `topics = [1, 2, 3, 4, 5]`. embeddings: The embeddings of all documents in `docs`. reduced_embeddings: The 2D reduced embeddings of all documents in `docs`. sample: The percentage of documents in each topic that you would like to keep. Value can be between 0 and 1. Setting this value to, for example, 0.1 (10% of documents in each topic) makes it easier to visualize millions of documents as a subset is chosen. hide_annotations: Hide the names of the traces on top of each cluster. hide_document_hover: Hide the content of the documents when hovering over specific points. Helps to speed up generation of visualizations. nr_levels: The number of levels to be visualized in the hierarchy. First, the distances in `hierarchical_topics.Distance` are split in `nr_levels` lists of distances with equal length. Then, for each list of distances, the merged topics are selected that have a distance less or equal to the maximum distance of the selected list of distances. NOTE: To get all possible merged steps, make sure that `nr_levels` is equal to the length of `hierarchical_topics`. custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_hierarchical_documents(docs, hierarchical_topics) ``` Do note that this re-calculates the embeddings and reduces them to 2D. The advised and prefered pipeline for using this function is as follows: ```python from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data'] sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\") embeddings = sentence_model.encode(docs, show_progress_bar=False) # Train BERTopic and extract hierarchical topics topic_model = BERTopic().fit(docs, embeddings) hierarchical_topics = topic_model.hierarchical_topics(docs) # Reduce dimensionality of embeddings, this step is optional # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings) # Run the visualization with the original embeddings topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, embeddings=embeddings) # Or, if you have reduced the original embeddings already: topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchical_documents(docs, hierarchical_topics, reduced_embeddings=reduced_embeddings) fig.write_html(\"path/to/file.html\") ``` NOTE: This visualization was inspired by the scatter plot representation of Doc2Map: https://github.com/louisgeisler/Doc2Map <iframe src=\"../../getting_started/visualization/hierarchical_documents.html\" style=\"width:1000px; height: 770px; border: 0px;\"\"></iframe> \"\"\" topic_per_doc = topic_model . topics_ # Sample the data to optimize for visualization and dimensionality reduction if sample is None or sample > 1 : sample = 1 indices = [] for topic in set ( topic_per_doc ): s = np . where ( np . array ( topic_per_doc ) == topic )[ 0 ] size = len ( s ) if len ( s ) < 100 else int ( len ( s ) * sample ) indices . extend ( np . random . choice ( s , size = size , replace = False )) indices = np . array ( indices ) df = pd . DataFrame ({ \"topic\" : np . array ( topic_per_doc )[ indices ]}) df [ \"doc\" ] = [ docs [ index ] for index in indices ] df [ \"topic\" ] = [ topic_per_doc [ index ] for index in indices ] # Extract embeddings if not already done if sample is None : if embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) else : embeddings_to_reduce = embeddings else : if embeddings is not None : embeddings_to_reduce = embeddings [ indices ] elif embeddings is None and reduced_embeddings is None : embeddings_to_reduce = topic_model . _extract_embeddings ( df . doc . to_list (), method = \"document\" ) # Reduce input embeddings if reduced_embeddings is None : umap_model = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit ( embeddings_to_reduce ) embeddings_2d = umap_model . embedding_ elif sample is not None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings [ indices ] elif sample is None and reduced_embeddings is not None : embeddings_2d = reduced_embeddings # Combine data df [ \"x\" ] = embeddings_2d [:, 0 ] df [ \"y\" ] = embeddings_2d [:, 1 ] # Create topic list for each level, levels are created by calculating the distance distances = hierarchical_topics . Distance . to_list () max_distances = [ distances [ indices [ - 1 ]] for indices in np . array_split ( range ( len ( hierarchical_topics )), nr_levels )][:: - 1 ] for index , max_distance in enumerate ( max_distances ): # Get topics below `max_distance` mapping = { topic : topic for topic in df . topic . unique ()} selection = hierarchical_topics . loc [ hierarchical_topics . Distance <= max_distance , :] selection . Parent_ID = selection . Parent_ID . astype ( int ) selection = selection . sort_values ( \"Parent_ID\" ) for row in selection . iterrows (): for topic in row [ 1 ] . Topics : mapping [ topic ] = row [ 1 ] . Parent_ID # Make sure the mappings are mapped 1:1 mappings = [ True for _ in mapping ] while any ( mappings ): for i , ( key , value ) in enumerate ( mapping . items ()): if value in mapping . keys () and key != value : mapping [ key ] = mapping [ value ] else : mappings [ i ] = False # Create new column df [ f \"level_ { index + 1 } \" ] = df . topic . map ( mapping ) df [ f \"level_ { index + 1 } \" ] = df [ f \"level_ { index + 1 } \" ] . astype ( int ) # Prepare topic names of original and merged topics trace_names = [] topic_names = {} for topic in range ( hierarchical_topics . Parent_ID . astype ( int ) . max ()): if topic < hierarchical_topics . Parent_ID . astype ( int ) . min (): if topic_model . get_topic ( topic ): if topic_model . custom_labels_ is not None and custom_labels : trace_name = topic_model . custom_labels_ [ topic + topic_model . _outliers ] else : trace_name = f \" { topic } _\" + \"_\" . join ([ word [: 20 ] for word , _ in topic_model . get_topic ( topic )][: 3 ]) topic_names [ topic ] = { \"trace_name\" : trace_name [: 40 ], \"plot_text\" : trace_name [: 40 ]} trace_names . append ( trace_name ) else : trace_name = f \" { topic } _\" + hierarchical_topics . loc [ hierarchical_topics . Parent_ID == str ( topic ), \"Parent_Name\" ] . values [ 0 ] plot_text = \"_\" . join ([ name [: 20 ] for name in trace_name . split ( \"_\" )[: 3 ]]) topic_names [ topic ] = { \"trace_name\" : trace_name [: 40 ], \"plot_text\" : plot_text [: 40 ]} trace_names . append ( trace_name ) # Prepare traces all_traces = [] for level in range ( len ( max_distances )): traces = [] # Outliers if topic_model . _outliers : traces . append ( go . Scattergl ( x = df . loc [( df [ f \"level_ { level + 1 } \" ] == - 1 ), \"x\" ], y = df . loc [ df [ f \"level_ { level + 1 } \" ] == - 1 , \"y\" ], mode = 'markers+text' , name = \"other\" , hoverinfo = \"text\" , hovertext = df . loc [( df [ f \"level_ { level + 1 } \" ] == - 1 ), \"doc\" ] if not hide_document_hover else None , showlegend = False , marker = dict ( color = '#CFD8DC' , size = 5 , opacity = 0.5 ) ) ) # Selected topics if topics : selection = df . loc [( df . topic . isin ( topics )), :] unique_topics = sorted ([ int ( topic ) for topic in selection [ f \"level_ { level + 1 } \" ] . unique ()]) else : unique_topics = sorted ([ int ( topic ) for topic in df [ f \"level_ { level + 1 } \" ] . unique ()]) for topic in unique_topics : if topic != - 1 : if topics : selection = df . loc [( df [ f \"level_ { level + 1 } \" ] == topic ) & ( df . topic . isin ( topics )), :] else : selection = df . loc [ df [ f \"level_ { level + 1 } \" ] == topic , :] if not hide_annotations : selection . loc [ len ( selection ), :] = None selection [ \"text\" ] = \"\" selection . loc [ len ( selection ) - 1 , \"x\" ] = selection . x . mean () selection . loc [ len ( selection ) - 1 , \"y\" ] = selection . y . mean () selection . loc [ len ( selection ) - 1 , \"text\" ] = topic_names [ int ( topic )][ \"plot_text\" ] traces . append ( go . Scattergl ( x = selection . x , y = selection . y , text = selection . text if not hide_annotations else None , hovertext = selection . doc if not hide_document_hover else None , hoverinfo = \"text\" , name = topic_names [ int ( topic )][ \"trace_name\" ], mode = 'markers+text' , marker = dict ( size = 5 , opacity = 0.5 ) ) ) all_traces . append ( traces ) # Track and count traces nr_traces_per_set = [ len ( traces ) for traces in all_traces ] trace_indices = [( 0 , nr_traces_per_set [ 0 ])] for index , nr_traces in enumerate ( nr_traces_per_set [ 1 :]): start = trace_indices [ index ][ 1 ] end = nr_traces + start trace_indices . append (( start , end )) # Visualization fig = go . Figure () for traces in all_traces : for trace in traces : fig . add_trace ( trace ) for index in range ( len ( fig . data )): if index >= nr_traces_per_set [ 0 ]: fig . data [ index ] . visible = False # Create and add slider steps = [] for index , indices in enumerate ( trace_indices ): step = dict ( method = \"update\" , label = str ( index ), args = [{ \"visible\" : [ False ] * len ( fig . data )}] ) for index in range ( indices [ 1 ] - indices [ 0 ]): step [ \"args\" ][ 0 ][ \"visible\" ][ index + indices [ 0 ]] = True steps . append ( step ) sliders = [ dict ( currentvalue = { \"prefix\" : \"Level: \" }, pad = { \"t\" : 20 }, steps = steps )] # Add grid in a 'plus' shape x_range = ( df . x . min () - abs (( df . x . min ()) * .15 ), df . x . max () + abs (( df . x . max ()) * .15 )) y_range = ( df . y . min () - abs (( df . y . min ()) * .15 ), df . y . max () + abs (( df . y . max ()) * .15 )) fig . add_shape ( type = \"line\" , x0 = sum ( x_range ) / 2 , y0 = y_range [ 0 ], x1 = sum ( x_range ) / 2 , y1 = y_range [ 1 ], line = dict ( color = \"#CFD8DC\" , width = 2 )) fig . add_shape ( type = \"line\" , x0 = x_range [ 0 ], y0 = sum ( y_range ) / 2 , x1 = x_range [ 1 ], y1 = sum ( y_range ) / 2 , line = dict ( color = \"#9E9E9E\" , width = 2 )) fig . add_annotation ( x = x_range [ 0 ], y = sum ( y_range ) / 2 , text = \"D1\" , showarrow = False , yshift = 10 ) fig . add_annotation ( y = y_range [ 1 ], x = sum ( x_range ) / 2 , text = \"D2\" , showarrow = False , xshift = 10 ) # Stylize layout fig . update_layout ( sliders = sliders , template = \"simple_white\" , title = { 'text' : \"<b>Hierarchical Documents and Topics\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , ) fig . update_xaxes ( visible = False ) fig . update_yaxes ( visible = False ) return fig","title":"Hierarchical Documents"},{"location":"api/plotting/hierarchy.html","text":"Hierarchy \u00b6 Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. Only works if orientation is set to 'left' 1000 height int The height of the figure. Only works if orientation is set to 'bottom' 600 hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both topics and top_n_topics are not set. None linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) NOTE: Make sure to use the same linkage_function as used in topic_model.hierarchical_topics . None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) NOTE: Make sure to use the same distance_function as used in topic_model.hierarchical_topics . None color_threshold int Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. 1 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () If you also want the labels visualized of hierarchical topics, run the following: # Extract hierarchical topics and their representations hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Visualize these representations topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_hierarchy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def visualize_hierarchy ( topic_model , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Select embeddings all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf_ [ indices ] # Annotations if hierarchical_topics is not None and len ( topics ) == len ( freq_df . Topic . to_list ()): annotations = _get_annotations ( topic_model = topic_model , hierarchical_topics = hierarchical_topics , embeddings = embeddings , distance_function = distance_function , linkage_function = linkage_function , orientation = orientation , custom_labels = custom_labels ) else : annotations = None # Create dendogram fig = ff . create_dendrogram ( embeddings , orientation = orientation , distfun = distance_function , linkagefun = linkage_function , hovertext = annotations , color_threshold = color_threshold ) # Create nicer labels axis = \"yaxis\" if orientation == \"left\" else \"xaxis\" if topic_model . custom_labels_ is not None and custom_labels : new_labels = [ topic_model . custom_labels_ [ topics [ int ( x )] + topic_model . _outliers ] for x in fig . layout [ axis ][ \"ticktext\" ]] else : new_labels = [[[ str ( topics [ int ( x )]), None ]] + topic_model . get_topic ( topics [ int ( x )]) for x in fig . layout [ axis ][ \"ticktext\" ]] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] # Stylize layout fig . update_layout ( plot_bgcolor = '#ECEFF1' , template = \"plotly_white\" , title = { 'text' : \"<b>Hierarchical Clustering\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) # Stylize orientation if orientation == \"left\" : fig . update_layout ( height = 200 + ( 15 * len ( topics )), width = width , yaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) # Fix empty space on the bottom of the graph y_max = max ([ trace [ 'y' ] . max () + 5 for trace in fig [ 'data' ]]) y_min = min ([ trace [ 'y' ] . min () - 5 for trace in fig [ 'data' ]]) fig . update_layout ( yaxis = dict ( range = [ y_min , y_max ])) else : fig . update_layout ( width = 200 + ( 15 * len ( topics )), height = height , xaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) if hierarchical_topics is not None : for index in [ 0 , 3 ]: axis = \"x\" if orientation == \"left\" else \"y\" xs = [ data [ \"x\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] ys = [ data [ \"y\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] hovertext = [ data [ \"text\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] fig . add_trace ( go . Scatter ( x = xs , y = ys , marker_color = 'black' , hovertext = hovertext , hoverinfo = \"text\" , mode = 'markers' , showlegend = False )) return fig","title":"Hierarchical topics"},{"location":"api/plotting/hierarchy.html#hierarchy","text":"Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required orientation str The orientation of the figure. Either 'left' or 'bottom' 'left' topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . NOTE: Custom labels are only generated for the original un-merged topics. False width int The width of the figure. Only works if orientation is set to 'left' 1000 height int The height of the figure. Only works if orientation is set to 'bottom' 600 hierarchical_topics pd . DataFrame A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both topics and top_n_topics are not set. None linkage_function Callable [[ csr_matrix ], np . ndarray ] The linkage function to use. Default is: lambda x: sch.linkage(x, 'ward', optimal_ordering=True) NOTE: Make sure to use the same linkage_function as used in topic_model.hierarchical_topics . None distance_function Callable [[ csr_matrix ], csr_matrix ] The distance function to use on the c-TF-IDF matrix. Default is: lambda x: 1 - cosine_similarity(x) NOTE: Make sure to use the same distance_function as used in topic_model.hierarchical_topics . None color_threshold int Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. 1 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the hierarchical structure of topics simply run: topic_model . visualize_hierarchy () If you also want the labels visualized of hierarchical topics, run the following: # Extract hierarchical topics and their representations hierarchical_topics = topic_model . hierarchical_topics ( docs ) # Visualize these representations topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you want to save the resulting figure: fig = topic_model . visualize_hierarchy () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_hierarchy.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def visualize_hierarchy ( topic_model , orientation : str = \"left\" , topics : List [ int ] = None , top_n_topics : int = None , custom_labels : bool = False , width : int = 1000 , height : int = 600 , hierarchical_topics : pd . DataFrame = None , linkage_function : Callable [[ csr_matrix ], np . ndarray ] = None , distance_function : Callable [[ csr_matrix ], csr_matrix ] = None , color_threshold : int = 1 ) -> go . Figure : \"\"\" Visualize a hierarchical structure of the topics A ward linkage function is used to perform the hierarchical clustering based on the cosine distance matrix between topic embeddings. Arguments: topic_model: A fitted BERTopic instance. orientation: The orientation of the figure. Either 'left' or 'bottom' topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. NOTE: Custom labels are only generated for the original un-merged topics. width: The width of the figure. Only works if orientation is set to 'left' height: The height of the figure. Only works if orientation is set to 'bottom' hierarchical_topics: A dataframe that contains a hierarchy of topics represented by their parents and their children. NOTE: The hierarchical topic names are only visualized if both `topics` and `top_n_topics` are not set. linkage_function: The linkage function to use. Default is: `lambda x: sch.linkage(x, 'ward', optimal_ordering=True)` NOTE: Make sure to use the same `linkage_function` as used in `topic_model.hierarchical_topics`. distance_function: The distance function to use on the c-TF-IDF matrix. Default is: `lambda x: 1 - cosine_similarity(x)` NOTE: Make sure to use the same `distance_function` as used in `topic_model.hierarchical_topics`. color_threshold: Value at which the separation of clusters will be made which will result in different colors for different clusters. A higher value will typically lead in less colored clusters. Returns: fig: A plotly figure Examples: To visualize the hierarchical structure of topics simply run: ```python topic_model.visualize_hierarchy() ``` If you also want the labels visualized of hierarchical topics, run the following: ```python # Extract hierarchical topics and their representations hierarchical_topics = topic_model.hierarchical_topics(docs) # Visualize these representations topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics) ``` If you want to save the resulting figure: ```python fig = topic_model.visualize_hierarchy() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/hierarchy.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" if distance_function is None : distance_function = lambda x : 1 - cosine_similarity ( x ) if linkage_function is None : linkage_function = lambda x : sch . linkage ( x , 'ward' , optimal_ordering = True ) # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Select embeddings all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf_ [ indices ] # Annotations if hierarchical_topics is not None and len ( topics ) == len ( freq_df . Topic . to_list ()): annotations = _get_annotations ( topic_model = topic_model , hierarchical_topics = hierarchical_topics , embeddings = embeddings , distance_function = distance_function , linkage_function = linkage_function , orientation = orientation , custom_labels = custom_labels ) else : annotations = None # Create dendogram fig = ff . create_dendrogram ( embeddings , orientation = orientation , distfun = distance_function , linkagefun = linkage_function , hovertext = annotations , color_threshold = color_threshold ) # Create nicer labels axis = \"yaxis\" if orientation == \"left\" else \"xaxis\" if topic_model . custom_labels_ is not None and custom_labels : new_labels = [ topic_model . custom_labels_ [ topics [ int ( x )] + topic_model . _outliers ] for x in fig . layout [ axis ][ \"ticktext\" ]] else : new_labels = [[[ str ( topics [ int ( x )]), None ]] + topic_model . get_topic ( topics [ int ( x )]) for x in fig . layout [ axis ][ \"ticktext\" ]] new_labels = [ \"_\" . join ([ label [ 0 ] for label in labels [: 4 ]]) for labels in new_labels ] new_labels = [ label if len ( label ) < 30 else label [: 27 ] + \"...\" for label in new_labels ] # Stylize layout fig . update_layout ( plot_bgcolor = '#ECEFF1' , template = \"plotly_white\" , title = { 'text' : \"<b>Hierarchical Clustering\" , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) # Stylize orientation if orientation == \"left\" : fig . update_layout ( height = 200 + ( 15 * len ( topics )), width = width , yaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) # Fix empty space on the bottom of the graph y_max = max ([ trace [ 'y' ] . max () + 5 for trace in fig [ 'data' ]]) y_min = min ([ trace [ 'y' ] . min () - 5 for trace in fig [ 'data' ]]) fig . update_layout ( yaxis = dict ( range = [ y_min , y_max ])) else : fig . update_layout ( width = 200 + ( 15 * len ( topics )), height = height , xaxis = dict ( tickmode = \"array\" , ticktext = new_labels )) if hierarchical_topics is not None : for index in [ 0 , 3 ]: axis = \"x\" if orientation == \"left\" else \"y\" xs = [ data [ \"x\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] ys = [ data [ \"y\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] hovertext = [ data [ \"text\" ][ index ] for data in fig . data if ( data [ \"text\" ] and data [ axis ][ index ] > 0 )] fig . add_trace ( go . Scatter ( x = xs , y = ys , marker_color = 'black' , hovertext = hovertext , hoverinfo = \"text\" , mode = 'markers' , showlegend = False )) return fig","title":"Hierarchy"},{"location":"api/plotting/term.html","text":"Term Score Decline \u00b6 Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the ranks of all words across all topics simply run: topic_model . visualize_term_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_term_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\plotting\\_term_rank.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def visualize_term_rank ( topic_model , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/term_rank.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> <iframe src=\"../../getting_started/visualization/term_rank_log.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" topics = [] if topics is None else topics topic_ids = topic_model . get_topic_info () . Topic . unique () . tolist () topic_words = [ topic_model . get_topic ( topic ) for topic in topic_ids ] values = np . array ([[ value [ 1 ] for value in values ] for values in topic_words ]) indices = np . array ([[ value + 1 for value in range ( len ( values ))] for values in topic_words ]) # Create figure lines = [] for topic , x , y in zip ( topic_ids , indices , values ): if not any ( y > 1.5 ): # labels if topic_model . custom_labels_ is not None and custom_labels : label = topic_model . custom_labels_ [ topic + topic_model . _outliers ] else : label = f \"<b>Topic { topic } </b>:\" + \"_\" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )]) label = label [: 50 ] # line parameters color = \"red\" if topic in topics else \"black\" opacity = 1 if topic in topics else .1 if any ( y == 0 ): y [ y == 0 ] = min ( values [ values > 0 ]) y = np . log10 ( y , out = y , where = y > 0 ) if log_scale else y line = go . Scatter ( x = x , y = y , name = \"\" , hovertext = label , mode = \"lines+lines\" , opacity = opacity , line = dict ( color = color , width = 1.5 )) lines . append ( line ) fig = go . Figure ( data = lines ) # Stylize layout fig . update_xaxes ( range = [ 0 , len ( indices [ 0 ])], tick0 = 1 , dtick = 2 ) fig . update_layout ( showlegend = False , template = \"plotly_white\" , title = { 'text' : \"<b>Term score decline per Topic</b>\" , 'y' : .9 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( title_text = 'Term Rank' ) if log_scale : fig . update_yaxes ( title_text = 'c-TF-IDF score (log scale)' ) else : fig . update_yaxes ( title_text = 'c-TF-IDF score' ) return fig","title":"Term Scores"},{"location":"api/plotting/term.html#term-score-decline","text":"Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize. These will be colored red where all others will be colored black. None log_scale bool Whether to represent the ranking on a log scale False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 800 height int The height of the figure. 500 Returns: Name Type Description fig go . Figure A plotly figure Examples: To visualize the ranks of all words across all topics simply run: topic_model . visualize_term_rank () Or if you want to save the resulting figure: fig = topic_model . visualize_term_rank () fig . write_html ( \"path/to/file.html\" ) Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Source code in bertopic\\plotting\\_term_rank.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def visualize_term_rank ( topic_model , topics : List [ int ] = None , log_scale : bool = False , custom_labels : bool = False , width : int = 800 , height : int = 500 ) -> go . Figure : \"\"\" Visualize the ranks of all terms across all topics Each topic is represented by a set of words. These words, however, do not all equally represent the topic. This visualization shows how many words are needed to represent a topic and at which point the beneficial effect of adding words starts to decline. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize. These will be colored red where all others will be colored black. log_scale: Whether to represent the ranking on a log scale custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: fig: A plotly figure Examples: To visualize the ranks of all words across all topics simply run: ```python topic_model.visualize_term_rank() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_term_rank() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/term_rank.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> <iframe src=\"../../getting_started/visualization/term_rank_log.html\" style=\"width:1000px; height: 530px; border: 0px;\"\"></iframe> Reference: This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing [tmtoolkit](https://tmtoolkit.readthedocs.io/). Reference to that specific analysis can be found [here](https://wzbsocialsciencecenter.github.io/tm_corona/tm_analysis.html). \"\"\" topics = [] if topics is None else topics topic_ids = topic_model . get_topic_info () . Topic . unique () . tolist () topic_words = [ topic_model . get_topic ( topic ) for topic in topic_ids ] values = np . array ([[ value [ 1 ] for value in values ] for values in topic_words ]) indices = np . array ([[ value + 1 for value in range ( len ( values ))] for values in topic_words ]) # Create figure lines = [] for topic , x , y in zip ( topic_ids , indices , values ): if not any ( y > 1.5 ): # labels if topic_model . custom_labels_ is not None and custom_labels : label = topic_model . custom_labels_ [ topic + topic_model . _outliers ] else : label = f \"<b>Topic { topic } </b>:\" + \"_\" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )]) label = label [: 50 ] # line parameters color = \"red\" if topic in topics else \"black\" opacity = 1 if topic in topics else .1 if any ( y == 0 ): y [ y == 0 ] = min ( values [ values > 0 ]) y = np . log10 ( y , out = y , where = y > 0 ) if log_scale else y line = go . Scatter ( x = x , y = y , name = \"\" , hovertext = label , mode = \"lines+lines\" , opacity = opacity , line = dict ( color = color , width = 1.5 )) lines . append ( line ) fig = go . Figure ( data = lines ) # Stylize layout fig . update_xaxes ( range = [ 0 , len ( indices [ 0 ])], tick0 = 1 , dtick = 2 ) fig . update_layout ( showlegend = False , template = \"plotly_white\" , title = { 'text' : \"<b>Term score decline per Topic</b>\" , 'y' : .9 , 'x' : 0.5 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), ) fig . update_xaxes ( title_text = 'Term Rank' ) if log_scale : fig . update_yaxes ( title_text = 'c-TF-IDF score (log scale)' ) else : fig . update_yaxes ( title_text = 'c-TF-IDF score' ) return fig","title":"Term Score Decline"},{"location":"api/plotting/topics.html","text":"Topics \u00b6 Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Examples: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def visualize_topics ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/viz.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Extract topic words and their frequencies topic_list = sorted ( topics ) frequencies = [ topic_model . topic_sizes_ [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf_ . toarray ()[ indices ] embeddings = MinMaxScaler () . fit_transform ( embeddings ) embeddings = UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [:, 0 ], \"y\" : embeddings [:, 1 ], \"Topic\" : topic_list , \"Words\" : words , \"Size\" : frequencies }) return _plotly_topic_visualization ( df , topic_list , width , height )","title":"Topics"},{"location":"api/plotting/topics.html#topics","text":"Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics List [ int ] A selection of topics to visualize None top_n_topics int Only select the top n most frequent topics None width int The width of the figure. 650 height int The height of the figure. 650 Examples: To visualize the topics simply run: topic_model . visualize_topics () Or if you want to save the resulting figure: fig = topic_model . visualize_topics () fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def visualize_topics ( topic_model , topics : List [ int ] = None , top_n_topics : int = None , width : int = 650 , height : int = 650 ) -> go . Figure : \"\"\" Visualize topics, their sizes, and their corresponding words This visualization is highly inspired by LDAvis, a great visualization technique typically reserved for LDA. Arguments: topic_model: A fitted BERTopic instance. topics: A selection of topics to visualize top_n_topics: Only select the top n most frequent topics width: The width of the figure. height: The height of the figure. Examples: To visualize the topics simply run: ```python topic_model.visualize_topics() ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics() fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/viz.html\" style=\"width:1000px; height: 680px; border: 0px;\"\"></iframe> \"\"\" # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : topics = list ( topics ) elif top_n_topics is not None : topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : topics = sorted ( freq_df . Topic . to_list ()) # Extract topic words and their frequencies topic_list = sorted ( topics ) frequencies = [ topic_model . topic_sizes_ [ topic ] for topic in topic_list ] words = [ \" | \" . join ([ word [ 0 ] for word in topic_model . get_topic ( topic )[: 5 ]]) for topic in topic_list ] # Embed c-TF-IDF into 2D all_topics = sorted ( list ( topic_model . get_topics () . keys ())) indices = np . array ([ all_topics . index ( topic ) for topic in topics ]) embeddings = topic_model . c_tf_idf_ . toarray ()[ indices ] embeddings = MinMaxScaler () . fit_transform ( embeddings ) embeddings = UMAP ( n_neighbors = 2 , n_components = 2 , metric = 'hellinger' ) . fit_transform ( embeddings ) # Visualize with plotly df = pd . DataFrame ({ \"x\" : embeddings [:, 0 ], \"y\" : embeddings [:, 1 ], \"Topic\" : topic_list , \"Words\" : words , \"Size\" : frequencies }) return _plotly_topic_visualization ( df , topic_list , width , height )","title":"Topics"},{"location":"api/plotting/topics_per_class.html","text":"Topics per Class \u00b6 Visualize topics per class Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_per_class pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_per_class.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def visualize_topics_per_class ( topic_model , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topic_model: A fitted BERTopic instance. topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/topics_per_class.html\" style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : selected_topics = list ( topics ) elif top_n_topics is not None : selected_topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : selected_topics = sorted ( freq_df . Topic . to_list ()) # Prepare data if topic_model . custom_labels_ is not None and custom_labels : topic_names = { key : topic_model . custom_labels_ [ key + topic_model . _outliers ] for key , _ in topic_model . topic_labels_ . items ()} else : topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_labels_ . items ()} topics_per_class [ \"Name\" ] = topics_per_class . Topic . map ( topic_names ) data = topics_per_class . loc [ topics_per_class . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( selected_topics ): if index == 0 : visible = True else : visible = \"legendonly\" trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : x = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : x = trace_data . Frequency fig . add_trace ( go . Bar ( y = trace_data . Class , x = x , visible = visible , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , orientation = \"h\" , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( xaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , yaxis_title = \"Class\" , title = { 'text' : \"<b>Topics per Class\" , 'y' : .95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"Topics per Class"},{"location":"api/plotting/topics_per_class.html#topics-per-class","text":"Visualize topics per class Parameters: Name Type Description Default topic_model A fitted BERTopic instance. required topics_per_class pd . DataFrame The topics you would like to be visualized with the corresponding topic representation required top_n_topics int To visualize the most frequent topics instead of all 10 topics List [ int ] Select which topics you would like to be visualized None normalize_frequency bool Whether to normalize each topic's frequency individually False custom_labels bool Whether to use custom topic labels that were defined using topic_model.set_topic_labels . False width int The width of the figure. 1250 height int The height of the figure. 900 Returns: Type Description go . Figure A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: topics_per_class = topic_model . topics_per_class ( docs , classes ) topic_model . visualize_topics_per_class ( topics_per_class ) Or if you want to save the resulting figure: fig = topic_model . visualize_topics_per_class ( topics_per_class ) fig . write_html ( \"path/to/file.html\" ) Source code in bertopic\\plotting\\_topics_per_class.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def visualize_topics_per_class ( topic_model , topics_per_class : pd . DataFrame , top_n_topics : int = 10 , topics : List [ int ] = None , normalize_frequency : bool = False , custom_labels : bool = False , width : int = 1250 , height : int = 900 ) -> go . Figure : \"\"\" Visualize topics per class Arguments: topic_model: A fitted BERTopic instance. topics_per_class: The topics you would like to be visualized with the corresponding topic representation top_n_topics: To visualize the most frequent topics instead of all topics: Select which topics you would like to be visualized normalize_frequency: Whether to normalize each topic's frequency individually custom_labels: Whether to use custom topic labels that were defined using `topic_model.set_topic_labels`. width: The width of the figure. height: The height of the figure. Returns: A plotly.graph_objects.Figure including all traces Examples: To visualize the topics per class, simply run: ```python topics_per_class = topic_model.topics_per_class(docs, classes) topic_model.visualize_topics_per_class(topics_per_class) ``` Or if you want to save the resulting figure: ```python fig = topic_model.visualize_topics_per_class(topics_per_class) fig.write_html(\"path/to/file.html\") ``` <iframe src=\"../../getting_started/visualization/topics_per_class.html\" style=\"width:1400px; height: 1000px; border: 0px;\"\"></iframe> \"\"\" colors = [ \"#E69F00\" , \"#56B4E9\" , \"#009E73\" , \"#F0E442\" , \"#D55E00\" , \"#0072B2\" , \"#CC79A7\" ] # Select topics based on top_n and topics args freq_df = topic_model . get_topic_freq () freq_df = freq_df . loc [ freq_df . Topic != - 1 , :] if topics is not None : selected_topics = list ( topics ) elif top_n_topics is not None : selected_topics = sorted ( freq_df . Topic . to_list ()[: top_n_topics ]) else : selected_topics = sorted ( freq_df . Topic . to_list ()) # Prepare data if topic_model . custom_labels_ is not None and custom_labels : topic_names = { key : topic_model . custom_labels_ [ key + topic_model . _outliers ] for key , _ in topic_model . topic_labels_ . items ()} else : topic_names = { key : value [: 40 ] + \"...\" if len ( value ) > 40 else value for key , value in topic_model . topic_labels_ . items ()} topics_per_class [ \"Name\" ] = topics_per_class . Topic . map ( topic_names ) data = topics_per_class . loc [ topics_per_class . Topic . isin ( selected_topics ), :] # Add traces fig = go . Figure () for index , topic in enumerate ( selected_topics ): if index == 0 : visible = True else : visible = \"legendonly\" trace_data = data . loc [ data . Topic == topic , :] topic_name = trace_data . Name . values [ 0 ] words = trace_data . Words . values if normalize_frequency : x = normalize ( trace_data . Frequency . values . reshape ( 1 , - 1 ))[ 0 ] else : x = trace_data . Frequency fig . add_trace ( go . Bar ( y = trace_data . Class , x = x , visible = visible , marker_color = colors [ index % 7 ], hoverinfo = \"text\" , name = topic_name , orientation = \"h\" , hovertext = [ f '<b>Topic { topic } </b><br>Words: { word } ' for word in words ])) # Styling of the visualization fig . update_xaxes ( showgrid = True ) fig . update_yaxes ( showgrid = True ) fig . update_layout ( xaxis_title = \"Normalized Frequency\" if normalize_frequency else \"Frequency\" , yaxis_title = \"Class\" , title = { 'text' : \"<b>Topics per Class\" , 'y' : .95 , 'x' : 0.40 , 'xanchor' : 'center' , 'yanchor' : 'top' , 'font' : dict ( size = 22 , color = \"Black\" ) }, template = \"simple_white\" , width = width , height = height , hoverlabel = dict ( bgcolor = \"white\" , font_size = 16 , font_family = \"Rockwell\" ), legend = dict ( title = \"<b>Global Topic Representation\" , ) ) return fig","title":"Topics per Class"},{"location":"getting_started/clustering/clustering.html","text":"After reducing the dimensionality of our input embeddings, we need to cluster them into groups of similar embeddings in order to extract our topics. This process of clustering is quite important because the more performant our clustering technique the more accurate our topic representations are. In BERTopic, we typically use HDBSCAN as it is quite capable of capturing structures with different densities. However, there is not perfect clustering model and you might want to be using something entirely different for you use case. Moreover, what if a new state-of-the-art model is released tomorrow? We would like to able to use that in BERTopic, right? As a result, the hdbscan_model parameter in BERTopic now allows for a variety of clustering models. To do so, the class should have the following attributes: .fit(X) A function that can be used to fit the model .predict(X) A predict function that transforms the input to cluster labels .labels_ The labels after fitting the model In other words, it should have the following structure: class ClusterModel : def fit ( self , X ): self . labels_ = None return self def predict ( self , X ): return X In this tutorial, I will show you how to use several clustering algorithms in BERTopic. HDBSCAN \u00b6 As a default, BERTopic uses HDBSCAN to perform its clustering. To use a HDBSCAN model with custom parameters, we simply define it and pass it to BERTopic: from bertopic import BERTopic from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 15 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) Here, we can define any parameters in HDBSCAN to optimize for the best performance based on whatever validation metrics that you are using. k-Means \u00b6 Although HDBSCAN works quite well in BERTopic and is typically advised, you might want to be using k-Means instead. It allows you to select how many clusters you would like and forces every single point to be in a cluster. Therefore, no outliers will be created. This has also has disadvantages. When you force every single point in a cluster, it will mean that the cluster is highly likely to contain noise which can hurt the topic representations. As a small tip, using the vectorizer_model=CountVectorizer(stop_words=\"english\") helps quite a bit to then improve the topic representation. Having said that, using k-Means is quite straightforward: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model ) Note As you might have noticed, the cluster_model is passed to hdbscan_model which might be a bit confusing considering you are not passing a HDBSCAN model. For now, the name of the parameter is kept the same to adhere to the current state of the API. Changing the name could lead to deprecation issues, which I want to prevent as much as possible. Agglomerative Clustering \u00b6 Like k-Means, there are a bunch more clustering algorithms in sklearn that you can be using. Some of these models do not have a .predict() method but still can be used in BERTopic. However, using BERTopic's .transform() function will then give errors. Here, we will demonstrate Agglomerative Clustering: from bertopic import BERTopic from sklearn.cluster import AgglomerativeClustering cluster_model = AgglomerativeClustering ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model )","title":"Clustering"},{"location":"getting_started/clustering/clustering.html#hdbscan","text":"As a default, BERTopic uses HDBSCAN to perform its clustering. To use a HDBSCAN model with custom parameters, we simply define it and pass it to BERTopic: from bertopic import BERTopic from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 15 , metric = 'euclidean' , cluster_selection_method = 'eom' , prediction_data = True ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) Here, we can define any parameters in HDBSCAN to optimize for the best performance based on whatever validation metrics that you are using.","title":"HDBSCAN"},{"location":"getting_started/clustering/clustering.html#k-means","text":"Although HDBSCAN works quite well in BERTopic and is typically advised, you might want to be using k-Means instead. It allows you to select how many clusters you would like and forces every single point to be in a cluster. Therefore, no outliers will be created. This has also has disadvantages. When you force every single point in a cluster, it will mean that the cluster is highly likely to contain noise which can hurt the topic representations. As a small tip, using the vectorizer_model=CountVectorizer(stop_words=\"english\") helps quite a bit to then improve the topic representation. Having said that, using k-Means is quite straightforward: from bertopic import BERTopic from sklearn.cluster import KMeans cluster_model = KMeans ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model ) Note As you might have noticed, the cluster_model is passed to hdbscan_model which might be a bit confusing considering you are not passing a HDBSCAN model. For now, the name of the parameter is kept the same to adhere to the current state of the API. Changing the name could lead to deprecation issues, which I want to prevent as much as possible.","title":"k-Means"},{"location":"getting_started/clustering/clustering.html#agglomerative-clustering","text":"Like k-Means, there are a bunch more clustering algorithms in sklearn that you can be using. Some of these models do not have a .predict() method but still can be used in BERTopic. However, using BERTopic's .transform() function will then give errors. Here, we will demonstrate Agglomerative Clustering: from bertopic import BERTopic from sklearn.cluster import AgglomerativeClustering cluster_model = AgglomerativeClustering ( n_clusters = 50 ) topic_model = BERTopic ( hdbscan_model = cluster_model )","title":"Agglomerative Clustering"},{"location":"getting_started/ctfidf/ctfidf.html","text":"c-TF-IDF \u00b6 In BERTopic, in order to get an accurate representation of the topics from our bag-of-words matrix, TF-IDF was adjusted to work on a cluster/categorical/topic-level instead of a document-level. This adjusted TF-IDF representation is called c-TF-IDF takes into account what makes the documents in once cluster different from documents in another cluster: Each cluster is converted to a single document instead of a set of documents. Then, we extract the frequency of word x in class c , where c refers to the cluster we created before. This results in our class-based tf representation. This representation is L1-normalized to account for the differences in topic sizes. Then, we take take the logarithm of one plus the average number of words per class A divided by the frequency of word x across all classes. We add plus one within the logarithm to force values to be positive. This results in our class-based idf representation. Like with the classic TF-IDF, we then multiply tf with idf to get the importance score per word in each class. In other words, the classical TF-IDF procedure is not used here but a modified version of the algorithm that allows for a much better representation. This class-based TF-IDF representation is enabled by default in BERTopic. However, we can explicitly pass it to BERTopic through the ctfidf_model allowing for parameter tuning and the customization of the topic extraction technique: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer () topic_model = BERTopic ( ctfidf_model = ctfidf_model ) Parameters \u00b6 There are two parameters worth exploring in the ClassTfidfTransformer , namely bm25_weighting and reduce_frequent_words . bm25_weighting \u00b6 The bm25_weighting is a boolean parameter that indicates whether a class-based BM-25 weighting measure is used instead of the default method as defined in the formula at the beginning of this page. Instead of the using the following weighting scheme: the class-based BM-25 weighting is used instead: At smaller datasets, this variant can be more robust to stop words that appear in your data. It can be enabled as follows: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( bm25_weighting = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model ) reduce_frequent_words \u00b6 Some words appear quite often in every topic but are generally not considered stop words as found in the CountVectorizer(stop_words=\"english\") list. To further reduce these frequent words, we can use reduce_frequent_words to take the square root of the term frequency after applying the weighting scheme. Instead of the default term frequency: we take the square root of the term frequency after applying normalizing the frequency matrix: Although seemingly a small change, it can have quite a large effect on the number of stop words in the resulting topic representations. It can be enabled as follows: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( reduce_frequent_words = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model ) Tip Both parameters can be used simultaneously: ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)","title":"c-TF-IDF"},{"location":"getting_started/ctfidf/ctfidf.html#c-tf-idf","text":"In BERTopic, in order to get an accurate representation of the topics from our bag-of-words matrix, TF-IDF was adjusted to work on a cluster/categorical/topic-level instead of a document-level. This adjusted TF-IDF representation is called c-TF-IDF takes into account what makes the documents in once cluster different from documents in another cluster: Each cluster is converted to a single document instead of a set of documents. Then, we extract the frequency of word x in class c , where c refers to the cluster we created before. This results in our class-based tf representation. This representation is L1-normalized to account for the differences in topic sizes. Then, we take take the logarithm of one plus the average number of words per class A divided by the frequency of word x across all classes. We add plus one within the logarithm to force values to be positive. This results in our class-based idf representation. Like with the classic TF-IDF, we then multiply tf with idf to get the importance score per word in each class. In other words, the classical TF-IDF procedure is not used here but a modified version of the algorithm that allows for a much better representation. This class-based TF-IDF representation is enabled by default in BERTopic. However, we can explicitly pass it to BERTopic through the ctfidf_model allowing for parameter tuning and the customization of the topic extraction technique: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer () topic_model = BERTopic ( ctfidf_model = ctfidf_model )","title":"c-TF-IDF"},{"location":"getting_started/ctfidf/ctfidf.html#parameters","text":"There are two parameters worth exploring in the ClassTfidfTransformer , namely bm25_weighting and reduce_frequent_words .","title":"Parameters"},{"location":"getting_started/ctfidf/ctfidf.html#bm25_weighting","text":"The bm25_weighting is a boolean parameter that indicates whether a class-based BM-25 weighting measure is used instead of the default method as defined in the formula at the beginning of this page. Instead of the using the following weighting scheme: the class-based BM-25 weighting is used instead: At smaller datasets, this variant can be more robust to stop words that appear in your data. It can be enabled as follows: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( bm25_weighting = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model )","title":"bm25_weighting"},{"location":"getting_started/ctfidf/ctfidf.html#reduce_frequent_words","text":"Some words appear quite often in every topic but are generally not considered stop words as found in the CountVectorizer(stop_words=\"english\") list. To further reduce these frequent words, we can use reduce_frequent_words to take the square root of the term frequency after applying the weighting scheme. Instead of the default term frequency: we take the square root of the term frequency after applying normalizing the frequency matrix: Although seemingly a small change, it can have quite a large effect on the number of stop words in the resulting topic representations. It can be enabled as follows: from bertopic import BERTopic from bertopic.vectorizers import ClassTfidfTransformer ctfidf_model = ClassTfidfTransformer ( reduce_frequent_words = True ) topic_model = BERTopic ( ctfidf_model = ctfidf_model ) Tip Both parameters can be used simultaneously: ClassTfidfTransformer(bm25_weighting=True, reduce_frequent_words=True)","title":"reduce_frequent_words"},{"location":"getting_started/dim_reduction/dim_reduction.html","text":"One important aspect of BERTopic is dimensionality reduction of the embeddings. Typically, embeddings are at least 384 in length and many clustering algorithms have difficulty clustering in such a high dimensional space. A solution is to reduce the dimensionality of the embeddings to a workable dimensional space (e.g., 5) for clustering algorithms to work with. In BERTopic, we typically use UMAP as it is able to capture both the local and global high-dimensional space in lower dimensions. However, there are other solutions out there, such as PCA that users might be interested in trying out. We have seen that developments in the artificial intelligence fields are quite fast and that whatever mights be state-of-the-art now, could be different a year or even months later. Therefore, BERTopic allows you to use any dimensionality reduction algorithm that you would like to be using. As a result, the umap_model parameter in BERTopic now allows for a variety of dimensionality reduction models. To do so, the class should have the following attributes: .fit(X) A function that can be used to fit the model .transform(X) A transform function that transforms the input to a lower dimensional size In other words, it should have the following structure: class DimensionalityReduction : def fit ( self , X ): return self def transform ( self , X ): return X In this tutorial, I will show you how to use several dimensionality reduction algorithms in BERTopic. UMAP \u00b6 As a default, BERTopic uses UMAP to perform its dimensionality reduction. To use a UMAP model with custom parameters, we simply define it and pass it to BERTopic: from bertopic import BERTopic from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) topic_model = BERTopic ( umap_model = umap_model ) Here, we can define any parameters in UMAP to optimize for the best performance based on whatever validation metrics that you are using. PCA \u00b6 Although UMAP works quite well in BERTopic and is typically advised, you might want to be using PCA instead. It can be faster to train and to perform inference with. To use PCA, we can simply import it from sklearn and pass it to the umap_model parameter: from bertopic import BERTopic from sklearn.decomposition import PCA dim_model = PCA ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model ) As a small note, PCA and k-Means have worked quite well in my experiments and might be interesting to use instead of PCA and HDBSCAN. Note As you might have noticed, the dim_model is passed to umap_model which might be a bit confusing considering you are not passing a UMAP model. For now, the name of the parameter is kept the same to adhere to the current state of the API. Changing the name could lead to deprecation issues, which I want to prevent as much as possible. Truncated SVD \u00b6 Like PCA, there are a bunch more dimensionality reduction techniques in sklearn that you can be using. Here, we will demonstrate Truncated SVD but any model can be used as long as it has both a .fit() and .transform() method: from bertopic import BERTopic from sklearn.decomposition import TruncatedSVD dim_model = TruncatedSVD ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model )","title":"Dimensionality Reduction"},{"location":"getting_started/dim_reduction/dim_reduction.html#umap","text":"As a default, BERTopic uses UMAP to perform its dimensionality reduction. To use a UMAP model with custom parameters, we simply define it and pass it to BERTopic: from bertopic import BERTopic from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' ) topic_model = BERTopic ( umap_model = umap_model ) Here, we can define any parameters in UMAP to optimize for the best performance based on whatever validation metrics that you are using.","title":"UMAP"},{"location":"getting_started/dim_reduction/dim_reduction.html#pca","text":"Although UMAP works quite well in BERTopic and is typically advised, you might want to be using PCA instead. It can be faster to train and to perform inference with. To use PCA, we can simply import it from sklearn and pass it to the umap_model parameter: from bertopic import BERTopic from sklearn.decomposition import PCA dim_model = PCA ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model ) As a small note, PCA and k-Means have worked quite well in my experiments and might be interesting to use instead of PCA and HDBSCAN. Note As you might have noticed, the dim_model is passed to umap_model which might be a bit confusing considering you are not passing a UMAP model. For now, the name of the parameter is kept the same to adhere to the current state of the API. Changing the name could lead to deprecation issues, which I want to prevent as much as possible.","title":"PCA"},{"location":"getting_started/dim_reduction/dim_reduction.html#truncated-svd","text":"Like PCA, there are a bunch more dimensionality reduction techniques in sklearn that you can be using. Here, we will demonstrate Truncated SVD but any model can be used as long as it has both a .fit() and .transform() method: from bertopic import BERTopic from sklearn.decomposition import TruncatedSVD dim_model = TruncatedSVD ( n_components = 5 ) topic_model = BERTopic ( umap_model = dim_model )","title":"Truncated SVD"},{"location":"getting_started/embeddings/embeddings.html","text":"Embedding Models \u00b6 In this tutorial, we will be going through the embedding models that can be used in BERTopic. Having the option to choose embedding models allows you to leverage pre-trained embeddings that suit your use case. Moreover, it helps to create a topic when you have little data available. Sentence Transformers \u00b6 You can select any model from sentence-transformers here and pass it through BERTopic with embedding_model : from bertopic import BERTopic topic_model = BERTopic ( embedding_model = \"all-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Tip! This embedding back-end was put here first for a reason, sentence-transformers works amazing out-of-the-box! Playing around with different models can give you great results. Also, make sure to frequently visit this page as new models are often released. \ud83e\udd17 Hugging Face Transformers \u00b6 To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models): from transformers.pipelines import pipeline embedding_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) topic_model = BERTopic ( embedding_model = embedding_model ) Tip! These transformers also work quite well using sentence-transformers which has a number of optimizations tricks that make using it a bit faster. Flair \u00b6 Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) topic_model = BERTopic ( embedding_model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to BERTopic in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) topic_model = BERTopic ( embedding_model = document_glove_embeddings ) Spacy \u00b6 Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. To use Spacy's non-transformer models in BERTopic: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) topic_model = BERTopic ( embedding_model = nlp ) Universal Sentence Encoder (USE) \u00b6 The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. Using USE in BERTopic is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) topic_model = BERTopic ( embedding_model = embedding_model ) Gensim \u00b6 BERTopic supports the gensim.downloader module, which allows it to download any word embedding model supported by Gensim. Typically, these are Glove, Word2Vec, or FastText embeddings: import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) topic_model = BERTopic ( embedding_model = ft ) Tip! Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled. Word + Document Embeddings \u00b6 You might want to be using different language models for creating document- and word-embeddings. For example, while SentenceTransformers might be great in embedding sentences and documents, you might prefer to use FastText to create the word embeddings. from bertopic.backend import WordDocEmbedder import gensim.downloader as api from sentence_transformers import SentenceTransformer # Word embedding model ft = api . load ( 'fasttext-wiki-news-subwords-300' ) # Document embedding model embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Create a model that uses both language models and pass it through BERTopic word_doc_embedder = WordDocEmbedder ( embedding_model = embedding_model , word_embedding_model = ft ) topic_model = BERTopic ( embedding_model = word_doc_embedder ) Note The word embeddings are only created when applying MMR. In other words, to use this feature, you will have to select a value for diversity between 0 and 1 when instantiating BERTopic. Custom Backend \u00b6 If your backend or model cannot be found in the ones currently available, you can use the bertopic.backend.BaseEmbedder class to create your backend. Below, you will find an example of creating a SentenceTransformer backend for BERTopic: from bertopic.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = embedding_model ) # Pass custom backend to bertopic topic_model = BERTopic ( embedding_model = custom_embedder ) Custom Embeddings \u00b6 The base models in BERTopic are BERT-based models that work well with document similarity tasks. Your documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train our topic model using our pre-trained sentence-transformers embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method. TF-IDF \u00b6 As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # Train our topic model using TF-IDF vectors topic_model = BERTopic ( stop_words = \"english\" ) topics , probs = topic_model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing the dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast.","title":"Embedding Models"},{"location":"getting_started/embeddings/embeddings.html#embedding-models","text":"In this tutorial, we will be going through the embedding models that can be used in BERTopic. Having the option to choose embedding models allows you to leverage pre-trained embeddings that suit your use case. Moreover, it helps to create a topic when you have little data available.","title":"Embedding Models"},{"location":"getting_started/embeddings/embeddings.html#sentence-transformers","text":"You can select any model from sentence-transformers here and pass it through BERTopic with embedding_model : from bertopic import BERTopic topic_model = BERTopic ( embedding_model = \"all-MiniLM-L6-v2\" ) Or select a SentenceTransformer model with your parameters: from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) topic_model = BERTopic ( embedding_model = sentence_model ) Tip! This embedding back-end was put here first for a reason, sentence-transformers works amazing out-of-the-box! Playing around with different models can give you great results. Also, make sure to frequently visit this page as new models are often released.","title":"Sentence Transformers"},{"location":"getting_started/embeddings/embeddings.html#hugging-face-transformers","text":"To use a Hugging Face transformers model, load in a pipeline and point to any model found on their model hub (https://huggingface.co/models): from transformers.pipelines import pipeline embedding_model = pipeline ( \"feature-extraction\" , model = \"distilbert-base-cased\" ) topic_model = BERTopic ( embedding_model = embedding_model ) Tip! These transformers also work quite well using sentence-transformers which has a number of optimizations tricks that make using it a bit faster.","title":"\ud83e\udd17 Hugging Face Transformers"},{"location":"getting_started/embeddings/embeddings.html#flair","text":"Flair allows you to choose almost any embedding model that is publicly available. Flair can be used as follows: from flair.embeddings import TransformerDocumentEmbeddings roberta = TransformerDocumentEmbeddings ( 'roberta-base' ) topic_model = BERTopic ( embedding_model = roberta ) You can select any \ud83e\udd17 transformers model here . Moreover, you can also use Flair to use word embeddings and pool them to create document embeddings. Under the hood, Flair simply averages all word embeddings in a document. Then, we can easily pass it to BERTopic in order to use those word embeddings as document embeddings: from flair.embeddings import WordEmbeddings , DocumentPoolEmbeddings glove_embedding = WordEmbeddings ( 'crawl' ) document_glove_embeddings = DocumentPoolEmbeddings ([ glove_embedding ]) topic_model = BERTopic ( embedding_model = document_glove_embeddings )","title":"Flair"},{"location":"getting_started/embeddings/embeddings.html#spacy","text":"Spacy is an amazing framework for processing text. There are many models available across many languages for modeling text. To use Spacy's non-transformer models in BERTopic: import spacy nlp = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) Using spacy-transformer models: import spacy spacy . prefer_gpu () nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) topic_model = BERTopic ( embedding_model = nlp ) If you run into memory issues with spacy-transformer models, try: import spacy from thinc.api import set_gpu_allocator , require_gpu nlp = spacy . load ( \"en_core_web_trf\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) set_gpu_allocator ( \"pytorch\" ) require_gpu ( 0 ) topic_model = BERTopic ( embedding_model = nlp )","title":"Spacy"},{"location":"getting_started/embeddings/embeddings.html#universal-sentence-encoder-use","text":"The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the documents. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. Using USE in BERTopic is rather straightforward: import tensorflow_hub embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) topic_model = BERTopic ( embedding_model = embedding_model )","title":"Universal Sentence Encoder (USE)"},{"location":"getting_started/embeddings/embeddings.html#gensim","text":"BERTopic supports the gensim.downloader module, which allows it to download any word embedding model supported by Gensim. Typically, these are Glove, Word2Vec, or FastText embeddings: import gensim.downloader as api ft = api . load ( 'fasttext-wiki-news-subwords-300' ) topic_model = BERTopic ( embedding_model = ft ) Tip! Gensim is primarily used for Word Embedding models. This works typically best for short documents since the word embeddings are pooled.","title":"Gensim"},{"location":"getting_started/embeddings/embeddings.html#word-document-embeddings","text":"You might want to be using different language models for creating document- and word-embeddings. For example, while SentenceTransformers might be great in embedding sentences and documents, you might prefer to use FastText to create the word embeddings. from bertopic.backend import WordDocEmbedder import gensim.downloader as api from sentence_transformers import SentenceTransformer # Word embedding model ft = api . load ( 'fasttext-wiki-news-subwords-300' ) # Document embedding model embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) # Create a model that uses both language models and pass it through BERTopic word_doc_embedder = WordDocEmbedder ( embedding_model = embedding_model , word_embedding_model = ft ) topic_model = BERTopic ( embedding_model = word_doc_embedder ) Note The word embeddings are only created when applying MMR. In other words, to use this feature, you will have to select a value for diversity between 0 and 1 when instantiating BERTopic.","title":"Word + Document Embeddings"},{"location":"getting_started/embeddings/embeddings.html#custom-backend","text":"If your backend or model cannot be found in the ones currently available, you can use the bertopic.backend.BaseEmbedder class to create your backend. Below, you will find an example of creating a SentenceTransformer backend for BERTopic: from bertopic.backend import BaseEmbedder from sentence_transformers import SentenceTransformer class CustomEmbedder ( BaseEmbedder ): def __init__ ( self , embedding_model ): super () . __init__ () self . embedding_model = embedding_model def embed ( self , documents , verbose = False ): embeddings = self . embedding_model . encode ( documents , show_progress_bar = verbose ) return embeddings # Create custom backend embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) custom_embedder = CustomEmbedder ( embedding_model = embedding_model ) # Pass custom backend to bertopic topic_model = BERTopic ( embedding_model = custom_embedder )","title":"Custom Backend"},{"location":"getting_started/embeddings/embeddings.html#custom-embeddings","text":"The base models in BERTopic are BERT-based models that work well with document similarity tasks. Your documents, however, might be too specific for a general pre-trained model to be used. Fortunately, you can use embedding model in BERTopic to create document features. You only need to prepare the document embeddings yourself and pass them through fit_transform of BERTopic: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train our topic model using our pre-trained sentence-transformers embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) As you can see above, we used a SentenceTransformer model to create the embedding. You could also have used \ud83e\udd17 transformers , Doc2Vec , or any other embedding method.","title":"Custom Embeddings"},{"location":"getting_started/embeddings/embeddings.html#tf-idf","text":"As mentioned above, any embedding technique can be used. However, when running umap, the typical distance metric is cosine which does not work quite well for a TF-IDF matrix. Instead, BERTopic will recognize that a sparse matrix is passed and use hellinger instead which works quite well for the similarity between probability distributions. We simply create a TF-IDF matrix and use them as embeddings in our fit_transform method: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer # Create TF-IDF sparse matrix docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] vectorizer = TfidfVectorizer ( min_df = 5 ) embeddings = vectorizer . fit_transform ( docs ) # Train our topic model using TF-IDF vectors topic_model = BERTopic ( stop_words = \"english\" ) topics , probs = topic_model . fit_transform ( docs , embeddings ) Here, you will probably notice that creating the embeddings is quite fast whereas fit_transform is quite slow. This is to be expected as reducing the dimensionality of a large sparse matrix takes some time. The inverse of using transformer embeddings is true: creating the embeddings is slow whereas fit_transform is quite fast.","title":"TF-IDF"},{"location":"getting_started/guided/guided.html","text":"Guided Topic Modeling or Seeded Topic Modeling is a collection of techniques that guides the topic modeling approach by setting a number of seed topics in which the model will converge to. These techniques allow the user to set a pre-defined number of topic representations that are sure to be in documents. For example, take an IT-business that has a ticket system for the software their clients use. Those tickets may typically contain information about a specific bug regarding login issues that the IT-business is aware off. To model that bug, we can create a seed topic representation containing the words bug , login , password , and username . By defining those words, a Guided Topic Modeling approach will try to converge at least one topic to those words. Guided BERTopic has two main steps: First, we create embeddings for each seeded topics by joining them and passing them through the document embedder. These embeddings will be compared with the existing document embeddings through cosine similarity and assigned a label. If the document is most similar to a seeded topic, then it will get that topic's label. If it is most similar to the average document embedding, it will get the -1 label. These labels are then passed through UMAP to create a semi-supervised approach that should nudge the topic creation to the seeded topics. Second, we take all words in seed_topic_list and assign them a multiplier larger than 1. Those multipliers will be used to increase the IDF values of the words across all topics thereby increasing the likelihood that a seeded topic word will appear in a topic. This does, however, also increase the chance of an irrelevant topic having unrelated words. In practice, this should not be an issue since the IDF value is likely to remain low regardless of the multiplier. The multiplier is now a fixed value but may change to something more elegant, like taking the distribution of IDF values and its position into account when defining the multiplier. Example \u00b6 To demonstrate Guided BERTopic, we use the 20 Newsgroups dataset as our example. We have frequently used this dataset in BERTopic examples and we sometimes see a topic generated about health with words as drug and cancer being important. However, due to the stocastisch nature of UMAP this topic is not always found. In order to guide BERTopic to that topic, we create a seed topic list that we pass through our model. However, there may be several other topics that we know should be in the documents. Let's also initialize those: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] seed_topic_list = [[ \"drug\" , \"cancer\" , \"drugs\" , \"doctor\" ], [ \"windows\" , \"drive\" , \"dos\" , \"file\" ], [ \"space\" , \"launch\" , \"orbit\" , \"lunar\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) AS you can see above, the seed_topic_list contains a list of topic representations. By defining the above topics BERTopic is more likely to model the defined seeded topics. However, BERTopic is merely nudged towards creating those topics. In practice, if the seeded topics do not exist or might be divided into smaller topics, then they will not be modeled. Thus, seed topics need to be accurate in order to accurately converge towards them.","title":"Guided Topic Modeling"},{"location":"getting_started/guided/guided.html#example","text":"To demonstrate Guided BERTopic, we use the 20 Newsgroups dataset as our example. We have frequently used this dataset in BERTopic examples and we sometimes see a topic generated about health with words as drug and cancer being important. However, due to the stocastisch nature of UMAP this topic is not always found. In order to guide BERTopic to that topic, we create a seed topic list that we pass through our model. However, there may be several other topics that we know should be in the documents. Let's also initialize those: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] seed_topic_list = [[ \"drug\" , \"cancer\" , \"drugs\" , \"doctor\" ], [ \"windows\" , \"drive\" , \"dos\" , \"file\" ], [ \"space\" , \"launch\" , \"orbit\" , \"lunar\" ]] topic_model = BERTopic ( seed_topic_list = seed_topic_list ) topics , probs = topic_model . fit_transform ( docs ) AS you can see above, the seed_topic_list contains a list of topic representations. By defining the above topics BERTopic is more likely to model the defined seeded topics. However, BERTopic is merely nudged towards creating those topics. In practice, if the seeded topics do not exist or might be divided into smaller topics, then they will not be modeled. Thus, seed topics need to be accurate in order to accurately converge towards them.","title":"Example"},{"location":"getting_started/hierarchicaltopics/hierarchicaltopics.html","text":"When tweaking your topic model, the number of topics that are generated has a large effect on the quality of the topic representations. Some topics could be merged together and having an understanding of the effect will help you understand which topics should and which should not be merged. That is where hierarchical topic modeling comes in. It tries to model the possible hierarchical nature of the topics you have created in order to understand which topics are similar to each other. Moreover, you will have more insight into sub-topics that might exist in your data. In BERTopic, we can approximate this potential hierarchy by making use of our topic-term matrix (c-TF-IDF matrix). This matrix contains information about the importance of every word in every topic and makes for a nice numerical representation of our topics. The smaller the distance between two c-TF-IDF representations, the more similar we assume they are. In practice, this process of merging topics is done through the hierarchical clustering capabilities of scipy (see here ). It allows for several linkage methods through which we can approximate our topic hierarchy. As a default, we are using the ward but many others are availabe. Whenever we merge two topics, we can calculate the c-TF-IDF representation of these two merged by summing their bag-of-words representation. We assume that two sets of topics are merged and that all others are kept the same, regardless of their location in the hierarchy. This helps us isolate the potential effect of merging sets of topics. As a result, we can see the topic representation at each level in the tree. Example \u00b6 To demonstrate hierarchical topic modeling with BERTopic, we use the 20 Newsgroups dataset to see how the topics that we uncover are represented in the 20 categories of documents. First, we train a basic BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) Next, we can use our fitted BERTopic model to extract possible hierarchies from our c-TF-IDF matrix: hierarchical_topics = topic_model . hierarchical_topics ( docs ) The resulting hierarchical_topics is a dataframe in which merged topics are described. For example, if you would merge two topics, what would the topic representation of the new topic be? Linkage functions \u00b6 When creating the potential hierarchical nature of topics, we use Scipy's ward linkage function as a default to generate the hierarchy. However, you might want to use a different linkage function for your use case, such as single , complete , average , centroid , or median . In BERTopic, you can define the linkage function yourself, including the distance function that you would like to use: from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Hierarchical topics linkage_function = lambda x : sch . linkage ( x , 'single' , optimal_ordering = True ) hierarchical_topics = topic_model . hierarchical_topics ( docs , linkage_function = linkage_function ) Visualizations \u00b6 To visualize these results, we can start by running a familiar function, namely topic_model.visualize_hierarchy : topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics together. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes. Although this gives a nice overview of the potential hierarchy, hovering over all black circles can be tiresome. Instead, we can use topic_model.get_topic_tree to create a text-based representation of this hierarchy. Although the general structure is more difficult to view, we can see better which topics could be logically merged: >>> tree = topic_model . get_topic_tree ( hierarchical_topics ) >>> print ( tree ) . \u2514\u2500 atheists_atheism_god_moral_atheist \u251c\u2500 atheists_atheism_god_atheist_argument \u2502 \u251c\u2500\u25a0\u2500\u2500 atheists_atheism_god_atheist_argument \u2500\u2500 Topic : 21 \u2502 \u2514\u2500\u25a0\u2500\u2500 br_god_exist_genetic_existence \u2500\u2500 Topic : 124 \u2514\u2500\u25a0\u2500\u2500 moral_morality_objective_immoral_morals \u2500\u2500 Topic : 29 Click here to view the full tree. . \u251c\u2500people_armenian_said_god_armenians \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u2502 \u251c\u2500jehovah_lord_mormon_mcconkie_god \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ra_satan_thou_god_lucifer \u2500\u2500 Topic: 94 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500jehovah_lord_mormon_mcconkie_unto \u2500\u2500 Topic: 78 \u2502 \u2502 \u2502 \u2514\u2500jesus_mary_god_hell_sin \u2502 \u2502 \u2502 \u251c\u2500jesus_hell_god_eternal_heaven \u2502 \u2502 \u2502 \u2502 \u251c\u2500hell_jesus_eternal_god_heaven \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jesus_tomb_disciples_resurrection_john \u2500\u2500 Topic: 69 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500hell_eternal_god_jesus_heaven \u2500\u2500 Topic: 53 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500aaron_baptism_sin_law_god \u2500\u2500 Topic: 89 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mary_sin_maria_priest_conception \u2500\u2500 Topic: 56 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500marriage_married_marry_ceremony_marriages \u2500\u2500 Topic: 110 \u2502 \u2514\u2500people_armenian_armenians_said_mr \u2502 \u251c\u2500people_armenian_armenians_said_israel \u2502 \u2502 \u251c\u2500god_homosexual_homosexuality_atheists_sex \u2502 \u2502 \u2502 \u251c\u2500homosexual_homosexuality_sex_gay_homosexuals \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500kinsey_sex_gay_men_sexual \u2500\u2500 Topic: 44 \u2502 \u2502 \u2502 \u2502 \u2514\u2500homosexuality_homosexual_sin_homosexuals_gay \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gay_homosexual_homosexuals_sexual_cramer \u2500\u2500 Topic: 50 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500homosexuality_homosexual_sin_paul_sex \u2500\u2500 Topic: 27 \u2502 \u2502 \u2502 \u2514\u2500god_atheists_atheism_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jim_context_challenges_articles_quote \u2500\u2500 Topic: 36 \u2502 \u2502 \u2502 \u2502 \u2514\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500islam_quran_islamic_rushdie_muslims \u2500\u2500 Topic: 31 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500judas_scripture_bible_books_greek \u2500\u2500 Topic: 33 \u2502 \u2502 \u2502 \u2514\u2500atheists_atheism_god_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500atheists_atheism_god_atheist_argument \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500atheists_atheism_god_atheist_argument \u2500\u2500 Topic: 21 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500br_god_exist_genetic_existence \u2500\u2500 Topic: 124 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500moral_morality_objective_immoral_morals \u2500\u2500 Topic: 29 \u2502 \u2502 \u2514\u2500armenian_armenians_people_israel_said \u2502 \u2502 \u251c\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500tax_rights_government_income_taxes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rights_right_slavery_slaves_residence \u2500\u2500 Topic: 106 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tax_government_taxes_income_libertarians \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500government_libertarians_libertarian_regulation_party \u2500\u2500 Topic: 58 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tax_taxes_income_billion_deficit \u2500\u2500 Topic: 41 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500gun_guns_militia_firearms_amendment \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500blacks_penalty_death_cruel_punishment \u2500\u2500 Topic: 55 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500gun_guns_militia_firearms_amendment \u2500\u2500 Topic: 7 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_jews_turkish \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500israel_israeli_jews_arab_jewish \u2500\u2500 Topic: 4 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500armenian_armenians_turkish_armenia_azerbaijan \u2500\u2500 Topic: 15 \u2502 \u2502 \u2514\u2500stephanopoulos_president_mr_myers_ms \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500serbs_muslims_stephanopoulos_mr_bosnia \u2500\u2500 Topic: 35 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500myers_stephanopoulos_president_ms_mr \u2500\u2500 Topic: 87 \u2502 \u2514\u2500batf_fbi_koresh_compound_gas \u2502 \u251c\u2500\u25a0\u2500\u2500reno_workers_janet_clinton_waco \u2500\u2500 Topic: 77 \u2502 \u2514\u2500batf_fbi_koresh_gas_compound \u2502 \u251c\u2500batf_koresh_fbi_warrant_compound \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500batf_warrant_raid_compound_fbi \u2500\u2500 Topic: 42 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500koresh_batf_fbi_children_compound \u2500\u2500 Topic: 61 \u2502 \u2514\u2500\u25a0\u2500\u2500fbi_gas_tear_bds_building \u2500\u2500 Topic: 23 \u2514\u2500use_like_just_dont_new \u251c\u2500game_team_year_games_like \u2502 \u251c\u2500game_team_games_25_year \u2502 \u2502 \u251c\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u2500window_printer_use_problem_mhz \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_wire_simms_wiring_battery \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_mhz_battery_cpu_heat \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_pds_simm_vram_lc \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500pds_nubus_lc_slot_card \u2500\u2500 Topic: 119 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500simms_simm_vram_meg_dram \u2500\u2500 Topic: 32 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mhz_battery_cpu_heat_speed \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500fan_cpu_heat_sink_fans \u2500\u2500 Topic: 92 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mhz_speed_cpu_fpu_clock \u2500\u2500 Topic: 22 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500monitor_turn_power_computer_electricity \u2500\u2500 Topic: 91 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500battery_batteries_concrete_duo_discharge \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500duo_battery_apple_230_problem \u2500\u2500 Topic: 121 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500battery_batteries_concrete_discharge_temperature \u2500\u2500 Topic: 75 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500leds_uv_blue_light_boards \u2500\u2500 Topic: 66 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500wire_wiring_ground_neutral_outlets \u2500\u2500 Topic: 120 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500scope_scopes_phone_dial_number \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500dial_number_phone_line_output \u2500\u2500 Topic: 93 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500scope_scopes_motorola_generator_oscilloscope \u2500\u2500 Topic: 113 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500celp_dsp_sampling_antenna_digital \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500antenna_antennas_receiver_cable_transmitter \u2500\u2500 Topic: 70 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500celp_dsp_sampling_speech_voice \u2500\u2500 Topic: 52 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_printer_xv_mouse_windows \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_xv_error_widget_problem \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500error_symbol_undefined_xterm_rx \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500symbol_error_undefined_doug_parse \u2500\u2500 Topic: 63 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500rx_remote_server_xdm_xterm \u2500\u2500 Topic: 45 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_xv_widget_application_expose \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_widget_expose_application_event \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gc_mydisplay_draw_gxxor_drawing \u2500\u2500 Topic: 103 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500window_widget_application_expose_event \u2500\u2500 Topic: 25 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500xv_den_polygon_points_algorithm \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500den_polygon_points_algorithm_polygons \u2500\u2500 Topic: 28 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500xv_24bit_image_bit_images \u2500\u2500 Topic: 57 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_mouse_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500scanner_logitech_grayscale_ocr_scanman \u2500\u2500 Topic: 108 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500printer_print_deskjet_hp_ink \u2500\u2500 Topic: 18 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500fonts_font_truetype_tt_atm \u2500\u2500 Topic: 49 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mouse_ghostscript_midi_driver_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500ghostscript_midi_postscript_files_file \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ghostscript_postscript_pageview_ghostview_dsc \u2500\u2500 Topic: 104 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500midi_sound_file_windows_driver \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500location_mar_file_host_rwrr \u2500\u2500 Topic: 83 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500midi_sound_driver_blaster_soundblaster \u2500\u2500 Topic: 98 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mouse_driver_mice_ball_problem \u2500\u2500 Topic: 68 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u25001st_sale_condition_comics_hulk \u2502 \u2502 \u2502 \u2502 \u251c\u2500sale_condition_offer_asking_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500condition_stereo_amp_speakers_asking \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500miles_car_amfm_toyota_cassette \u2500\u2500 Topic: 62 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500amp_speakers_condition_stereo_audio \u2500\u2500 Topic: 24 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500games_sale_pom_cds_shipping \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500pom_cds_sale_shipping_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500size_shipping_sale_condition_mattress \u2500\u2500 Topic: 100 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pom_cds_cd_sale_picture \u2500\u2500 Topic: 37 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500games_game_snes_sega_genesis \u2500\u2500 Topic: 40 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u251c\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500lens_tape_camera_backup_lenses \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500tape_backup_tapes_drive_4mm \u2500\u2500 Topic: 107 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500lens_camera_lenses_zoom_pouch \u2500\u2500 Topic: 114 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u25001st_hulk_comics_art_appears \u2500\u2500 Topic: 105 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500books_book_cover_trek_chemistry \u2500\u2500 Topic: 125 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tickets_hotel_ticket_voucher_package \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hotel_voucher_package_vacation_room \u2500\u2500 Topic: 74 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tickets_ticket_june_airlines_july \u2500\u2500 Topic: 84 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_season_hockey \u2502 \u2502 \u2502 \u251c\u2500game_hockey_team_25_550 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500espn_pt_pts_game_la \u2500\u2500 Topic: 17 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500team_25_game_hockey_550 \u2500\u2500 Topic: 2 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500year_game_hit_baseball_players \u2500\u2500 Topic: 0 \u2502 \u2502 \u2514\u2500bike_car_greek_insurance_msg \u2502 \u2502 \u251c\u2500car_bike_insurance_cars_engine \u2502 \u2502 \u2502 \u251c\u2500car_insurance_cars_radar_engine \u2502 \u2502 \u2502 \u2502 \u251c\u2500insurance_health_private_care_canada \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500insurance_health_private_care_canada \u2500\u2500 Topic: 99 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500insurance_car_accident_rates_sue \u2500\u2500 Topic: 82 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_radar_engine_detector \u2502 \u2502 \u2502 \u2502 \u251c\u2500car_radar_cars_detector_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500radar_detector_detectors_ka_alarm \u2500\u2500 Topic: 39 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_mustang_ford_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clutch_shift_shifting_transmission_gear \u2500\u2500 Topic: 88 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500car_cars_mustang_ford_v8 \u2500\u2500 Topic: 14 \u2502 \u2502 \u2502 \u2502 \u2514\u2500oil_diesel_odometer_diesels_car \u2502 \u2502 \u2502 \u2502 \u251c\u2500odometer_oil_sensor_car_drain \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500odometer_sensor_speedo_gauge_mileage \u2500\u2500 Topic: 96 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oil_drain_car_leaks_taillights \u2500\u2500 Topic: 102 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500diesel_diesels_emissions_fuel_oil \u2500\u2500 Topic: 79 \u2502 \u2502 \u2502 \u2514\u2500bike_riding_ride_bikes_motorcycle \u2502 \u2502 \u2502 \u251c\u2500bike_ride_riding_bikes_lane \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500bike_ride_riding_lane_car \u2500\u2500 Topic: 11 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500bike_bikes_miles_honda_motorcycle \u2500\u2500 Topic: 19 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500countersteering_bike_motorcycle_rear_shaft \u2500\u2500 Topic: 46 \u2502 \u2502 \u2514\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_kuwait_greece_turkish_greeks \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500greek_greece_turkish_greeks_cyprus \u2500\u2500 Topic: 71 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kuwait_iraq_iran_gulf_arabia \u2500\u2500 Topic: 76 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_dog_drugs_drug_food \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clinton_bush_quayle_reagan_panicking \u2500\u2500 Topic: 101 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cooper_trial_weaver_spence_witnesses \u2500\u2500 Topic: 90 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500dog_dogs_bike_trained_springer \u2500\u2500 Topic: 67 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_drugs_drug_food_chinese \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500msg_food_chinese_foods_taste \u2500\u2500 Topic: 30 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500drugs_drug_marijuana_cocaine_alcohol \u2500\u2500 Topic: 72 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_theory_universe_science_larsons \u2502 \u2502 \u2502 \u2502 \u251c\u2500water_nuclear_cooling_steam_dept \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rocketry_rockets_engines_nuclear_plutonium \u2500\u2500 Topic: 115 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_cooling_steam_dept_plants \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500water_dept_phd_environmental_atmospheric \u2500\u2500 Topic: 97 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500cooling_water_steam_towers_plants \u2500\u2500 Topic: 109 \u2502 \u2502 \u2502 \u2502 \u2514\u2500theory_universe_larsons_larson_science \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500theory_universe_larsons_larson_science \u2500\u2500 Topic: 54 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oort_cloud_grbs_gamma_burst \u2500\u2500 Topic: 80 \u2502 \u2502 \u2502 \u2514\u2500helmet_kirlian_photography_lock_wax \u2502 \u2502 \u2502 \u251c\u2500helmet_kirlian_photography_leaf_mask \u2502 \u2502 \u2502 \u2502 \u251c\u2500kirlian_photography_leaf_pictures_deleted \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500deleted_joke_stuff_maddi_nickname \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500joke_maddi_nickname_nicknames_frank \u2500\u2500 Topic: 43 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500deleted_stuff_bookstore_joke_motto \u2500\u2500 Topic: 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kirlian_photography_leaf_pictures_aura \u2500\u2500 Topic: 85 \u2502 \u2502 \u2502 \u2502 \u2514\u2500helmet_mask_liner_foam_cb \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500helmet_liner_foam_cb_helmets \u2500\u2500 Topic: 112 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mask_goalies_77_santore_tl \u2500\u2500 Topic: 123 \u2502 \u2502 \u2502 \u2514\u2500lock_wax_paint_plastic_ear \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500lock_cable_locks_bike_600 \u2500\u2500 Topic: 117 \u2502 \u2502 \u2502 \u2514\u2500wax_paint_ear_plastic_skin \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500wax_paint_plastic_scratches_solvent \u2500\u2500 Topic: 65 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ear_wax_skin_greasy_acne \u2500\u2500 Topic: 116 \u2502 \u2502 \u2514\u2500m4_mp_14_mw_mo \u2502 \u2502 \u251c\u2500m4_mp_14_mw_mo \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500m4_mp_14_mw_mo \u2500\u2500 Topic: 111 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500test_ensign_nameless_deane_deanebinahccbrandeisedu \u2500\u2500 Topic: 118 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ites_cheek_hello_hi_ken \u2500\u2500 Topic: 3 \u2502 \u2514\u2500space_medical_health_disease_cancer \u2502 \u251c\u2500medical_health_disease_cancer_patients \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cancer_centers_center_medical_research \u2500\u2500 Topic: 122 \u2502 \u2502 \u2514\u2500health_medical_disease_patients_hiv \u2502 \u2502 \u251c\u2500patients_medical_disease_candida_health \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2502 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 \u2502 \u2514\u2500space_launch_nasa_shuttle_orbit \u2502 \u251c\u2500space_moon_station_nasa_launch \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500sky_advertising_billboard_billboards_space \u2500\u2500 Topic: 59 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500space_station_moon_redesign_nasa \u2500\u2500 Topic: 16 \u2502 \u2514\u2500space_mission_hst_launch_orbit \u2502 \u251c\u2500space_launch_nasa_orbit_propulsion \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500space_launch_nasa_propulsion_astronaut \u2500\u2500 Topic: 47 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500orbit_km_jupiter_probe_earth \u2500\u2500 Topic: 86 \u2502 \u2514\u2500\u25a0\u2500\u2500hst_mission_shuttle_orbit_arrays \u2500\u2500 Topic: 60 \u2514\u2500drive_file_key_windows_use \u251c\u2500key_file_jpeg_encryption_image \u2502 \u251c\u2500key_encryption_clipper_chip_keys \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500key_clipper_encryption_chip_keys \u2500\u2500 Topic: 1 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500entry_file_ripem_entries_key \u2500\u2500 Topic: 73 \u2502 \u2514\u2500jpeg_image_file_gif_images \u2502 \u251c\u2500motif_graphics_ftp_available_3d \u2502 \u2502 \u251c\u2500motif_graphics_openwindows_ftp_available \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500openwindows_motif_xview_windows_mouse \u2500\u2500 Topic: 20 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500graphics_widget_ray_3d_available \u2500\u2500 Topic: 95 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u25003d_machines_version_comments_contact \u2500\u2500 Topic: 38 \u2502 \u2514\u2500jpeg_image_gif_images_format \u2502 \u251c\u2500\u25a0\u2500\u2500gopher_ftp_files_stuffit_images \u2500\u2500 Topic: 51 \u2502 \u2514\u2500\u25a0\u2500\u2500jpeg_image_gif_format_images \u2500\u2500 Topic: 13 \u2514\u2500drive_db_card_scsi_windows \u251c\u2500db_windows_dos_mov_os2 \u2502 \u251c\u2500\u25a0\u2500\u2500copy_protection_program_software_disk \u2500\u2500 Topic: 64 \u2502 \u2514\u2500\u25a0\u2500\u2500db_windows_dos_mov_os2 \u2500\u2500 Topic: 8 \u2514\u2500drive_card_scsi_drives_ide \u251c\u2500drive_scsi_drives_ide_disk \u2502 \u251c\u2500\u25a0\u2500\u2500drive_scsi_drives_ide_disk \u2500\u2500 Topic: 6 \u2502 \u2514\u2500\u25a0\u2500\u2500meg_sale_ram_drive_shipping \u2500\u2500 Topic: 12 \u2514\u2500card_modem_monitor_video_drivers \u251c\u2500\u25a0\u2500\u2500card_monitor_video_drivers_vga \u2500\u2500 Topic: 5 \u2514\u2500\u25a0\u2500\u2500modem_port_serial_irq_com \u2500\u2500 Topic: 10 Merge topics \u00b6 After seeing the potential hierarchy of your topic, you might want to merge specific topics. For example, if topic 1 is 1_space_launch_moon_nasa and topic 2 is 2_spacecraft_solar_space_orbit it might make sense to merge those two topics as they are quite similar in meaning. In BERTopic, you can use .merge_topics to manually select and merge those topics. Doing so will update their topic representation which in turn updates the entire model: topics_to_merge = [ 1 , 2 ] topic_model . merge_topics ( docs , topics_to_merge ) If you have several groups of topics you want to merge, create a list of lists instead: topics_to_merge = [[ 1 , 2 ], [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge )","title":"Hierarchical Topic Modeling"},{"location":"getting_started/hierarchicaltopics/hierarchicaltopics.html#example","text":"To demonstrate hierarchical topic modeling with BERTopic, we use the 20 Newsgroups dataset to see how the topics that we uncover are represented in the 20 categories of documents. First, we train a basic BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) Next, we can use our fitted BERTopic model to extract possible hierarchies from our c-TF-IDF matrix: hierarchical_topics = topic_model . hierarchical_topics ( docs ) The resulting hierarchical_topics is a dataframe in which merged topics are described. For example, if you would merge two topics, what would the topic representation of the new topic be?","title":"Example"},{"location":"getting_started/hierarchicaltopics/hierarchicaltopics.html#linkage-functions","text":"When creating the potential hierarchical nature of topics, we use Scipy's ward linkage function as a default to generate the hierarchy. However, you might want to use a different linkage function for your use case, such as single , complete , average , centroid , or median . In BERTopic, you can define the linkage function yourself, including the distance function that you would like to use: from scipy.cluster import hierarchy as sch from bertopic import BERTopic topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Hierarchical topics linkage_function = lambda x : sch . linkage ( x , 'single' , optimal_ordering = True ) hierarchical_topics = topic_model . hierarchical_topics ( docs , linkage_function = linkage_function )","title":"Linkage functions"},{"location":"getting_started/hierarchicaltopics/hierarchicaltopics.html#visualizations","text":"To visualize these results, we can start by running a familiar function, namely topic_model.visualize_hierarchy : topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics together. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes. Although this gives a nice overview of the potential hierarchy, hovering over all black circles can be tiresome. Instead, we can use topic_model.get_topic_tree to create a text-based representation of this hierarchy. Although the general structure is more difficult to view, we can see better which topics could be logically merged: >>> tree = topic_model . get_topic_tree ( hierarchical_topics ) >>> print ( tree ) . \u2514\u2500 atheists_atheism_god_moral_atheist \u251c\u2500 atheists_atheism_god_atheist_argument \u2502 \u251c\u2500\u25a0\u2500\u2500 atheists_atheism_god_atheist_argument \u2500\u2500 Topic : 21 \u2502 \u2514\u2500\u25a0\u2500\u2500 br_god_exist_genetic_existence \u2500\u2500 Topic : 124 \u2514\u2500\u25a0\u2500\u2500 moral_morality_objective_immoral_morals \u2500\u2500 Topic : 29 Click here to view the full tree. . \u251c\u2500people_armenian_said_god_armenians \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u2502 \u251c\u2500jehovah_lord_mormon_mcconkie_god \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ra_satan_thou_god_lucifer \u2500\u2500 Topic: 94 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500jehovah_lord_mormon_mcconkie_unto \u2500\u2500 Topic: 78 \u2502 \u2502 \u2502 \u2514\u2500jesus_mary_god_hell_sin \u2502 \u2502 \u2502 \u251c\u2500jesus_hell_god_eternal_heaven \u2502 \u2502 \u2502 \u2502 \u251c\u2500hell_jesus_eternal_god_heaven \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jesus_tomb_disciples_resurrection_john \u2500\u2500 Topic: 69 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500hell_eternal_god_jesus_heaven \u2500\u2500 Topic: 53 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500aaron_baptism_sin_law_god \u2500\u2500 Topic: 89 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mary_sin_maria_priest_conception \u2500\u2500 Topic: 56 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500marriage_married_marry_ceremony_marriages \u2500\u2500 Topic: 110 \u2502 \u2514\u2500people_armenian_armenians_said_mr \u2502 \u251c\u2500people_armenian_armenians_said_israel \u2502 \u2502 \u251c\u2500god_homosexual_homosexuality_atheists_sex \u2502 \u2502 \u2502 \u251c\u2500homosexual_homosexuality_sex_gay_homosexuals \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500kinsey_sex_gay_men_sexual \u2500\u2500 Topic: 44 \u2502 \u2502 \u2502 \u2502 \u2514\u2500homosexuality_homosexual_sin_homosexuals_gay \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gay_homosexual_homosexuals_sexual_cramer \u2500\u2500 Topic: 50 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500homosexuality_homosexual_sin_paul_sex \u2500\u2500 Topic: 27 \u2502 \u2502 \u2502 \u2514\u2500god_atheists_atheism_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jim_context_challenges_articles_quote \u2500\u2500 Topic: 36 \u2502 \u2502 \u2502 \u2502 \u2514\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500islam_quran_islamic_rushdie_muslims \u2500\u2500 Topic: 31 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500judas_scripture_bible_books_greek \u2500\u2500 Topic: 33 \u2502 \u2502 \u2502 \u2514\u2500atheists_atheism_god_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500atheists_atheism_god_atheist_argument \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500atheists_atheism_god_atheist_argument \u2500\u2500 Topic: 21 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500br_god_exist_genetic_existence \u2500\u2500 Topic: 124 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500moral_morality_objective_immoral_morals \u2500\u2500 Topic: 29 \u2502 \u2502 \u2514\u2500armenian_armenians_people_israel_said \u2502 \u2502 \u251c\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500tax_rights_government_income_taxes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rights_right_slavery_slaves_residence \u2500\u2500 Topic: 106 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tax_government_taxes_income_libertarians \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500government_libertarians_libertarian_regulation_party \u2500\u2500 Topic: 58 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tax_taxes_income_billion_deficit \u2500\u2500 Topic: 41 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500gun_guns_militia_firearms_amendment \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500blacks_penalty_death_cruel_punishment \u2500\u2500 Topic: 55 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500gun_guns_militia_firearms_amendment \u2500\u2500 Topic: 7 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_jews_turkish \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500israel_israeli_jews_arab_jewish \u2500\u2500 Topic: 4 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500armenian_armenians_turkish_armenia_azerbaijan \u2500\u2500 Topic: 15 \u2502 \u2502 \u2514\u2500stephanopoulos_president_mr_myers_ms \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500serbs_muslims_stephanopoulos_mr_bosnia \u2500\u2500 Topic: 35 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500myers_stephanopoulos_president_ms_mr \u2500\u2500 Topic: 87 \u2502 \u2514\u2500batf_fbi_koresh_compound_gas \u2502 \u251c\u2500\u25a0\u2500\u2500reno_workers_janet_clinton_waco \u2500\u2500 Topic: 77 \u2502 \u2514\u2500batf_fbi_koresh_gas_compound \u2502 \u251c\u2500batf_koresh_fbi_warrant_compound \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500batf_warrant_raid_compound_fbi \u2500\u2500 Topic: 42 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500koresh_batf_fbi_children_compound \u2500\u2500 Topic: 61 \u2502 \u2514\u2500\u25a0\u2500\u2500fbi_gas_tear_bds_building \u2500\u2500 Topic: 23 \u2514\u2500use_like_just_dont_new \u251c\u2500game_team_year_games_like \u2502 \u251c\u2500game_team_games_25_year \u2502 \u2502 \u251c\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u2500window_printer_use_problem_mhz \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_wire_simms_wiring_battery \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_mhz_battery_cpu_heat \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_pds_simm_vram_lc \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500pds_nubus_lc_slot_card \u2500\u2500 Topic: 119 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500simms_simm_vram_meg_dram \u2500\u2500 Topic: 32 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mhz_battery_cpu_heat_speed \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500fan_cpu_heat_sink_fans \u2500\u2500 Topic: 92 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mhz_speed_cpu_fpu_clock \u2500\u2500 Topic: 22 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500monitor_turn_power_computer_electricity \u2500\u2500 Topic: 91 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500battery_batteries_concrete_duo_discharge \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500duo_battery_apple_230_problem \u2500\u2500 Topic: 121 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500battery_batteries_concrete_discharge_temperature \u2500\u2500 Topic: 75 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500leds_uv_blue_light_boards \u2500\u2500 Topic: 66 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500wire_wiring_ground_neutral_outlets \u2500\u2500 Topic: 120 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500scope_scopes_phone_dial_number \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500dial_number_phone_line_output \u2500\u2500 Topic: 93 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500scope_scopes_motorola_generator_oscilloscope \u2500\u2500 Topic: 113 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500celp_dsp_sampling_antenna_digital \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500antenna_antennas_receiver_cable_transmitter \u2500\u2500 Topic: 70 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500celp_dsp_sampling_speech_voice \u2500\u2500 Topic: 52 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_printer_xv_mouse_windows \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_xv_error_widget_problem \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500error_symbol_undefined_xterm_rx \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500symbol_error_undefined_doug_parse \u2500\u2500 Topic: 63 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500rx_remote_server_xdm_xterm \u2500\u2500 Topic: 45 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_xv_widget_application_expose \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_widget_expose_application_event \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gc_mydisplay_draw_gxxor_drawing \u2500\u2500 Topic: 103 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500window_widget_application_expose_event \u2500\u2500 Topic: 25 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500xv_den_polygon_points_algorithm \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500den_polygon_points_algorithm_polygons \u2500\u2500 Topic: 28 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500xv_24bit_image_bit_images \u2500\u2500 Topic: 57 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_mouse_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500scanner_logitech_grayscale_ocr_scanman \u2500\u2500 Topic: 108 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500printer_print_deskjet_hp_ink \u2500\u2500 Topic: 18 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500fonts_font_truetype_tt_atm \u2500\u2500 Topic: 49 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mouse_ghostscript_midi_driver_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500ghostscript_midi_postscript_files_file \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ghostscript_postscript_pageview_ghostview_dsc \u2500\u2500 Topic: 104 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500midi_sound_file_windows_driver \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500location_mar_file_host_rwrr \u2500\u2500 Topic: 83 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500midi_sound_driver_blaster_soundblaster \u2500\u2500 Topic: 98 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mouse_driver_mice_ball_problem \u2500\u2500 Topic: 68 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u25001st_sale_condition_comics_hulk \u2502 \u2502 \u2502 \u2502 \u251c\u2500sale_condition_offer_asking_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500condition_stereo_amp_speakers_asking \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500miles_car_amfm_toyota_cassette \u2500\u2500 Topic: 62 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500amp_speakers_condition_stereo_audio \u2500\u2500 Topic: 24 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500games_sale_pom_cds_shipping \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500pom_cds_sale_shipping_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500size_shipping_sale_condition_mattress \u2500\u2500 Topic: 100 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pom_cds_cd_sale_picture \u2500\u2500 Topic: 37 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500games_game_snes_sega_genesis \u2500\u2500 Topic: 40 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u251c\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500lens_tape_camera_backup_lenses \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500tape_backup_tapes_drive_4mm \u2500\u2500 Topic: 107 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500lens_camera_lenses_zoom_pouch \u2500\u2500 Topic: 114 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u25001st_hulk_comics_art_appears \u2500\u2500 Topic: 105 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500books_book_cover_trek_chemistry \u2500\u2500 Topic: 125 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tickets_hotel_ticket_voucher_package \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hotel_voucher_package_vacation_room \u2500\u2500 Topic: 74 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tickets_ticket_june_airlines_july \u2500\u2500 Topic: 84 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_season_hockey \u2502 \u2502 \u2502 \u251c\u2500game_hockey_team_25_550 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500espn_pt_pts_game_la \u2500\u2500 Topic: 17 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500team_25_game_hockey_550 \u2500\u2500 Topic: 2 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500year_game_hit_baseball_players \u2500\u2500 Topic: 0 \u2502 \u2502 \u2514\u2500bike_car_greek_insurance_msg \u2502 \u2502 \u251c\u2500car_bike_insurance_cars_engine \u2502 \u2502 \u2502 \u251c\u2500car_insurance_cars_radar_engine \u2502 \u2502 \u2502 \u2502 \u251c\u2500insurance_health_private_care_canada \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500insurance_health_private_care_canada \u2500\u2500 Topic: 99 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500insurance_car_accident_rates_sue \u2500\u2500 Topic: 82 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_radar_engine_detector \u2502 \u2502 \u2502 \u2502 \u251c\u2500car_radar_cars_detector_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500radar_detector_detectors_ka_alarm \u2500\u2500 Topic: 39 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_mustang_ford_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clutch_shift_shifting_transmission_gear \u2500\u2500 Topic: 88 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500car_cars_mustang_ford_v8 \u2500\u2500 Topic: 14 \u2502 \u2502 \u2502 \u2502 \u2514\u2500oil_diesel_odometer_diesels_car \u2502 \u2502 \u2502 \u2502 \u251c\u2500odometer_oil_sensor_car_drain \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500odometer_sensor_speedo_gauge_mileage \u2500\u2500 Topic: 96 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oil_drain_car_leaks_taillights \u2500\u2500 Topic: 102 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500diesel_diesels_emissions_fuel_oil \u2500\u2500 Topic: 79 \u2502 \u2502 \u2502 \u2514\u2500bike_riding_ride_bikes_motorcycle \u2502 \u2502 \u2502 \u251c\u2500bike_ride_riding_bikes_lane \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500bike_ride_riding_lane_car \u2500\u2500 Topic: 11 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500bike_bikes_miles_honda_motorcycle \u2500\u2500 Topic: 19 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500countersteering_bike_motorcycle_rear_shaft \u2500\u2500 Topic: 46 \u2502 \u2502 \u2514\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_kuwait_greece_turkish_greeks \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500greek_greece_turkish_greeks_cyprus \u2500\u2500 Topic: 71 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kuwait_iraq_iran_gulf_arabia \u2500\u2500 Topic: 76 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_dog_drugs_drug_food \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clinton_bush_quayle_reagan_panicking \u2500\u2500 Topic: 101 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cooper_trial_weaver_spence_witnesses \u2500\u2500 Topic: 90 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500dog_dogs_bike_trained_springer \u2500\u2500 Topic: 67 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_drugs_drug_food_chinese \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500msg_food_chinese_foods_taste \u2500\u2500 Topic: 30 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500drugs_drug_marijuana_cocaine_alcohol \u2500\u2500 Topic: 72 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_theory_universe_science_larsons \u2502 \u2502 \u2502 \u2502 \u251c\u2500water_nuclear_cooling_steam_dept \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rocketry_rockets_engines_nuclear_plutonium \u2500\u2500 Topic: 115 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_cooling_steam_dept_plants \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500water_dept_phd_environmental_atmospheric \u2500\u2500 Topic: 97 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500cooling_water_steam_towers_plants \u2500\u2500 Topic: 109 \u2502 \u2502 \u2502 \u2502 \u2514\u2500theory_universe_larsons_larson_science \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500theory_universe_larsons_larson_science \u2500\u2500 Topic: 54 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oort_cloud_grbs_gamma_burst \u2500\u2500 Topic: 80 \u2502 \u2502 \u2502 \u2514\u2500helmet_kirlian_photography_lock_wax \u2502 \u2502 \u2502 \u251c\u2500helmet_kirlian_photography_leaf_mask \u2502 \u2502 \u2502 \u2502 \u251c\u2500kirlian_photography_leaf_pictures_deleted \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500deleted_joke_stuff_maddi_nickname \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500joke_maddi_nickname_nicknames_frank \u2500\u2500 Topic: 43 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500deleted_stuff_bookstore_joke_motto \u2500\u2500 Topic: 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kirlian_photography_leaf_pictures_aura \u2500\u2500 Topic: 85 \u2502 \u2502 \u2502 \u2502 \u2514\u2500helmet_mask_liner_foam_cb \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500helmet_liner_foam_cb_helmets \u2500\u2500 Topic: 112 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mask_goalies_77_santore_tl \u2500\u2500 Topic: 123 \u2502 \u2502 \u2502 \u2514\u2500lock_wax_paint_plastic_ear \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500lock_cable_locks_bike_600 \u2500\u2500 Topic: 117 \u2502 \u2502 \u2502 \u2514\u2500wax_paint_ear_plastic_skin \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500wax_paint_plastic_scratches_solvent \u2500\u2500 Topic: 65 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ear_wax_skin_greasy_acne \u2500\u2500 Topic: 116 \u2502 \u2502 \u2514\u2500m4_mp_14_mw_mo \u2502 \u2502 \u251c\u2500m4_mp_14_mw_mo \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500m4_mp_14_mw_mo \u2500\u2500 Topic: 111 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500test_ensign_nameless_deane_deanebinahccbrandeisedu \u2500\u2500 Topic: 118 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ites_cheek_hello_hi_ken \u2500\u2500 Topic: 3 \u2502 \u2514\u2500space_medical_health_disease_cancer \u2502 \u251c\u2500medical_health_disease_cancer_patients \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cancer_centers_center_medical_research \u2500\u2500 Topic: 122 \u2502 \u2502 \u2514\u2500health_medical_disease_patients_hiv \u2502 \u2502 \u251c\u2500patients_medical_disease_candida_health \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2502 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 \u2502 \u2514\u2500space_launch_nasa_shuttle_orbit \u2502 \u251c\u2500space_moon_station_nasa_launch \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500sky_advertising_billboard_billboards_space \u2500\u2500 Topic: 59 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500space_station_moon_redesign_nasa \u2500\u2500 Topic: 16 \u2502 \u2514\u2500space_mission_hst_launch_orbit \u2502 \u251c\u2500space_launch_nasa_orbit_propulsion \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500space_launch_nasa_propulsion_astronaut \u2500\u2500 Topic: 47 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500orbit_km_jupiter_probe_earth \u2500\u2500 Topic: 86 \u2502 \u2514\u2500\u25a0\u2500\u2500hst_mission_shuttle_orbit_arrays \u2500\u2500 Topic: 60 \u2514\u2500drive_file_key_windows_use \u251c\u2500key_file_jpeg_encryption_image \u2502 \u251c\u2500key_encryption_clipper_chip_keys \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500key_clipper_encryption_chip_keys \u2500\u2500 Topic: 1 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500entry_file_ripem_entries_key \u2500\u2500 Topic: 73 \u2502 \u2514\u2500jpeg_image_file_gif_images \u2502 \u251c\u2500motif_graphics_ftp_available_3d \u2502 \u2502 \u251c\u2500motif_graphics_openwindows_ftp_available \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500openwindows_motif_xview_windows_mouse \u2500\u2500 Topic: 20 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500graphics_widget_ray_3d_available \u2500\u2500 Topic: 95 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u25003d_machines_version_comments_contact \u2500\u2500 Topic: 38 \u2502 \u2514\u2500jpeg_image_gif_images_format \u2502 \u251c\u2500\u25a0\u2500\u2500gopher_ftp_files_stuffit_images \u2500\u2500 Topic: 51 \u2502 \u2514\u2500\u25a0\u2500\u2500jpeg_image_gif_format_images \u2500\u2500 Topic: 13 \u2514\u2500drive_db_card_scsi_windows \u251c\u2500db_windows_dos_mov_os2 \u2502 \u251c\u2500\u25a0\u2500\u2500copy_protection_program_software_disk \u2500\u2500 Topic: 64 \u2502 \u2514\u2500\u25a0\u2500\u2500db_windows_dos_mov_os2 \u2500\u2500 Topic: 8 \u2514\u2500drive_card_scsi_drives_ide \u251c\u2500drive_scsi_drives_ide_disk \u2502 \u251c\u2500\u25a0\u2500\u2500drive_scsi_drives_ide_disk \u2500\u2500 Topic: 6 \u2502 \u2514\u2500\u25a0\u2500\u2500meg_sale_ram_drive_shipping \u2500\u2500 Topic: 12 \u2514\u2500card_modem_monitor_video_drivers \u251c\u2500\u25a0\u2500\u2500card_monitor_video_drivers_vga \u2500\u2500 Topic: 5 \u2514\u2500\u25a0\u2500\u2500modem_port_serial_irq_com \u2500\u2500 Topic: 10","title":"Visualizations"},{"location":"getting_started/hierarchicaltopics/hierarchicaltopics.html#merge-topics","text":"After seeing the potential hierarchy of your topic, you might want to merge specific topics. For example, if topic 1 is 1_space_launch_moon_nasa and topic 2 is 2_spacecraft_solar_space_orbit it might make sense to merge those two topics as they are quite similar in meaning. In BERTopic, you can use .merge_topics to manually select and merge those topics. Doing so will update their topic representation which in turn updates the entire model: topics_to_merge = [ 1 , 2 ] topic_model . merge_topics ( docs , topics_to_merge ) If you have several groups of topics you want to merge, create a list of lists instead: topics_to_merge = [[ 1 , 2 ], [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge )","title":"Merge topics"},{"location":"getting_started/online/online.html","text":"Online topic modeling (sometimes called \"incremental topic modeling\") is the ability to learn incrementally from a mini-batch of instances. Essentially, it is a way to update your topic model with data on which it was not trained on before. In Scikit-Learn, this technique is often modeled through a .partial_fit function, which is also used in BERTopic. In BERTopic, there are three main goals for using this technique. To reduce the memory necessary for training a topic model. To continuously update the topic model as new data comes in. To continuously find new topics as new data comes in. In BERTopic, online topic modeling can be a bit tricky as there are several steps involved in which online learning needs to be made available. To recap, BERTopic consists of the following 6 steps: Extract embeddings Reduce dimensionality Cluster reduced embeddings Tokenize topics Extract topic words Diversify topic words For some steps, an online variant is more important than others. Typically, in step 1 we use pre-trained language models that are in less need for continuous updates. This means that we can use an embedding model like Sentence-Transformers for extracting the embeddings and still use it in an online setting. Similarly, step 5 and 6 do not necessarily need online variants since they are built upon step 4, the tokenization. If that tokenization is by itself incremental, then so will steps 5 and 6. This means that we will need online variants for steps 2 through 4. Steps 2 and 3, dimensionality reduction and clustering, can be modeled through the use of Scikit-Learn's .partial_fit function. In other words, it supports any algorithm that can be trained using .partial_fit since these algorithms can be trained incrementally. For example, incremental dimensionality reduction can be achieved using Scikit-Learn's IncrementalPCA and incremental clustering with MiniBatchKMeans . Lastly, we need to develop an online variant for step 5, tokenization. In this step, a Bag-of-words representation is created through the CountVectorizer . However, as new data comes in, its vocabulary will need to be updated. For that purpose, bertopic.vectorizers.OnlineCountVectorizer was created that not only updates out-of-vocabulary words but also implements decay and cleaning functions to prevent the sparse bag-of-words matrix to become too large in size. Most notably, the decay parameter is a value between 0 and 1 to weight the percentage of frequencies that the previous bag-of-words matrix should be reduced to. For example, a value of .1 will decrease the frequencies in the bag-of-words matrix with 10% at each iteration. This will make sure that recent data has more weight than previously iterations. Similarly, delete_min_df will remove certain words from its vocabulary if its frequency is lower than a set value. This ties together with the decay parameter as some words will decay over time if not used. For more information regarding the OnlineCountVectorizer , please see the vectorizers documentation . Example \u00b6 Online topic modeling in BERTopic is rather straightforward. We first need to have our documents in split in chunks such that we can train and update our topic model incrementally. from sklearn.datasets import fetch_20newsgroups # Prepare documents all_docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] doc_chunks = [ all_docs [ i : i + 1000 ] for i in range ( 0 , len ( all_docs ), 1000 )] Here, we created chunks of 1000 documents to be fed in BERTopic. Then, we will need to define a number of sub-models that support online learning. Specifically, we are going to be using IncrementalPCA , MiniBatchKMeans , and the OnlineCountVectorizer : from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) Tip You can use any other dimensionality reduction and clustering algorithm as long as they have a .partial_fit function. Moreover, you can use dimensionality reduction algorithms that do not support .partial_fit functions but do have a .fit function to first train it on a large amount of data and then continously add documents. The dimensionality reduction will not be updated but may be trained sufficiently to properly reduce the embeddings without the need to continuously add documents. After having defined our sub-models, we can start training our topic model incrementally by looping over our document chunks: from bertopic import BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs ) And that is it! During each iteration, you can access the predicted topics through the .topics_ attribute. Do note though that only the most recent batch of documents are tracked. If you want to be using online topic modeling for low-memory use cases, then it is advised to also update the .topics_ attribute. Otherwise, variations such as hierarchical topic model will not work. # Incrementally fit the topic model by training on 1000 documents at a time and track the topics in each iteration topics = [] for docs in doc_chunks : topic_model . partial_fit ( docs ) topics . extend ( topic_model . topics_ ) topic_model . topics_ = topics Note Do note that in BERTopic it is not possible to use .partial_fit after the .fit as they work quite differently with respect to internally updating topics, frequencies, representations, etc. River \u00b6 To continuously find new topics as they come in, we can use the package river . It contains several clustering models that can create new clusters as new data comes in. To make sure we can use their models, we first need to create a class that has a .partial_fit function and the option to extract labels through .labels_ : from river import stream from river import cluster class River : def __init__ ( self , model ): self . model = model def partial_fit ( self , umap_embeddings ): for umap_embedding , _ in stream . iter_array ( umap_embeddings ): self . model = self . model . learn_one ( umap_embedding ) labels = [] for umap_embedding , _ in stream . iter_array ( umap_embeddings ): label = self . model . predict_one ( umap_embedding ) labels . append ( label ) self . labels_ = labels return self Then, we can choose any river.cluster model that we are interested in and pass it to the River class before using it in BERTopic: # Using DBSTREAM to detect new topics as they come in cluster_model = River ( cluster . DBSTREAM ()) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" ) ctfidf_model = ClassTfidfTransformer ( reduce_frequent_words = True , bm25_weighting = True ) # Prepare model topic_model = BERTopic ( hdbscan_model = cluster_model , vectorizer_model = vectorizer_model , ctfidf_model = ctfidf_model , ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs )","title":"Online Topic Modeling"},{"location":"getting_started/online/online.html#example","text":"Online topic modeling in BERTopic is rather straightforward. We first need to have our documents in split in chunks such that we can train and update our topic model incrementally. from sklearn.datasets import fetch_20newsgroups # Prepare documents all_docs = fetch_20newsgroups ( subset = subset , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] doc_chunks = [ all_docs [ i : i + 1000 ] for i in range ( 0 , len ( all_docs ), 1000 )] Here, we created chunks of 1000 documents to be fed in BERTopic. Then, we will need to define a number of sub-models that support online learning. Specifically, we are going to be using IncrementalPCA , MiniBatchKMeans , and the OnlineCountVectorizer : from sklearn.cluster import MiniBatchKMeans from sklearn.decomposition import IncrementalPCA from bertopic.vectorizers import OnlineCountVectorizer # Prepare sub-models that support online learning umap_model = IncrementalPCA ( n_components = 5 ) cluster_model = MiniBatchKMeans ( n_clusters = 50 , random_state = 0 ) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" , decay = .01 ) Tip You can use any other dimensionality reduction and clustering algorithm as long as they have a .partial_fit function. Moreover, you can use dimensionality reduction algorithms that do not support .partial_fit functions but do have a .fit function to first train it on a large amount of data and then continously add documents. The dimensionality reduction will not be updated but may be trained sufficiently to properly reduce the embeddings without the need to continuously add documents. After having defined our sub-models, we can start training our topic model incrementally by looping over our document chunks: from bertopic import BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = cluster_model , vectorizer_model = vectorizer_model ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs ) And that is it! During each iteration, you can access the predicted topics through the .topics_ attribute. Do note though that only the most recent batch of documents are tracked. If you want to be using online topic modeling for low-memory use cases, then it is advised to also update the .topics_ attribute. Otherwise, variations such as hierarchical topic model will not work. # Incrementally fit the topic model by training on 1000 documents at a time and track the topics in each iteration topics = [] for docs in doc_chunks : topic_model . partial_fit ( docs ) topics . extend ( topic_model . topics_ ) topic_model . topics_ = topics Note Do note that in BERTopic it is not possible to use .partial_fit after the .fit as they work quite differently with respect to internally updating topics, frequencies, representations, etc.","title":"Example"},{"location":"getting_started/online/online.html#river","text":"To continuously find new topics as they come in, we can use the package river . It contains several clustering models that can create new clusters as new data comes in. To make sure we can use their models, we first need to create a class that has a .partial_fit function and the option to extract labels through .labels_ : from river import stream from river import cluster class River : def __init__ ( self , model ): self . model = model def partial_fit ( self , umap_embeddings ): for umap_embedding , _ in stream . iter_array ( umap_embeddings ): self . model = self . model . learn_one ( umap_embedding ) labels = [] for umap_embedding , _ in stream . iter_array ( umap_embeddings ): label = self . model . predict_one ( umap_embedding ) labels . append ( label ) self . labels_ = labels return self Then, we can choose any river.cluster model that we are interested in and pass it to the River class before using it in BERTopic: # Using DBSTREAM to detect new topics as they come in cluster_model = River ( cluster . DBSTREAM ()) vectorizer_model = OnlineCountVectorizer ( stop_words = \"english\" ) ctfidf_model = ClassTfidfTransformer ( reduce_frequent_words = True , bm25_weighting = True ) # Prepare model topic_model = BERTopic ( hdbscan_model = cluster_model , vectorizer_model = vectorizer_model , ctfidf_model = ctfidf_model , ) # Incrementally fit the topic model by training on 1000 documents at a time for docs in doc_chunks : topic_model . partial_fit ( docs )","title":"River"},{"location":"getting_started/parameter%20tuning/parametertuning.html","text":"Hyperparameter Tuning \u00b6 Although BERTopic works quite well out of the box, there are a number of hyperparameters to tune according to your use-case. This section will focus on important parameters directly accessable in BERTopic but also hyperparameter optimization in sub-models such as HDBSCAN and UMAP. BERTopic \u00b6 When instantiating BERTopic, there are a number of hyperparameters that you can directly adjust that could significantly improve the performance of your topic model. In this section, we will go through the most impactful parameters in BERTopic and directions on how to optimize them. language \u00b6 The language parameter is used to simplify the selection of models for those who are not familiar with sentence-transformers models. In essence, there are two options to choose from: language = \"english\" or language = \"multilingual\" The English model is \"all-MiniLM-L6-v2\" and can be found here . It is the default model that is used in BERTopic and works great for English documents. The multilingual model is \"paraphrase-multilingual-MiniLM-L12-v2\" and supports over 50+ languages which can be found here . The model is very similar to the base model but is trained on many languages and has a slightly different architecture. top_n_words \u00b6 top_n_words refers to the number of words per topic that you want extracted. In practice, I would advise you to keep this value below 30 and preferably between 10 and 20. The reasoning for this is that the more words you put in a topic the less coherent it can become. The top words are the most representative for the topic and should be focused on. n_gram_range \u00b6 The n_gram_range parameter refers to the CountVectorizer used when creating the topic representation. It relates to the number of words you want in your topic representation. For example, \"New\" and \"York\" are two seperate words but are often used as \"New York\" which represents an n-gram of 2. Thus, the n_gram_range should be set to (1, 2) if you want \"New York\" in your topic representation. min_topic_size \u00b6 min_topic_size is an important parameter! It is used to specify what the minimum size of a topic can be. The lower this value the more topics are created. If you set this value too high, then it is possible that simply no topics will be created! Set this value too low and you will get many micro clusters. It is advised to play around with this value depending on the size of the your dataset. If it nears a million documents, then it advised to set it much higher than the default of 10, for example 100 or even 500. nr_topics \u00b6 nr_topics can be a tricky parameter. It specifies, after training the topic model, the number of topics that will be reduced to. For example, if your topic model results in 100 topics but you have set nr_topics to 20 then the topic model will try to reduce the number of topics from 100 to 20. This reduction can take awhile as each reduction in topics activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that using HDBSCAN. low_memory \u00b6 low_memory sets UMAP's low_memory to True to make sure that less memory is used in computation. This slows down computation but allows UMAP to be ran on low memory machines. calculate_probabilities \u00b6 calculate_probabilities lets you calculate the probabilities of each topic to each document. This is computationally quite expensive and is turned off by default. UMAP \u00b6 UMAP is an amazing technique for dimensionality reduction. In BERTopic, it is used to reduce the dimensionality of document embedding into something that is easier to use with HDBSCAN in order to create good clusters. However, it does has a significant number of parameters you could take into account. As exposing all parameters in BERTopic would be difficult to manage, we can instantiate our UMAP model and pass it to BERTopic: from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 10 , metric = 'cosine' , low_memory = False ) topic_model = BERTopic ( umap_model = umap_model ) . fit ( docs ) n_neighbors \u00b6 n_neighbors is the numer of neighboring sample points used when making the manifold approximation. Increasing this value typically results in a more global view of the embedding structure whilst smaller values result in a more local view. Increasing this value often results in larger clusters being created. n_components \u00b6 n_components refers to the dimensionality of the embeddings after reducing them. This is set as a default to 5 in order to reduce dimensionality as much as possible whilst trying to maximize the information kept in the resulting embeddings. Although lowering or increasing this value has an influence on the quality of embeddings, its effect is largest on the performance of HDBSCAN. Increasing this value too much and HDBSCAN will have a hard time clustering the high-dimensional embeddings. Lower this value too much and too little information in the resulting embeddings is available to create proper clusters. If you want to increase this value, I would advise setting using a metric for HDBSCAN that works well in high dimensional data. metric \u00b6 metric refers to the method used to compute the distances in high dimensional space. The default is cosine as we are dealing with high dimensional data. However, BERTopic is also able to use any input, even regular tabular data, to cluster the documents. Thus, you might want to change the metric to something that fits with your use case. low_memory \u00b6 low_memory is used when datasets may consume a lot of memory. Using millions of documents can lead to memory issues and setting this value to True might alleviate some of the issues. HDBSCAN \u00b6 After reducing the embeddings with UMAP, we use HDBSCAN to cluster our documents into clusters of similar documents. Similar to UMAP, HDBSCAN has many parameters that could be tweaked in order to improve the cluster's quality. from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 10 , metric = 'euclidean' , prediction_data = True ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) . fit ( docs ) min_cluster_size \u00b6 min_cluster_size is arguably the most important parameter in HDBSCAN. It controls the minimum size of a cluster and thereby the amount of clusters that will be generated. It is set to 10 as a default. Increasing this value results in less clusters but of larger size whereas decreasing this value results in more micro clusters being generated. Typically, I would advise on increasing this value rather than decreasing it. min_samples \u00b6 min_samples is automatically set to min_cluster_size and controls the amount of outliers are generated. Setting this value significantly lower than min_cluster_size might help you reduce the amount of noise you will get. Do note that outliers are typically to be expected and forcing the output to have no outliers may not properly represent the data. metric \u00b6 metric , like with HDBSCAN is used to calculate the distances. Here, we went with euclidean as, after reducing the dimensionality, we have low dimensional data and not much optimization is necessary. However, if you increase n_components in UMAP, then it would be advised to look into metrics that work with high dimensional data. prediction_data \u00b6 Make sure you always set this value to True as it is needed to predict new points later on. You can set this to False if you do not wish to predict any unseen datapoints.","title":"Parameter tuning"},{"location":"getting_started/parameter%20tuning/parametertuning.html#hyperparameter-tuning","text":"Although BERTopic works quite well out of the box, there are a number of hyperparameters to tune according to your use-case. This section will focus on important parameters directly accessable in BERTopic but also hyperparameter optimization in sub-models such as HDBSCAN and UMAP.","title":"Hyperparameter Tuning"},{"location":"getting_started/parameter%20tuning/parametertuning.html#bertopic","text":"When instantiating BERTopic, there are a number of hyperparameters that you can directly adjust that could significantly improve the performance of your topic model. In this section, we will go through the most impactful parameters in BERTopic and directions on how to optimize them.","title":"BERTopic"},{"location":"getting_started/parameter%20tuning/parametertuning.html#language","text":"The language parameter is used to simplify the selection of models for those who are not familiar with sentence-transformers models. In essence, there are two options to choose from: language = \"english\" or language = \"multilingual\" The English model is \"all-MiniLM-L6-v2\" and can be found here . It is the default model that is used in BERTopic and works great for English documents. The multilingual model is \"paraphrase-multilingual-MiniLM-L12-v2\" and supports over 50+ languages which can be found here . The model is very similar to the base model but is trained on many languages and has a slightly different architecture.","title":"language"},{"location":"getting_started/parameter%20tuning/parametertuning.html#top_n_words","text":"top_n_words refers to the number of words per topic that you want extracted. In practice, I would advise you to keep this value below 30 and preferably between 10 and 20. The reasoning for this is that the more words you put in a topic the less coherent it can become. The top words are the most representative for the topic and should be focused on.","title":"top_n_words"},{"location":"getting_started/parameter%20tuning/parametertuning.html#n_gram_range","text":"The n_gram_range parameter refers to the CountVectorizer used when creating the topic representation. It relates to the number of words you want in your topic representation. For example, \"New\" and \"York\" are two seperate words but are often used as \"New York\" which represents an n-gram of 2. Thus, the n_gram_range should be set to (1, 2) if you want \"New York\" in your topic representation.","title":"n_gram_range"},{"location":"getting_started/parameter%20tuning/parametertuning.html#min_topic_size","text":"min_topic_size is an important parameter! It is used to specify what the minimum size of a topic can be. The lower this value the more topics are created. If you set this value too high, then it is possible that simply no topics will be created! Set this value too low and you will get many micro clusters. It is advised to play around with this value depending on the size of the your dataset. If it nears a million documents, then it advised to set it much higher than the default of 10, for example 100 or even 500.","title":"min_topic_size"},{"location":"getting_started/parameter%20tuning/parametertuning.html#nr_topics","text":"nr_topics can be a tricky parameter. It specifies, after training the topic model, the number of topics that will be reduced to. For example, if your topic model results in 100 topics but you have set nr_topics to 20 then the topic model will try to reduce the number of topics from 100 to 20. This reduction can take awhile as each reduction in topics activates a c-TF-IDF calculation. If this is set to None, no reduction is applied. Use \"auto\" to automatically reduce topics that using HDBSCAN.","title":"nr_topics"},{"location":"getting_started/parameter%20tuning/parametertuning.html#low_memory","text":"low_memory sets UMAP's low_memory to True to make sure that less memory is used in computation. This slows down computation but allows UMAP to be ran on low memory machines.","title":"low_memory"},{"location":"getting_started/parameter%20tuning/parametertuning.html#calculate_probabilities","text":"calculate_probabilities lets you calculate the probabilities of each topic to each document. This is computationally quite expensive and is turned off by default.","title":"calculate_probabilities"},{"location":"getting_started/parameter%20tuning/parametertuning.html#umap","text":"UMAP is an amazing technique for dimensionality reduction. In BERTopic, it is used to reduce the dimensionality of document embedding into something that is easier to use with HDBSCAN in order to create good clusters. However, it does has a significant number of parameters you could take into account. As exposing all parameters in BERTopic would be difficult to manage, we can instantiate our UMAP model and pass it to BERTopic: from umap import UMAP umap_model = UMAP ( n_neighbors = 15 , n_components = 10 , metric = 'cosine' , low_memory = False ) topic_model = BERTopic ( umap_model = umap_model ) . fit ( docs )","title":"UMAP"},{"location":"getting_started/parameter%20tuning/parametertuning.html#n_neighbors","text":"n_neighbors is the numer of neighboring sample points used when making the manifold approximation. Increasing this value typically results in a more global view of the embedding structure whilst smaller values result in a more local view. Increasing this value often results in larger clusters being created.","title":"n_neighbors"},{"location":"getting_started/parameter%20tuning/parametertuning.html#n_components","text":"n_components refers to the dimensionality of the embeddings after reducing them. This is set as a default to 5 in order to reduce dimensionality as much as possible whilst trying to maximize the information kept in the resulting embeddings. Although lowering or increasing this value has an influence on the quality of embeddings, its effect is largest on the performance of HDBSCAN. Increasing this value too much and HDBSCAN will have a hard time clustering the high-dimensional embeddings. Lower this value too much and too little information in the resulting embeddings is available to create proper clusters. If you want to increase this value, I would advise setting using a metric for HDBSCAN that works well in high dimensional data.","title":"n_components"},{"location":"getting_started/parameter%20tuning/parametertuning.html#metric","text":"metric refers to the method used to compute the distances in high dimensional space. The default is cosine as we are dealing with high dimensional data. However, BERTopic is also able to use any input, even regular tabular data, to cluster the documents. Thus, you might want to change the metric to something that fits with your use case.","title":"metric"},{"location":"getting_started/parameter%20tuning/parametertuning.html#low_memory_1","text":"low_memory is used when datasets may consume a lot of memory. Using millions of documents can lead to memory issues and setting this value to True might alleviate some of the issues.","title":"low_memory"},{"location":"getting_started/parameter%20tuning/parametertuning.html#hdbscan","text":"After reducing the embeddings with UMAP, we use HDBSCAN to cluster our documents into clusters of similar documents. Similar to UMAP, HDBSCAN has many parameters that could be tweaked in order to improve the cluster's quality. from hdbscan import HDBSCAN hdbscan_model = HDBSCAN ( min_cluster_size = 10 , metric = 'euclidean' , prediction_data = True ) topic_model = BERTopic ( hdbscan_model = hdbscan_model ) . fit ( docs )","title":"HDBSCAN"},{"location":"getting_started/parameter%20tuning/parametertuning.html#min_cluster_size","text":"min_cluster_size is arguably the most important parameter in HDBSCAN. It controls the minimum size of a cluster and thereby the amount of clusters that will be generated. It is set to 10 as a default. Increasing this value results in less clusters but of larger size whereas decreasing this value results in more micro clusters being generated. Typically, I would advise on increasing this value rather than decreasing it.","title":"min_cluster_size"},{"location":"getting_started/parameter%20tuning/parametertuning.html#min_samples","text":"min_samples is automatically set to min_cluster_size and controls the amount of outliers are generated. Setting this value significantly lower than min_cluster_size might help you reduce the amount of noise you will get. Do note that outliers are typically to be expected and forcing the output to have no outliers may not properly represent the data.","title":"min_samples"},{"location":"getting_started/parameter%20tuning/parametertuning.html#metric_1","text":"metric , like with HDBSCAN is used to calculate the distances. Here, we went with euclidean as, after reducing the dimensionality, we have low dimensional data and not much optimization is necessary. However, if you increase n_components in UMAP, then it would be advised to look into metrics that work with high dimensional data.","title":"metric"},{"location":"getting_started/parameter%20tuning/parametertuning.html#prediction_data","text":"Make sure you always set this value to True as it is needed to predict new points later on. You can set this to False if you do not wish to predict any unseen datapoints.","title":"prediction_data"},{"location":"getting_started/quickstart/quickstart.html","text":"Installation \u00b6 Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ] Quick Start \u00b6 We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] Tip! Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages. Visualize Topics \u00b6 After having trained our BERTopic model, we can iteratively go through perhaps a hundred topics to get a good understanding of the topics that were extracted. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis : topic_model . visualize_topics () Save/Load BERTopic model \u00b6 We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic topic_model = BERTopic () topic_model . save ( \"my_model\" ) Then, we can load the model in one line: topic_model = BERTopic . load ( \"my_model\" ) Tip! If you do not want to save the embedding model because it is loaded from the cloud, simply run model.save(\"my_model\", save_embedding_model=False) instead. Then, you can load in the model with BERTopic.load(\"my_model\", embedding_model=\"whatever_model_you_used\") .","title":"Quickstart"},{"location":"getting_started/quickstart/quickstart.html#installation","text":"Installation, with sentence-transformers, can be done using pypi : pip install bertopic You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install bertopic [ flair ] pip install bertopic [ gensim ] pip install bertopic [ spacy ] pip install bertopic [ use ]","title":"Installation"},{"location":"getting_started/quickstart/quickstart.html#quick-start","text":"We start by extracting topics from the well-known 20 newsgroups dataset which is comprised of English documents: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After generating topics, we can access the frequent topics that were generated: >>> topic_model . get_topic_info () Topic Count Name - 1 4630 - 1 _can_your_will_any 0 693 49 _windows_drive_dos_file 1 466 32 _jesus_bible_christian_faith 2 441 2 _space_launch_orbit_lunar 3 381 22 _key_encryption_keys_encrypted -1 refers to all outliers and should typically be ignored. Next, let's take a look at the most frequent topic that was generated, topic 0: >>> topic_model . get_topic ( 0 ) [( 'windows' , 0.006152228076250982 ), ( 'drive' , 0.004982897610645755 ), ( 'dos' , 0.004845038866360651 ), ( 'file' , 0.004140142872194834 ), ( 'disk' , 0.004131678774810884 ), ( 'mac' , 0.003624848635985097 ), ( 'memory' , 0.0034840976976789903 ), ( 'software' , 0.0034415334250699077 ), ( 'email' , 0.0034239554442333257 ), ( 'pc' , 0.003047105930670237 )] Tip! Use BERTopic(language=\"multilingual\") to select a model that supports 50+ languages.","title":"Quick Start"},{"location":"getting_started/quickstart/quickstart.html#visualize-topics","text":"After having trained our BERTopic model, we can iteratively go through perhaps a hundred topics to get a good understanding of the topics that were extracted. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis : topic_model . visualize_topics ()","title":"Visualize Topics"},{"location":"getting_started/quickstart/quickstart.html#saveload-bertopic-model","text":"We can easily save a trained BERTopic model by calling save : from bertopic import BERTopic topic_model = BERTopic () topic_model . save ( \"my_model\" ) Then, we can load the model in one line: topic_model = BERTopic . load ( \"my_model\" ) Tip! If you do not want to save the embedding model because it is loaded from the cloud, simply run model.save(\"my_model\", save_embedding_model=False) instead. Then, you can load in the model with BERTopic.load(\"my_model\", embedding_model=\"whatever_model_you_used\") .","title":"Save/Load BERTopic model"},{"location":"getting_started/search/search.html","text":"After having created a BERTopic model, you might end up with over a hundred topics. Searching through those can be quite cumbersome especially if you are searching for a specific topic. Fortunately, BERTopic allows you to search for topics using search terms. First, let's create and train a BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) After having trained our model, we can use find_topics to search for topics that are similar to an input search_term. Here, we are going to be searching for topics that closely relate the search term \"motor\". Then, we extract the most similar topic and check the results: >>> similar_topics , similarity = topic_model . find_topics ( \"motor\" , top_n = 5 ) >>> topic_model . get_topic ( similar_topics [ 0 ]) [( 'bike' , 0.02275997701645559 ), ( 'motorcycle' , 0.011391202866080292 ), ( 'bikes' , 0.00981187573649205 ), ( 'dod' , 0.009614623748226669 ), ( 'honda' , 0.008247663662558535 ), ( 'ride' , 0.0064683227888861945 ), ( 'harley' , 0.006355502638631013 ), ( 'riding' , 0.005766601561614182 ), ( 'motorcycles' , 0.005596372493714447 ), ( 'advice' , 0.005534544418830091 )] It definitely seems that a topic was found that closely matches with \"motor\". The topic seems to be motorcycle related and therefore matches with our \"motor\" input. You can use the similarity variable to see how similar the extracted topics are to the search term. Note You can only use this method if an embedding model was supplied to BERTopic using embedding_model .","title":"Search Topics"},{"location":"getting_started/supervised/supervised.html","text":"In this tutorial, we will be looking at a new feature of BERTopic, namely (semi)-supervised topic modeling! This allows us to steer the dimensionality reduction of the embeddings into a space that closely follows any labels you might already have. In other words, we use a semi-supervised UMAP instance to reduce the dimensionality of embeddings before clustering the documents with HDBSCAN. First, let us prepare the data needed for our topic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] categories = data [ \"target\" ] category_names = data [ \"target_names\" ] We are using the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups posts that each is assigned to one of 20 categories. Using this dataset we can try to extract its corresponding topic model whilst taking its underlying categories into account. These categories are here the variable targets . Each document can be put into one of the following categories: >>> category_names [ 'alt.atheism' , 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' , 'misc.forsale' , 'rec.autos' , 'rec.motorcycles' , 'rec.sport.baseball' , 'rec.sport.hockey' , 'sci.crypt' , 'sci.electronics' , 'sci.med' , 'sci.space' , 'soc.religion.christian' , 'talk.politics.guns' , 'talk.politics.mideast' , 'talk.politics.misc' , 'talk.religion.misc' ] Semi-supervised Topic Modeling \u00b6 In semi-supervised topic modeling, we only have some labels for our documents. The documents for which we do have labels are used to somewhat guide BERTopic to the extraction of topics for those labels. The documents for which we do not have labels are assigned a -1. For this example, imagine we only the labels of categories that are related to computers and we want to create a topic model using semi-supervised modeling: labels_to_add = [ 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' ,] indices = [ category_names . index ( label ) for label in labels_to_add ] y = [ label if label in indices else - 1 for label in categories ] The y variable contains many -1 values since we do not know all the categories. Next, we use those newly constructed labels to again BERTopic semi-supervised: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = y ) And that is it! By defining certain classes for our documents, we can steer the topic modeling towards modeling the pre-defined categories. Supervised Topic Modeling \u00b6 In supervised topic modeling, we have labels for all our documents. This can be pre-defined topics or simply documents that you feel belong together regardless of their content. BERTopic will nudge the creation of topics towards these categories using the pre-defined labels. To perform supervised topic modeling, we simply use all categories: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = categories ) The topic model will be much more attuned to the categories that were defined previously. However, this does not mean that only topics for these categories will be found. BERTopic is likely to find more specific topics in those you have already defined. This allows you to discover previously unknown topics!","title":"(semi)-Supervised Topic Modeling"},{"location":"getting_started/supervised/supervised.html#semi-supervised-topic-modeling","text":"In semi-supervised topic modeling, we only have some labels for our documents. The documents for which we do have labels are used to somewhat guide BERTopic to the extraction of topics for those labels. The documents for which we do not have labels are assigned a -1. For this example, imagine we only the labels of categories that are related to computers and we want to create a topic model using semi-supervised modeling: labels_to_add = [ 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' ,] indices = [ category_names . index ( label ) for label in labels_to_add ] y = [ label if label in indices else - 1 for label in categories ] The y variable contains many -1 values since we do not know all the categories. Next, we use those newly constructed labels to again BERTopic semi-supervised: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = y ) And that is it! By defining certain classes for our documents, we can steer the topic modeling towards modeling the pre-defined categories.","title":"Semi-supervised Topic Modeling"},{"location":"getting_started/supervised/supervised.html#supervised-topic-modeling","text":"In supervised topic modeling, we have labels for all our documents. This can be pre-defined topics or simply documents that you feel belong together regardless of their content. BERTopic will nudge the creation of topics towards these categories using the pre-defined labels. To perform supervised topic modeling, we simply use all categories: topic_model = BERTopic ( verbose = True ) . fit ( docs , y = categories ) The topic model will be much more attuned to the categories that were defined previously. However, this does not mean that only topics for these categories will be found. BERTopic is likely to find more specific topics in those you have already defined. This allows you to discover previously unknown topics!","title":"Supervised Topic Modeling"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html","text":"Tips & Tricks \u00b6 Document length \u00b6 As a default, we are using sentence-transformers to embed our documents. However, as the name implies, the embedding model works best for either sentences or paragraphs. This means that whenever you have a set of documents, where each documents contains several paragraphs, BERTopic will struggle getting accurately extracting a topic from that document. Several paragraphs typically means several topics and BERTopic will assign only one topic to a document. Therefore, it is advised to split up longer documents into either sentences or paragraphs before embedding them. That way, BERTopic will have a much easier job identifying topics in isolation. Removing stop words \u00b6 At times, stop words might end up in our topic representations. This is something we typically want to avoid as they contribute little to the interpretation of the topics. However, removing stop words as a preprocessing step is not advised as the transformer-based embedding models that we use need the full context in order to create accurate embeddings. Instead, we can use the CountVectorizer to preprocess our documents after having generated embeddings and clustered our documents. Personally, I have found almost no disadvantages to using the CountVectorizer to remove stopwords and it is something I would strongly advise to try out: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) Diversify topic representation \u00b6 After having calculated our top n words per topic there might be many words that essentially mean the same thing. As a little bonus, we can use the diversity parameter in BERTopic to diversity words in each topic such that we limit the number of duplicate words we find in each topic. This is done using an algorithm called Maximal Marginal Relevance which compares word embeddings with the topic embedding. We do this by specifying a value between 0 and 1, with 0 being not at all diverse and 1 being completely diverse: from bertopic import BERTopic topic_model = BERTopic ( diversity = 0.2 ) Since MMR is using word embeddings to diversify the topic representations, it is necessary to pass the embedding model to BERTopic if you are using pre-computed embeddings: from bertopic import BERTopic from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) topic_model = BERTopic ( embedding_model = sentence_model , diversity = 0.2 ) Topic-term matrix \u00b6 Although BERTopic focuses on clustering our documents, the end result does contain a topic-term matrix. This topic-term matrix is calculated using c-TF-IDF, a TF-IDF procedure optimized for class-based analyses. To extract the topic-term matrix (or c-TF-IDF matrix) with the corresponding words, we can simply do the following: topic_term_matrix = topic_model . c_tf_idf_ words = topic_model . vectorizer_model . get_feature_names () Note This only works if you have set diversity=None , for all other values the top n are further optimized using MMR which is not represented in the topic-term matrix as it does not optimize the entire matrix. Pre-compute embeddings \u00b6 Typically, we want to iterate fast over different versions of our BERTopic model whilst we are trying to optimize it to a specific use case. To speed up this process, we can pre-compute the embeddings, save them, and pass them to BERTopic so it does not need to calculate the embeddings each time: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train our topic model using our pre-trained sentence-transformers embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings ) Speed up UMAP \u00b6 At times, UMAP may take a while to fit on the embeddings that you have. This often happens when you have the embeddings millions of documents that you want to reduce in dimensionality. There is a trick that can speed up this process somewhat: Initializing UMAP with rescaled PCA embeddings. Without going in too much detail (look here for more information), you can reduce the embeddings using PCA and use that as a starting point. This can speed up the dimensionality reduction a bit: import numpy as np from umap import UMAP from bertopic import BERTopic from sklearn.decomposition import PCA def rescale ( x , inplace = False ): \"\"\" Rescale an embedding so optimization will not have convergence issues. \"\"\" if not inplace : x = np . array ( x , copy = True ) x /= np . std ( x [:, 0 ]) * 10000 return x # Initialize and rescale PCA embeddings pca_embeddings = rescale ( PCA ( n_components = 5 ) . fit_transform ( embeddings )) # Start UMAP from PCA embeddings umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = \"cosine\" , init = pca_embeddings , ) # Pass the model to BERTopic: topic_model = BERTopic ( umap_model = umap_model ) GPU acceleration \u00b6 You can use cuML to speed up both UMAP and HDBSCAN through GPU acceleration: from bertopic import BERTopic from cuml.cluster import HDBSCAN from cuml.manifold import UMAP # Create instances of GPU-accelerated UMAP and HDBSCAN umap_model = UMAP ( n_components = 5 , n_neighbors = 15 , min_dist = 0.0 ) hdbscan_model = HDBSCAN ( min_samples = 10 , gen_min_span_tree = True ) # Pass the above models to be used in BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Depending on the embeddings you are using, you might want to normalize them first in order to force a cosine-related distance metric in UMAP: from cuml.preprocessing import normalize embeddings = normalize ( embeddings ) Finding similar topics between models \u00b6 Whenever you have trained seperate BERTopic models on different datasets, it might be worthful to find the similarities among these models. Is there overlap between topics in model A and topic in model B? In other words, can we find topics in model A that are similar to those in model B? We can compare the topic representations of several models in two ways. First, by comparing the topic embeddings that are created when using the same embedding model across both fitted BERTopic instances. Second, we can compare the c-TF-IDF representations instead assuming we have fixed the vocabulary in both instances. This example will go into the former, using the same embedding model across two BERTopic instances. To do this comparison, let's first create an example where I trained two models, one on an English dataset and one on a Dutch dataset: from datasets import load_dataset from bertopic import BERTopic from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # The same embedding model needs to be used for both topic models # and since we are dealing with multiple languages, the model needs to be multi-lingual sentence_model = SentenceTransformer ( \"paraphrase-multilingual-MiniLM-L12-v2\" ) # To make this example reproducible umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , random_state = 42 ) # English en_dataset = load_dataset ( \"stsb_multi_mt\" , name = \"en\" , split = \"train\" ) . to_pandas () . sentence1 . tolist () en_model = BERTopic ( embedding_model = sentence_model , umap_model = umap_model ) en_model . fit ( en_dataset ) # Dutch nl_dataset = load_dataset ( \"stsb_multi_mt\" , name = \"nl\" , split = \"train\" ) . to_pandas () . sentence1 . tolist () nl_model = BERTopic ( embedding_model = sentence_model , umap_model = umap_model ) nl_model . fit ( nl_dataset ) In the code above, there is one important thing to note and that is the sentence_model . This model needs to be exactly the same in all BERTopic models, otherwise, it is not possible to compare topic models. Next, we can calculate the similarity between topics in the English topic model en_model and the Dutch model nl_model . To do so, we can simply calculate the cosine similarity between the topic_embedding of both models: from sklearn.metrics.pairwise import cosine_similarity sim_matrix = cosine_similarity ( en_model . topic_embeddings_ , nl_model . topic_embeddings_ ) Now that we know which topics are similar to each other, we can extract the most similar topics. Let's say that we have topic 10 in the en_model which represents a topic related to trains: >>> topic = 10 >>> en_model . get_topic ( topic ) [( 'train' , 0.2588080580844999 ), ( 'tracks' , 0.1392140438801078 ), ( 'station' , 0.12126454635946024 ), ( 'passenger' , 0.058057876475695866 ), ( 'engine' , 0.05123717127783682 ), ( 'railroad' , 0.048142847325312044 ), ( 'waiting' , 0.04098973702226946 ), ( 'track' , 0.03978248702913929 ), ( 'subway' , 0.03834661195748458 ), ( 'steam' , 0.03834661195748458 )] To find the matching topic, we extract the most similar topic in the sim_matrix : >>> most_similar_topic = np . argmax ( sim_matrix [ topic + 1 ]) - 1 >>> nl_model . get_topic ( most_similar_topic ) [( 'trein' , 0.24186603209316418 ), ( 'spoor' , 0.1338118418551581 ), ( 'sporen' , 0.07683661859111401 ), ( 'station' , 0.056990389779394225 ), ( 'stoommachine' , 0.04905829711711234 ), ( 'zilveren' , 0.04083879598477808 ), ( 'treinen' , 0.03534099197032758 ), ( 'treinsporen' , 0.03534099197032758 ), ( 'staat' , 0.03481332997324445 ), ( 'zwarte' , 0.03179591746822408 )] It seems to be working as, for example, trein is a translation of train and sporen a translation of tracks ! You can do this for every single topic to find out which topic in the en_model might belong to a model in the nl_model . Multi-modal data \u00b6 Concept is a variation of BERTopic for multi-modal data, such as images with captions. Although we can use that package for multi-modal data, we can perform a small trick with BERTopic to have a similar feature. BERTopic is a relatively modular approach that attempts to isolate steps from one another. This means, for example, that you can use k-Means instead of HDBSCAN or PCA instead of UMAP as it does not make any assumptions with respect to the nature of the clustering. Similarly, you can pass pre-calculated embeddings to BERTopic that represent the documents that you have. However, it does not make any assumption with respect to the relationship between those embeddings and the documents. This means that we could pass any metadata to BERTopic to cluster on instead of document embeddings. In this example, we can separate our embeddings from our documents so that the embeddings are generated from images instead of their corresponding images. Thus, we will cluster image embeddings but create the topic representation from the related captions. In this example, we first need to fetch our data, namely the Flickr 8k dataset that contains images with captions: import os import glob import zipfile import numpy as np import pandas as pd from tqdm import tqdm from PIL import Image from sentence_transformers import SentenceTransformer , util # Flickr 8k images img_folder = 'photos/' caps_folder = 'captions/' if not os . path . exists ( img_folder ) or len ( os . listdir ( img_folder )) == 0 : os . makedirs ( img_folder , exist_ok = True ) if not os . path . exists ( 'Flickr8k_Dataset.zip' ): #Download dataset if does not exist util . http_get ( 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip' , 'Flickr8k_Dataset.zip' ) util . http_get ( 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip' , 'Flickr8k_text.zip' ) for folder , file in [( img_folder , 'Flickr8k_Dataset.zip' ), ( caps_folder , 'Flickr8k_text.zip' )]: with zipfile . ZipFile ( file , 'r' ) as zf : for member in tqdm ( zf . infolist (), desc = 'Extracting' ): zf . extract ( member , folder ) images = list ( glob . glob ( 'photos/Flicker8k_Dataset/*.jpg' )) # Prepare dataframe captions = pd . read_csv ( \"captions/Flickr8k.lemma.token.txt\" , sep = ' \\t ' , names = [ \"img_id\" , \"img_caption\" ]) captions . img_id = captions . apply ( lambda row : \"photos/Flicker8k_Dataset/\" + row . img_id . split ( \".jpg\" )[ 0 ] + \".jpg\" , 1 ) captions = captions . groupby ([ \"img_id\" ])[ \"img_caption\" ] . apply ( ',' . join ) . reset_index () captions = pd . merge ( captions , pd . Series ( images , name = \"img_id\" ), on = \"img_id\" ) # Extract images together with their documents/captions images = captions . img_id . to_list () docs = captions . img_caption . to_list () Now that we have our images and captions, we need to generate our image embeddings: model = SentenceTransformer ( 'clip-ViT-B-32' ) # Prepare images batch_size = 32 nr_iterations = int ( np . ceil ( len ( images ) / batch_size )) # Embed images per batch embeddings = [] for i in tqdm ( range ( nr_iterations )): start_index = i * batch_size end_index = ( i * batch_size ) + batch_size images_to_embed = [ Image . open ( filepath ) for filepath in images [ start_index : end_index ]] img_emb = model . encode ( images_to_embed , show_progress_bar = False ) embeddings . extend ( img_emb . tolist ()) # Close images for image in images_to_embed : image . close () embeddings = np . array ( embeddings ) Finally, we can fit BERTopic the way we are used to, with documents and embeddings: from bertopic import BERTopic from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs , embeddings ) captions [ \"Topic\" ] = topics After fitting our model, let's inspect a topic about skateboarders: >>> topic_model . get_topic ( 2 ) [( 'skateboard' , 0.09592033177340711 ), ( 'skateboarder' , 0.07792520092546491 ), ( 'trick' , 0.07481578896400298 ), ( 'ramp' , 0.056952605147927216 ), ( 'skate' , 0.03745127816149923 ), ( 'perform' , 0.036546213623432654 ), ( 'bicycle' , 0.03453483070441857 ), ( 'bike' , 0.033233021253898994 ), ( 'jump' , 0.026709362981948037 ), ( 'air' , 0.025422798170830936 )] Based on the above output, we can take an image to see if the representation makes sense: image = captions . loc [ captions . Topic == 2 , \"img_id\" ] . values . tolist ()[ 0 ] Image . open ( image ) KeyBERT & BERTopic \u00b6 Although BERTopic focuses on topic extraction methods that does not assume specific structures for the generated clusters, it is possible to do this on a more local level. More specifically, we can use KeyBERT to generate a number of keywords for each document and then build a vocabulary on top of that as the input for BERTopic. This way, we can select words that we know have meaning to a topic, without focusing on the centroid of that cluster. This also allows more frequent words to pop-up regardless of the structure and density of a cluster. To do this, we first need to run KeyBERT on our data and create our vocabulary: from sklearn.datasets import fetch_20newsgroups from keybert import KeyBERT # Prepare documents docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] # Extract keywords kw_model = KeyBERT () keywords = kw_model . extract_keywords ( docs ) # Create our vocabulary vocabulary = [ k [ 0 ] for keyword in keywords for k in keyword ] vocabulary = list ( set ( vocabulary )) Then, we pass our vocabulary to BERTopic and train the model: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( vocabulary = vocabulary ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs )","title":"Tips & Tricks"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#tips-tricks","text":"","title":"Tips &amp; Tricks"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#document-length","text":"As a default, we are using sentence-transformers to embed our documents. However, as the name implies, the embedding model works best for either sentences or paragraphs. This means that whenever you have a set of documents, where each documents contains several paragraphs, BERTopic will struggle getting accurately extracting a topic from that document. Several paragraphs typically means several topics and BERTopic will assign only one topic to a document. Therefore, it is advised to split up longer documents into either sentences or paragraphs before embedding them. That way, BERTopic will have a much easier job identifying topics in isolation.","title":"Document length"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#removing-stop-words","text":"At times, stop words might end up in our topic representations. This is something we typically want to avoid as they contribute little to the interpretation of the topics. However, removing stop words as a preprocessing step is not advised as the transformer-based embedding models that we use need the full context in order to create accurate embeddings. Instead, we can use the CountVectorizer to preprocess our documents after having generated embeddings and clustered our documents. Personally, I have found almost no disadvantages to using the CountVectorizer to remove stopwords and it is something I would strongly advise to try out: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model )","title":"Removing stop words"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#diversify-topic-representation","text":"After having calculated our top n words per topic there might be many words that essentially mean the same thing. As a little bonus, we can use the diversity parameter in BERTopic to diversity words in each topic such that we limit the number of duplicate words we find in each topic. This is done using an algorithm called Maximal Marginal Relevance which compares word embeddings with the topic embedding. We do this by specifying a value between 0 and 1, with 0 being not at all diverse and 1 being completely diverse: from bertopic import BERTopic topic_model = BERTopic ( diversity = 0.2 ) Since MMR is using word embeddings to diversify the topic representations, it is necessary to pass the embedding model to BERTopic if you are using pre-computed embeddings: from bertopic import BERTopic from sentence_transformers import SentenceTransformer sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) topic_model = BERTopic ( embedding_model = sentence_model , diversity = 0.2 )","title":"Diversify topic representation"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#topic-term-matrix","text":"Although BERTopic focuses on clustering our documents, the end result does contain a topic-term matrix. This topic-term matrix is calculated using c-TF-IDF, a TF-IDF procedure optimized for class-based analyses. To extract the topic-term matrix (or c-TF-IDF matrix) with the corresponding words, we can simply do the following: topic_term_matrix = topic_model . c_tf_idf_ words = topic_model . vectorizer_model . get_feature_names () Note This only works if you have set diversity=None , for all other values the top n are further optimized using MMR which is not represented in the topic-term matrix as it does not optimize the entire matrix.","title":"Topic-term matrix"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#pre-compute-embeddings","text":"Typically, we want to iterate fast over different versions of our BERTopic model whilst we are trying to optimize it to a specific use case. To speed up this process, we can pre-compute the embeddings, save them, and pass them to BERTopic so it does not need to calculate the embeddings each time: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train our topic model using our pre-trained sentence-transformers embeddings topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs , embeddings )","title":"Pre-compute embeddings"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#speed-up-umap","text":"At times, UMAP may take a while to fit on the embeddings that you have. This often happens when you have the embeddings millions of documents that you want to reduce in dimensionality. There is a trick that can speed up this process somewhat: Initializing UMAP with rescaled PCA embeddings. Without going in too much detail (look here for more information), you can reduce the embeddings using PCA and use that as a starting point. This can speed up the dimensionality reduction a bit: import numpy as np from umap import UMAP from bertopic import BERTopic from sklearn.decomposition import PCA def rescale ( x , inplace = False ): \"\"\" Rescale an embedding so optimization will not have convergence issues. \"\"\" if not inplace : x = np . array ( x , copy = True ) x /= np . std ( x [:, 0 ]) * 10000 return x # Initialize and rescale PCA embeddings pca_embeddings = rescale ( PCA ( n_components = 5 ) . fit_transform ( embeddings )) # Start UMAP from PCA embeddings umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = \"cosine\" , init = pca_embeddings , ) # Pass the model to BERTopic: topic_model = BERTopic ( umap_model = umap_model )","title":"Speed up UMAP"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#gpu-acceleration","text":"You can use cuML to speed up both UMAP and HDBSCAN through GPU acceleration: from bertopic import BERTopic from cuml.cluster import HDBSCAN from cuml.manifold import UMAP # Create instances of GPU-accelerated UMAP and HDBSCAN umap_model = UMAP ( n_components = 5 , n_neighbors = 15 , min_dist = 0.0 ) hdbscan_model = HDBSCAN ( min_samples = 10 , gen_min_span_tree = True ) # Pass the above models to be used in BERTopic topic_model = BERTopic ( umap_model = umap_model , hdbscan_model = hdbscan_model ) topics , probs = topic_model . fit_transform ( docs ) Depending on the embeddings you are using, you might want to normalize them first in order to force a cosine-related distance metric in UMAP: from cuml.preprocessing import normalize embeddings = normalize ( embeddings )","title":"GPU acceleration"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#finding-similar-topics-between-models","text":"Whenever you have trained seperate BERTopic models on different datasets, it might be worthful to find the similarities among these models. Is there overlap between topics in model A and topic in model B? In other words, can we find topics in model A that are similar to those in model B? We can compare the topic representations of several models in two ways. First, by comparing the topic embeddings that are created when using the same embedding model across both fitted BERTopic instances. Second, we can compare the c-TF-IDF representations instead assuming we have fixed the vocabulary in both instances. This example will go into the former, using the same embedding model across two BERTopic instances. To do this comparison, let's first create an example where I trained two models, one on an English dataset and one on a Dutch dataset: from datasets import load_dataset from bertopic import BERTopic from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # The same embedding model needs to be used for both topic models # and since we are dealing with multiple languages, the model needs to be multi-lingual sentence_model = SentenceTransformer ( \"paraphrase-multilingual-MiniLM-L12-v2\" ) # To make this example reproducible umap_model = UMAP ( n_neighbors = 15 , n_components = 5 , min_dist = 0.0 , metric = 'cosine' , random_state = 42 ) # English en_dataset = load_dataset ( \"stsb_multi_mt\" , name = \"en\" , split = \"train\" ) . to_pandas () . sentence1 . tolist () en_model = BERTopic ( embedding_model = sentence_model , umap_model = umap_model ) en_model . fit ( en_dataset ) # Dutch nl_dataset = load_dataset ( \"stsb_multi_mt\" , name = \"nl\" , split = \"train\" ) . to_pandas () . sentence1 . tolist () nl_model = BERTopic ( embedding_model = sentence_model , umap_model = umap_model ) nl_model . fit ( nl_dataset ) In the code above, there is one important thing to note and that is the sentence_model . This model needs to be exactly the same in all BERTopic models, otherwise, it is not possible to compare topic models. Next, we can calculate the similarity between topics in the English topic model en_model and the Dutch model nl_model . To do so, we can simply calculate the cosine similarity between the topic_embedding of both models: from sklearn.metrics.pairwise import cosine_similarity sim_matrix = cosine_similarity ( en_model . topic_embeddings_ , nl_model . topic_embeddings_ ) Now that we know which topics are similar to each other, we can extract the most similar topics. Let's say that we have topic 10 in the en_model which represents a topic related to trains: >>> topic = 10 >>> en_model . get_topic ( topic ) [( 'train' , 0.2588080580844999 ), ( 'tracks' , 0.1392140438801078 ), ( 'station' , 0.12126454635946024 ), ( 'passenger' , 0.058057876475695866 ), ( 'engine' , 0.05123717127783682 ), ( 'railroad' , 0.048142847325312044 ), ( 'waiting' , 0.04098973702226946 ), ( 'track' , 0.03978248702913929 ), ( 'subway' , 0.03834661195748458 ), ( 'steam' , 0.03834661195748458 )] To find the matching topic, we extract the most similar topic in the sim_matrix : >>> most_similar_topic = np . argmax ( sim_matrix [ topic + 1 ]) - 1 >>> nl_model . get_topic ( most_similar_topic ) [( 'trein' , 0.24186603209316418 ), ( 'spoor' , 0.1338118418551581 ), ( 'sporen' , 0.07683661859111401 ), ( 'station' , 0.056990389779394225 ), ( 'stoommachine' , 0.04905829711711234 ), ( 'zilveren' , 0.04083879598477808 ), ( 'treinen' , 0.03534099197032758 ), ( 'treinsporen' , 0.03534099197032758 ), ( 'staat' , 0.03481332997324445 ), ( 'zwarte' , 0.03179591746822408 )] It seems to be working as, for example, trein is a translation of train and sporen a translation of tracks ! You can do this for every single topic to find out which topic in the en_model might belong to a model in the nl_model .","title":"Finding similar topics between models"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#multi-modal-data","text":"Concept is a variation of BERTopic for multi-modal data, such as images with captions. Although we can use that package for multi-modal data, we can perform a small trick with BERTopic to have a similar feature. BERTopic is a relatively modular approach that attempts to isolate steps from one another. This means, for example, that you can use k-Means instead of HDBSCAN or PCA instead of UMAP as it does not make any assumptions with respect to the nature of the clustering. Similarly, you can pass pre-calculated embeddings to BERTopic that represent the documents that you have. However, it does not make any assumption with respect to the relationship between those embeddings and the documents. This means that we could pass any metadata to BERTopic to cluster on instead of document embeddings. In this example, we can separate our embeddings from our documents so that the embeddings are generated from images instead of their corresponding images. Thus, we will cluster image embeddings but create the topic representation from the related captions. In this example, we first need to fetch our data, namely the Flickr 8k dataset that contains images with captions: import os import glob import zipfile import numpy as np import pandas as pd from tqdm import tqdm from PIL import Image from sentence_transformers import SentenceTransformer , util # Flickr 8k images img_folder = 'photos/' caps_folder = 'captions/' if not os . path . exists ( img_folder ) or len ( os . listdir ( img_folder )) == 0 : os . makedirs ( img_folder , exist_ok = True ) if not os . path . exists ( 'Flickr8k_Dataset.zip' ): #Download dataset if does not exist util . http_get ( 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip' , 'Flickr8k_Dataset.zip' ) util . http_get ( 'https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip' , 'Flickr8k_text.zip' ) for folder , file in [( img_folder , 'Flickr8k_Dataset.zip' ), ( caps_folder , 'Flickr8k_text.zip' )]: with zipfile . ZipFile ( file , 'r' ) as zf : for member in tqdm ( zf . infolist (), desc = 'Extracting' ): zf . extract ( member , folder ) images = list ( glob . glob ( 'photos/Flicker8k_Dataset/*.jpg' )) # Prepare dataframe captions = pd . read_csv ( \"captions/Flickr8k.lemma.token.txt\" , sep = ' \\t ' , names = [ \"img_id\" , \"img_caption\" ]) captions . img_id = captions . apply ( lambda row : \"photos/Flicker8k_Dataset/\" + row . img_id . split ( \".jpg\" )[ 0 ] + \".jpg\" , 1 ) captions = captions . groupby ([ \"img_id\" ])[ \"img_caption\" ] . apply ( ',' . join ) . reset_index () captions = pd . merge ( captions , pd . Series ( images , name = \"img_id\" ), on = \"img_id\" ) # Extract images together with their documents/captions images = captions . img_id . to_list () docs = captions . img_caption . to_list () Now that we have our images and captions, we need to generate our image embeddings: model = SentenceTransformer ( 'clip-ViT-B-32' ) # Prepare images batch_size = 32 nr_iterations = int ( np . ceil ( len ( images ) / batch_size )) # Embed images per batch embeddings = [] for i in tqdm ( range ( nr_iterations )): start_index = i * batch_size end_index = ( i * batch_size ) + batch_size images_to_embed = [ Image . open ( filepath ) for filepath in images [ start_index : end_index ]] img_emb = model . encode ( images_to_embed , show_progress_bar = False ) embeddings . extend ( img_emb . tolist ()) # Close images for image in images_to_embed : image . close () embeddings = np . array ( embeddings ) Finally, we can fit BERTopic the way we are used to, with documents and embeddings: from bertopic import BERTopic from sklearn.cluster import KMeans from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs , embeddings ) captions [ \"Topic\" ] = topics After fitting our model, let's inspect a topic about skateboarders: >>> topic_model . get_topic ( 2 ) [( 'skateboard' , 0.09592033177340711 ), ( 'skateboarder' , 0.07792520092546491 ), ( 'trick' , 0.07481578896400298 ), ( 'ramp' , 0.056952605147927216 ), ( 'skate' , 0.03745127816149923 ), ( 'perform' , 0.036546213623432654 ), ( 'bicycle' , 0.03453483070441857 ), ( 'bike' , 0.033233021253898994 ), ( 'jump' , 0.026709362981948037 ), ( 'air' , 0.025422798170830936 )] Based on the above output, we can take an image to see if the representation makes sense: image = captions . loc [ captions . Topic == 2 , \"img_id\" ] . values . tolist ()[ 0 ] Image . open ( image )","title":"Multi-modal data"},{"location":"getting_started/tips_and_tricks/tips_and_tricks.html#keybert-bertopic","text":"Although BERTopic focuses on topic extraction methods that does not assume specific structures for the generated clusters, it is possible to do this on a more local level. More specifically, we can use KeyBERT to generate a number of keywords for each document and then build a vocabulary on top of that as the input for BERTopic. This way, we can select words that we know have meaning to a topic, without focusing on the centroid of that cluster. This also allows more frequent words to pop-up regardless of the structure and density of a cluster. To do this, we first need to run KeyBERT on our data and create our vocabulary: from sklearn.datasets import fetch_20newsgroups from keybert import KeyBERT # Prepare documents docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] # Extract keywords kw_model = KeyBERT () keywords = kw_model . extract_keywords ( docs ) # Create our vocabulary vocabulary = [ k [ 0 ] for keyword in keywords for k in keyword ] vocabulary = list ( set ( vocabulary )) Then, we pass our vocabulary to BERTopic and train the model: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( vocabulary = vocabulary ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs )","title":"KeyBERT &amp; BERTopic"},{"location":"getting_started/topicreduction/topicreduction.html","text":"BERTopic uses HDSCAN for clustering the data and it cannot specify the number of clusters you would want. To a certain extent, this is an advantage, as we can trust HDBSCAN to be better in finding the number of clusters than we are. Instead, we can try to reduce the number of topics that have been created. Below, you will find three methods of doing so. Manual Topic Reduction \u00b6 Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = 20 ) It is also possible to manually select certain topics that you believe should be merged. For example, if topic 1 is 1_space_launch_moon_nasa and topic 2 is 2_spacecraft_solar_space_orbit it might make sense to merge those two topics: topics_to_merge = [ 1 , 2 ] topic_model . merge_topics ( docs , topics_to_merge ) If you have several groups of topics you want to merge, create a list of lists instead: topics_to_merge = [[ 1 , 2 ] [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge ) Automatic Topic Reduction \u00b6 One issue with the approach above is that it will merge topics regardless of whether they are very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. To do this, we can use HDBSCAN to cluster our topics using each c-TF-IDF representation. Then, we merge topics that are clustered together. Another benefit of HDBSCAN is that it generates outliers. These outliers prevent topics from being merged if no other topics are similar. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = \"auto\" ) Topic Reduction after Training \u00b6 Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterward how many topics seem realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Further reduce topics topic_model . reduce_topics ( docs , nr_topics = 30 ) # Access updated topics topics = topic_model . topics_ The reasoning for putting docs as a parameter is that the documents are not saved within BERTopic on purpose. If you were to have a million documents, it is very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction"},{"location":"getting_started/topicreduction/topicreduction.html#manual-topic-reduction","text":"Each resulting topic has its own feature vector constructed from c-TF-IDF. Using those feature vectors, we can find the most similar topics and merge them. If we do this iteratively, starting from the least frequent topic, we can reduce the number of topics quite easily. We do this until we reach the value of nr_topics : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = 20 ) It is also possible to manually select certain topics that you believe should be merged. For example, if topic 1 is 1_space_launch_moon_nasa and topic 2 is 2_spacecraft_solar_space_orbit it might make sense to merge those two topics: topics_to_merge = [ 1 , 2 ] topic_model . merge_topics ( docs , topics_to_merge ) If you have several groups of topics you want to merge, create a list of lists instead: topics_to_merge = [[ 1 , 2 ] [ 3 , 4 ]] topic_model . merge_topics ( docs , topics_to_merge )","title":"Manual Topic Reduction"},{"location":"getting_started/topicreduction/topicreduction.html#automatic-topic-reduction","text":"One issue with the approach above is that it will merge topics regardless of whether they are very similar. They are simply the most similar out of all options. This can be resolved by reducing the number of topics automatically. To do this, we can use HDBSCAN to cluster our topics using each c-TF-IDF representation. Then, we merge topics that are clustered together. Another benefit of HDBSCAN is that it generates outliers. These outliers prevent topics from being merged if no other topics are similar. To use this option, we simply set nr_topics to \"auto\" : from bertopic import BERTopic topic_model = BERTopic ( nr_topics = \"auto\" )","title":"Automatic Topic Reduction"},{"location":"getting_started/topicreduction/topicreduction.html#topic-reduction-after-training","text":"Finally, we can also reduce the number of topics after having trained a BERTopic model. The advantage of doing so, is that you can decide the number of topics after knowing how many are created. It is difficult to predict before training your model how many topics that are in your documents and how many will be extracted. Instead, we can decide afterward how many topics seem realistic: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics -> Typically over 50 topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Further reduce topics topic_model . reduce_topics ( docs , nr_topics = 30 ) # Access updated topics topics = topic_model . topics_ The reasoning for putting docs as a parameter is that the documents are not saved within BERTopic on purpose. If you were to have a million documents, it is very inefficient to save those in BERTopic instead of a dedicated database.","title":"Topic Reduction after Training"},{"location":"getting_started/topicrepresentation/topicrepresentation.html","text":"The topics that are extracted from BERTopic are represented by words. These words are extracted from the documents occupying their topics using a class-based TF-IDF. This allows us to extract words that are interesting to a topic but less so to another. Update Topic Representation after Training \u00b6 When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( n_gram_range = ( 2 , 3 )) topics , probs = topic_model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> topic_model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> topic_model . update_topics ( docs , n_gram_range = ( 1 , 3 )) >>> topic_model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range or use your own custom sklearn.feature_extraction.text.CountVectorizer and pass that instead: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"English\" , ngram_range = ( 1 , 5 )) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) Tip! If you want to change the topics to something else, whether that is merging them or removing outliers, you can pass in a custom list of topics in order to update them: topic_model.update_topics(docs, topics=my_updated_topics) Custom labels \u00b6 The topic labels are currently automatically generated by taking the top 3 words and combining them using the _ separator. Although this is an informative label, in practice, this is definitely not the prettiest nor necessarily the most accurate label. For example, although the topic label 1_space_nasa_orbit is informative, we would prefer to have a bit more intuitive label, such as space travel . The difficulty with creating such topic labels is that much of the interpretation is left to the user. Would space travel be more accurate or perhaps space explorations ? To truly understand which labels are most suited, going into some of the documents in topics is especially helpful. Although we can go through every single topic ourselves and try to label them, we can start by creating an overview of labels that have the length and number of words that we are looking for. To do so, we can generate our list of topic labels with .get_topic_labels and define the number of words, the separator, word length, etc: topic_labels = topic_model . generate_topic_labels ( nr_words = 3 , topic_prefix = False , word_length = 10 , separator = \", \" ) In the above example, 1_space_nasa_orbit would turn into space, nasa, orbit since we selected 3 words, no topic prefix, and the , separator. We can then either change our topic_labels to whatever we want or directly pass them to .set_topic_labels so that they can be used across most visualization functions: topic_model . set_topic_labels ( topic_labels ) It is also possible to only change a few topic labels at a time by passing a dictionary where the key represents the topic ID and the value is the topic label : topic_model . set_topic_labels ({ 1 : \"Space Travel\" , 7 : \"Religion\" }) Then, to make use of those custom topic labels across visualizations, such as .visualize_hierarchy() , we can use the custom_labels=True parameter that is found in most visualizations. fig = topic_model . visualize_barchart ( custom_labels = True ) Optimize labels \u00b6 The great advantage of passing custom labels to BERTopic is that when more accurate zero-shot are released, we can simply use those on top of BERTopic to further fine-tune the labeling. For example, let's say you have a set of potential topic labels that you want to use instead of the ones generated by BERTopic. You could use the bart-large-mnli model to find which user-defined labels best represent the BERTopic-generated labels: from transformers import pipeline classifier = pipeline ( \"zero-shot-classification\" , model = \"facebook/bart-large-mnli\" ) # A selected topic representation # 'god jesus atheists atheism belief atheist believe exist beliefs existence' sequence_to_classify = \" \" . join ([ word for word , _ in topic_model . get_topic ( 1 )]) # Our set of potential topic labels candidate_labels = [ 'cooking' , 'dancing' , 'religion' ] classifier ( sequence_to_classify , candidate_labels ) #{'labels': ['cooking', 'dancing', 'religion'], # 'scores': [0.086, 0.063, 0.850], # 'sequence': 'god jesus atheists atheism belief atheist believe exist beliefs existence'}","title":"Topic Representation"},{"location":"getting_started/topicrepresentation/topicrepresentation.html#update-topic-representation-after-training","text":"When you have trained a model and viewed the topics and the words that represent them, you might not be satisfied with the representation. Perhaps you forgot to remove stop_words or you want to try out a different n_gram_range. We can use the function update_topics to update the topic representation with new parameters for c-TF-IDF : from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Create topics docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( n_gram_range = ( 2 , 3 )) topics , probs = topic_model . fit_transform ( docs ) From the model created above, one of the most frequent topics is the following: >>> topic_model . get_topic ( 31 )[: 10 ] [( 'clipper chip' , 0.007240771542316232 ), ( 'key escrow' , 0.004601603973377443 ), ( 'law enforcement' , 0.004277247929596332 ), ( 'intercon com' , 0.0035961920238955824 ), ( 'amanda walker' , 0.003474856425297157 ), ( 'serial number' , 0.0029876119137150358 ), ( 'com amanda' , 0.002789303096817983 ), ( 'intercon com amanda' , 0.0027386688593327084 ), ( 'amanda intercon' , 0.002585262048515583 ), ( 'amanda intercon com' , 0.002585262048515583 )] Although there does seem to be some relation between words, it is difficult, at least for me, to intuitively understand what the topic is about. Instead, let's simplify the topic representation by setting n_gram_range to (1, 3) to also allow for single words. >>> topic_model . update_topics ( docs , n_gram_range = ( 1 , 3 )) >>> topic_model . get_topic ( 31 )[: 10 ] [( 'encryption' , 0.008021846079148017 ), ( 'clipper' , 0.00789642647602742 ), ( 'chip' , 0.00637127942464045 ), ( 'key' , 0.006363124787175884 ), ( 'escrow' , 0.005030980365244285 ), ( 'clipper chip' , 0.0048271268437973395 ), ( 'keys' , 0.0043245812747907545 ), ( 'crypto' , 0.004311198708675516 ), ( 'intercon' , 0.0038772934659295076 ), ( 'amanda' , 0.003516026493904586 )] To me, the combination of the words above seem a bit more intuitive than the words we previously had! You can play around with n_gram_range or use your own custom sklearn.feature_extraction.text.CountVectorizer and pass that instead: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"English\" , ngram_range = ( 1 , 5 )) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) Tip! If you want to change the topics to something else, whether that is merging them or removing outliers, you can pass in a custom list of topics in order to update them: topic_model.update_topics(docs, topics=my_updated_topics)","title":"Update Topic Representation after Training"},{"location":"getting_started/topicrepresentation/topicrepresentation.html#custom-labels","text":"The topic labels are currently automatically generated by taking the top 3 words and combining them using the _ separator. Although this is an informative label, in practice, this is definitely not the prettiest nor necessarily the most accurate label. For example, although the topic label 1_space_nasa_orbit is informative, we would prefer to have a bit more intuitive label, such as space travel . The difficulty with creating such topic labels is that much of the interpretation is left to the user. Would space travel be more accurate or perhaps space explorations ? To truly understand which labels are most suited, going into some of the documents in topics is especially helpful. Although we can go through every single topic ourselves and try to label them, we can start by creating an overview of labels that have the length and number of words that we are looking for. To do so, we can generate our list of topic labels with .get_topic_labels and define the number of words, the separator, word length, etc: topic_labels = topic_model . generate_topic_labels ( nr_words = 3 , topic_prefix = False , word_length = 10 , separator = \", \" ) In the above example, 1_space_nasa_orbit would turn into space, nasa, orbit since we selected 3 words, no topic prefix, and the , separator. We can then either change our topic_labels to whatever we want or directly pass them to .set_topic_labels so that they can be used across most visualization functions: topic_model . set_topic_labels ( topic_labels ) It is also possible to only change a few topic labels at a time by passing a dictionary where the key represents the topic ID and the value is the topic label : topic_model . set_topic_labels ({ 1 : \"Space Travel\" , 7 : \"Religion\" }) Then, to make use of those custom topic labels across visualizations, such as .visualize_hierarchy() , we can use the custom_labels=True parameter that is found in most visualizations. fig = topic_model . visualize_barchart ( custom_labels = True )","title":"Custom labels"},{"location":"getting_started/topicrepresentation/topicrepresentation.html#optimize-labels","text":"The great advantage of passing custom labels to BERTopic is that when more accurate zero-shot are released, we can simply use those on top of BERTopic to further fine-tune the labeling. For example, let's say you have a set of potential topic labels that you want to use instead of the ones generated by BERTopic. You could use the bart-large-mnli model to find which user-defined labels best represent the BERTopic-generated labels: from transformers import pipeline classifier = pipeline ( \"zero-shot-classification\" , model = \"facebook/bart-large-mnli\" ) # A selected topic representation # 'god jesus atheists atheism belief atheist believe exist beliefs existence' sequence_to_classify = \" \" . join ([ word for word , _ in topic_model . get_topic ( 1 )]) # Our set of potential topic labels candidate_labels = [ 'cooking' , 'dancing' , 'religion' ] classifier ( sequence_to_classify , candidate_labels ) #{'labels': ['cooking', 'dancing', 'religion'], # 'scores': [0.086, 0.063, 0.850], # 'sequence': 'god jesus atheists atheism belief atheist believe exist beliefs existence'}","title":"Optimize labels"},{"location":"getting_started/topicsovertime/topicsovertime.html","text":"Dynamic topic modeling (DTM) is a collection of techniques aimed at analyzing the evolution of topics over time. These methods allow you to understand how a topic is represented across different times. For example, in 1995 people may talk differently about environmental awareness than those in 2015. Although the topic itself remains the same, environmental awareness, the exact representation of that topic might differ. BERTopic allows for DTM by calculating the topic representation at each timestep without the need to run the entire model several times. To do this, we first need to fit BERTopic as if there were no temporal aspect in the data. Thus, a general topic model will be created. We use the global representation as to the main topics that can be found at, most likely, different timesteps. For each topic and timestep, we calculate the c-TF-IDF representation. This will result in a specific topic representation at each timestep without the need to create clusters from embeddings as they were already created. Next, there are two main ways to further fine-tune these specific topic representations, namely globally and evolutionary . A topic representation at timestep t can fine-tuned globally by averaging its c-TF-IDF representation with that of the global representation. This allows each topic representation to move slightly towards the global representation whilst still keeping some its specific words. A topic representation at timestep t can be fine-tuned evolutionary by averaging its c-TF-IDF representation with that of the c-TF-IDF representation at timestep t-1 . This is done for each topic representation allowing for the representations to evolve over time. Both fine-tuning methods are set to True as a default and allow for interesting representations to be created. Example \u00b6 To demonstrate DTM in BERTopic, we first need to prepare our data. A good example of where DTM is useful is topic modeling on Twitter data. We can analyze how certain people have talked about certain topics in the years they have been on Twitter. Due to the controversial nature of his tweets, we are going to be using all tweets by Donald Trump. First, we need to load in the data and do some very basic cleaning. For example, I am not interested in his re-tweets for this use-case: import re import pandas as pd # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () Then, we need to extract the global topic representations by simply creating and training a BERTopic model: from bertopic import BERTopic topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( tweets ) From these topics, we are going to generate the topic representations at each timestamp for each topic. We do this by simply calling topics_over_time and pass in his tweets, the corresponding timestamps, and the related topics: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , nr_bins = 20 ) And that is it! Aside from what you always need for BERTopic, you now only need to add timestamps to quickly calculate the topics over time. Parameters \u00b6 There are a few parameters that are of interest which will be discussed below. Tuning \u00b6 Both global_tuning and evolutionary_tuning are set to True as a default, but can easily be changed. Perhaps you do not want the representations to be influenced by the global representation and merely see how they evolved over time: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , global_tuning = True , evolution_tuning = True , nr_bins = 20 ) Bins \u00b6 If you have more than 100 unique timestamps, then there will be topic representations created for each of those timestamps which can negatively affect the topic representations. It is advised to keep the number of unique timestamps below 50. To do this, you can simply set the number of bins that are created when calculating the topic representations. The timestamps will be taken and put into equal-sized bins: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , nr_bins = 20 ) Datetime format \u00b6 If you are passing strings (dates) instead of integers, then BERTopic will try to automatically detect which datetime format your strings have. Unfortunately, this will not always work if they are in an unexpected format. We can use datetime_format to pass the format the timestamps have: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , datetime_format = \"%b%M\" , nr_bins = 20 ) Visualization \u00b6 To me, DTM becomes truly interesting when you have a good way of visualizing how topics have changed over time. A nice way of doing so is leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: topic_model . visualize_topics_over_time ( topics_over_time , top_n_topics = 20 ) I used top_n_topics to only show the top 20 most frequent topics. If I were to visualize all topics, which is possible by leaving top_n_topics empty, there is a chance that hundreds of lines will fill the plot. You can also use topics to show specific topics: topic_model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Dynamic Topic Modeling"},{"location":"getting_started/topicsovertime/topicsovertime.html#example","text":"To demonstrate DTM in BERTopic, we first need to prepare our data. A good example of where DTM is useful is topic modeling on Twitter data. We can analyze how certain people have talked about certain topics in the years they have been on Twitter. Due to the controversial nature of his tweets, we are going to be using all tweets by Donald Trump. First, we need to load in the data and do some very basic cleaning. For example, I am not interested in his re-tweets for this use-case: import re import pandas as pd # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () Then, we need to extract the global topic representations by simply creating and training a BERTopic model: from bertopic import BERTopic topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( tweets ) From these topics, we are going to generate the topic representations at each timestamp for each topic. We do this by simply calling topics_over_time and pass in his tweets, the corresponding timestamps, and the related topics: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , nr_bins = 20 ) And that is it! Aside from what you always need for BERTopic, you now only need to add timestamps to quickly calculate the topics over time.","title":"Example"},{"location":"getting_started/topicsovertime/topicsovertime.html#parameters","text":"There are a few parameters that are of interest which will be discussed below.","title":"Parameters"},{"location":"getting_started/topicsovertime/topicsovertime.html#tuning","text":"Both global_tuning and evolutionary_tuning are set to True as a default, but can easily be changed. Perhaps you do not want the representations to be influenced by the global representation and merely see how they evolved over time: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , global_tuning = True , evolution_tuning = True , nr_bins = 20 )","title":"Tuning"},{"location":"getting_started/topicsovertime/topicsovertime.html#bins","text":"If you have more than 100 unique timestamps, then there will be topic representations created for each of those timestamps which can negatively affect the topic representations. It is advised to keep the number of unique timestamps below 50. To do this, you can simply set the number of bins that are created when calculating the topic representations. The timestamps will be taken and put into equal-sized bins: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , nr_bins = 20 )","title":"Bins"},{"location":"getting_started/topicsovertime/topicsovertime.html#datetime-format","text":"If you are passing strings (dates) instead of integers, then BERTopic will try to automatically detect which datetime format your strings have. Unfortunately, this will not always work if they are in an unexpected format. We can use datetime_format to pass the format the timestamps have: topics_over_time = topic_model . topics_over_time ( tweets , timestamps , datetime_format = \"%b%M\" , nr_bins = 20 )","title":"Datetime format"},{"location":"getting_started/topicsovertime/topicsovertime.html#visualization","text":"To me, DTM becomes truly interesting when you have a good way of visualizing how topics have changed over time. A nice way of doing so is leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call visualize_topics_over_time with the newly created topics over time: topic_model . visualize_topics_over_time ( topics_over_time , top_n_topics = 20 ) I used top_n_topics to only show the top 20 most frequent topics. If I were to visualize all topics, which is possible by leaving top_n_topics empty, there is a chance that hundreds of lines will fill the plot. You can also use topics to show specific topics: topic_model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Visualization"},{"location":"getting_started/topicsperclass/topicsperclass.html","text":"In some cases, you might be interested in how certain topics are represented over certain categories. Perhaps there are specific groups of users for which you want to see how they talk about certain topics. Instead of running the topic model per class, we can simply create a topic model and then extract, for each topic, its representation per class. This allows you to see how certain topics, calculated over all documents, are represented for certain subgroups. To do so, we use the 20 Newsgroups dataset to see how the topics that we uncover are represented in the 20 categories of documents. First, let's prepare the data: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] targets = data [ \"target\" ] target_names = data [ \"target_names\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] Next, we want to extract the topics across all documents without taking the categories into account: topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) Now that we have created our global topic model, let us calculate the topic representations across each category: topics_per_class = topic_model . topics_per_class ( docs , classes = classes ) The classes variable contains the class for each document. Then, we simply visualize these topics per class: topic_model . visualize_topics_per_class ( topics_per_class , top_n_topics = 10 ) You can hover over the bars to see the topic representation per class. As you can see in the visualization above, the topics 93_homosexual_homosexuality_sex and 58_bike_bikes_motorcycle are somewhat distributed over all classes. You can see that the topic representation between rec.motorcycles and rec.autos in 58_bike_bikes_motorcycle clearly differs from one another. It seems that BERTopic has tried to combine those two categories into a single topic. However, since they do contain two separate topics, the topic representation in those two categories differs. We see something similar for 93_homosexual_homosexuality_sex , where the topic is distributed among several categories and is represented slightly differently. Thus, you can see that although in certain categories the topic is similar, the way the topic is represented can differ.","title":"Topics per Class"},{"location":"getting_started/vectorizers/vectorizers.html","text":"Vectorizers \u00b6 In topic modeling, the quality of the topic representations are key for interpreting the topics, communicating results, and understanding patterns. It is of utmost importance to make sure that the topic representations fits with your use case. In practice, there is not one correct way of creating topic representations. Some use cases might opt more higher n-grams, whereas others might focus more on single words without any stop words. The diversity in use cases also means that we need to have some flexibility in BERTopic to make sure it can be used across most use cases. CountVectorizer \u00b6 One often understimated component of BERTopic is the CountVectorizer and c-TF-IDF calculation. Together, they are responsible for creating the topic representations and luckily can be quite flexible in parameter tuning. Here, we will go through tips and tricks for tuning your CountVectorizer and see how they might affect the topic representations. Before starting, it should be noted that you can pass the CountVectorizer before and after training your topic model. Passing it before training allows you to minimize the size of the resulting c-TF-IDF matrix: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer # Train BERTopic with a custom CountVectorizer vectorizer_model = CountVectorizer ( min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs ) Passing it after training allows you to fine-tune the topic representations by using .update_topics() : from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer # Train a BERTopic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Fine-tune topic representations after training BERTopic vectorizer_model = CountVectorizer ( stop_words = \"english\" , ngram_range = ( 1 , 3 ), min_df = 10 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) The great thing about using .update_topics() is that it allows you to tweak the topic representations without re-training your model! Thus, here we will be focusing on fine-tuning our topic representations after training our model. Note The great thing about processing our topic representations with the CountVectorizer is that it does not influence the quality of clusters as that is being performed before generating the topic representations. Basic Usage \u00b6 First, let's start with defining our documents and train our topic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare documents docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] # Train a BERTopic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Now, let's see the top 10 most frequent topics that have been generated: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _ites_hello_cheek_hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _amp_audio_condition_asking The topic representations generated already seem quite interpretable! However, I am quite sure we do much better without having to re-train our model. Next, we will go through common parameters in CountVectorizer and focus on the effects that they might have. As a baseline, we will be comparing them to the topic representation above. Parameters \u00b6 There are a number of basic parameters in the CountVectorizer that we can use to improve upon the quality of the resulting topic representations. ngram_range \u00b6 The ngram_range parameter allows us to decide how many tokens each entity is in a topic representation. For example, we have words like game and team with a length of 1 in a topic but it would also make sense to have words like hockey league with a length of 2. To allow for these words to be generated, we can set the ngram_range parameter: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 3 ), stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) As you might have noticed, I also added stop_words=\"english\" . This is necessary as longer words tend to have many stop words and removing them allows for nicer topic representations: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_players 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello ites_forget hello_ites 15 _huh hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _post_jim_context_forged 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_gas_compound 10 9 174 9 _amp_audio_condition_asking Although they look very similar, if we zoom in on topic 8, we can see longer words in our representation: >>> topic_model . get_topic ( 8 ) [( 'fbi' , 0.019637149205975653 ), ( 'koresh' , 0.019054514637064403 ), ( 'gas' , 0.014156057632897179 ), ( 'compound' , 0.012381224868591681 ), ( 'batf' , 0.010349992314076047 ), ( 'children' , 0.009336408916322387 ), ( 'tear gas' , 0.008941747802855279 ), ( 'tear' , 0.008446786597564537 ), ( 'davidians' , 0.007911119583253022 ), ( 'started' , 0.007398687505638955 )] tear and gas have now been combined into a single representation. This helps us understand what those individual words might have been representing. stop_words \u00b6 In some of the topics, we can see stop words appearing like he or the . Stop words is something we typically want to prevent in our topic representations as they do not give additional information to the topic. To prevent those stop words, we can use the stop_words parameter in the CountVectorizer to remove them from the representations: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) After running the above, we get the following output: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_players 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _ites_cheek_hello_hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _monitor_card_video_vga 6 5 438 5 _post_jim_context_forged 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_tobacco 9 8 218 8 _fbi_koresh_gas_compound 10 9 174 9 _amp_audio_condition_stereo As you can see, the topic representations already look much better! Stop words are removed and the representations are more interpretable. We can also pass in a list of stop words if you have multiple languages to take into account. min_df \u00b6 One important parameter to keep in mind is the min_df . This is typically an integer representing how frequent a word must be before being added to our representation. You can imagine that if we have a million documents and a certain word only appears a single time across all of them, then it would be highly unlikely to be representive of a topic. Typically, the c-TF-IDF calculation removes that word from the topic representation but when you have millions of documents, that will also lead to very large topic-term matrix. To prevent a huge vocabulary, we can set the min_df to only accept words that have a minimum frequency. When you have millions of documents, or error issues, I would advise increasing the value of min_df as long as the topic representations might sense: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( min_df = 10 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) With the following topic representation: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello_hi_yep_huh 4 3 493 3 _israel_jews_jewish_peace 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _audio_condition_stereo_asking As you can see, the output is nearly the same which is actually what we would like to achieve. All words that appear less than 10 times are now removed from our topic-term matrix (i.e., c-TF-IDF matrix) which drastically lowers the matrix in size. max_features \u00b6 A parameter similar to min_df is max_features which allows you to select the top n most frequent words to be used in the topic representation. Setting this to, for example, 10_000 creates a topic-term matrix with 10_000 terms. This helps you control the size of the topic-term matrix directly without having to fiddle around with the min_df parameter: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( max_features = 10_000 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) With the following representation: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello_hi_yep_huh 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _amp_audio_condition_asking As with min_df , we would like the topic representations to be very similar. tokenizer \u00b6 The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. Fortunately, we can use the tokenizer variable in the CountVectorizer to use jieba , which is a package for Chinese text segmentation. Using it is straightforward: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, we can simply pass the vectorizer update our topic representations: topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) OnlineCountVectorizer \u00b6 When using the online/incremental variant of BERTopic, we need a CountVectorizer than can incrementally update its representation. For that purpose, OnlineCountVectorizer was created that not only updates out-of-vocabulary words but also implements decay and cleaning functions to prevent the sparse bag-of-words matrix to become too large in size. It is a class that can be found in bertopic.vectorizers which extends sklearn.feature_extraction.text.CountVectorizer . In other words, you can use the exact same parameter in OnlineCountVectorizer as found in Scikit-Learn's CountVectorizer . We can use it as follows: from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer # Train BERTopic with a custom OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer () topic_model = BERTopic ( vectorizer_model = vectorizer_model ) Parameters \u00b6 Other than parameters found in CountVectorizer , such as stop_words and ngram_range , we can two parameters in OnlineCountVectorizer to adjust the way old data is processed and kept. decay \u00b6 At each iteration, we sum the bag-of-words representation of the new documents with the bag-of-words representation of all documents processed thus far. In other words, the bag-of-words matrix keeps increasing with each iteration. However, especially in a streaming setting, older documents might become less and less relevant as time goes on. Therefore, a decay parameter was implemented that decays the bag-of-words's frequencies at each iteration before adding the document frequencies of new documents. The decay parameter is a value between 0 and 1 and indicates the percentage of frequencies the previous bag-of-words matrix should be reduced to. For example, a value of .1 will decrease the frequencies in the bag-of-words matrix with 10% at each iteration before adding the new bag-of-words matrix. This will make sure that recent data has more weight than previously iterations. delete_min_df \u00b6 In BERTopic, we might want to remove words from the topic representation that ppear infrequently. The min_df in the CountVectorizer works quite well for that. However, when have a streaming setting, the min_df does not work as well since a word's frequency might start below min_df but will end up higher than that over time. Setting that value high might not always be advised. As a result, the vocabulary of the resulting bag-of-words matrix can become quite large. Similarly, if we implement the decay parameter, then some values will actually decrease over time until they are below min_df . For these reasons, the delete_min_df parameter was implemented. The parameter takes positive integers and indicates, at each iteration, which words will be removed. If the value is set to 5, it will check after each iteration if the total frequency of a word is exceed by that value. If so, the word will be removed in its entirety from the bag-of-words matrix. This helps to keep the bag-of-words matrix of a manageble size. Note Although the delete_min_df parameter removes words from the bag-of-words matrix, it is not permament. If new documents come in where those previously deleted words are used frequently, they get added back to the matrix.","title":"Vectorizers"},{"location":"getting_started/vectorizers/vectorizers.html#vectorizers","text":"In topic modeling, the quality of the topic representations are key for interpreting the topics, communicating results, and understanding patterns. It is of utmost importance to make sure that the topic representations fits with your use case. In practice, there is not one correct way of creating topic representations. Some use cases might opt more higher n-grams, whereas others might focus more on single words without any stop words. The diversity in use cases also means that we need to have some flexibility in BERTopic to make sure it can be used across most use cases.","title":"Vectorizers"},{"location":"getting_started/vectorizers/vectorizers.html#countvectorizer","text":"One often understimated component of BERTopic is the CountVectorizer and c-TF-IDF calculation. Together, they are responsible for creating the topic representations and luckily can be quite flexible in parameter tuning. Here, we will go through tips and tricks for tuning your CountVectorizer and see how they might affect the topic representations. Before starting, it should be noted that you can pass the CountVectorizer before and after training your topic model. Passing it before training allows you to minimize the size of the resulting c-TF-IDF matrix: from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer # Train BERTopic with a custom CountVectorizer vectorizer_model = CountVectorizer ( min_df = 10 ) topic_model = BERTopic ( vectorizer_model = vectorizer_model ) topics , probs = topic_model . fit_transform ( docs ) Passing it after training allows you to fine-tune the topic representations by using .update_topics() : from bertopic import BERTopic from sklearn.feature_extraction.text import CountVectorizer # Train a BERTopic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) # Fine-tune topic representations after training BERTopic vectorizer_model = CountVectorizer ( stop_words = \"english\" , ngram_range = ( 1 , 3 ), min_df = 10 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) The great thing about using .update_topics() is that it allows you to tweak the topic representations without re-training your model! Thus, here we will be focusing on fine-tuning our topic representations after training our model. Note The great thing about processing our topic representations with the CountVectorizer is that it does not influence the quality of clusters as that is being performed before generating the topic representations.","title":"CountVectorizer"},{"location":"getting_started/vectorizers/vectorizers.html#basic-usage","text":"First, let's start with defining our documents and train our topic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare documents docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] # Train a BERTopic model topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Now, let's see the top 10 most frequent topics that have been generated: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _ites_hello_cheek_hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _amp_audio_condition_asking The topic representations generated already seem quite interpretable! However, I am quite sure we do much better without having to re-train our model. Next, we will go through common parameters in CountVectorizer and focus on the effects that they might have. As a baseline, we will be comparing them to the topic representation above.","title":"Basic Usage"},{"location":"getting_started/vectorizers/vectorizers.html#parameters","text":"There are a number of basic parameters in the CountVectorizer that we can use to improve upon the quality of the resulting topic representations.","title":"Parameters"},{"location":"getting_started/vectorizers/vectorizers.html#ngram_range","text":"The ngram_range parameter allows us to decide how many tokens each entity is in a topic representation. For example, we have words like game and team with a length of 1 in a topic but it would also make sense to have words like hockey league with a length of 2. To allow for these words to be generated, we can set the ngram_range parameter: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( ngram_range = ( 1 , 3 ), stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) As you might have noticed, I also added stop_words=\"english\" . This is necessary as longer words tend to have many stop words and removing them allows for nicer topic representations: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_players 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello ites_forget hello_ites 15 _huh hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _post_jim_context_forged 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_gas_compound 10 9 174 9 _amp_audio_condition_asking Although they look very similar, if we zoom in on topic 8, we can see longer words in our representation: >>> topic_model . get_topic ( 8 ) [( 'fbi' , 0.019637149205975653 ), ( 'koresh' , 0.019054514637064403 ), ( 'gas' , 0.014156057632897179 ), ( 'compound' , 0.012381224868591681 ), ( 'batf' , 0.010349992314076047 ), ( 'children' , 0.009336408916322387 ), ( 'tear gas' , 0.008941747802855279 ), ( 'tear' , 0.008446786597564537 ), ( 'davidians' , 0.007911119583253022 ), ( 'started' , 0.007398687505638955 )] tear and gas have now been combined into a single representation. This helps us understand what those individual words might have been representing.","title":"ngram_range"},{"location":"getting_started/vectorizers/vectorizers.html#stop_words","text":"In some of the topics, we can see stop words appearing like he or the . Stop words is something we typically want to prevent in our topic representations as they do not give additional information to the topic. To prevent those stop words, we can use the stop_words parameter in the CountVectorizer to remove them from the representations: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( stop_words = \"english\" ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) After running the above, we get the following output: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_players 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _ites_cheek_hello_hi 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _monitor_card_video_vga 6 5 438 5 _post_jim_context_forged 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_tobacco 9 8 218 8 _fbi_koresh_gas_compound 10 9 174 9 _amp_audio_condition_stereo As you can see, the topic representations already look much better! Stop words are removed and the representations are more interpretable. We can also pass in a list of stop words if you have multiple languages to take into account.","title":"stop_words"},{"location":"getting_started/vectorizers/vectorizers.html#min_df","text":"One important parameter to keep in mind is the min_df . This is typically an integer representing how frequent a word must be before being added to our representation. You can imagine that if we have a million documents and a certain word only appears a single time across all of them, then it would be highly unlikely to be representive of a topic. Typically, the c-TF-IDF calculation removes that word from the topic representation but when you have millions of documents, that will also lead to very large topic-term matrix. To prevent a huge vocabulary, we can set the min_df to only accept words that have a minimum frequency. When you have millions of documents, or error issues, I would advise increasing the value of min_df as long as the topic representations might sense: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( min_df = 10 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) With the following topic representation: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello_hi_yep_huh 4 3 493 3 _israel_jews_jewish_peace 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _audio_condition_stereo_asking As you can see, the output is nearly the same which is actually what we would like to achieve. All words that appear less than 10 times are now removed from our topic-term matrix (i.e., c-TF-IDF matrix) which drastically lowers the matrix in size.","title":"min_df"},{"location":"getting_started/vectorizers/vectorizers.html#max_features","text":"A parameter similar to min_df is max_features which allows you to select the top n most frequent words to be used in the topic representation. Setting this to, for example, 10_000 creates a topic-term matrix with 10_000 terms. This helps you control the size of the topic-term matrix directly without having to fiddle around with the min_df parameter: from sklearn.feature_extraction.text import CountVectorizer vectorizer_model = CountVectorizer ( max_features = 10_000 ) topic_model . update_topics ( docs , vectorizer_model = vectorizer_model ) With the following representation: >>> topic_model . get_topic_info ()[ 1 : 11 ] Topic Count Name 1 0 1822 0 _game_team_games_he 2 1 580 1 _key_clipper_chip_encryption 3 2 532 2 _hello_hi_yep_huh 4 3 493 3 _israel_israeli_jews_arab 5 4 453 4 _card_monitor_video_drivers 6 5 438 5 _you_your_post_jim 7 6 314 6 _car_cars_engine_ford 8 7 279 7 _health_newsgroup_cancer_1993 9 8 218 8 _fbi_koresh_fire_gas 10 9 174 9 _amp_audio_condition_asking As with min_df , we would like the topic representations to be very similar.","title":"max_features"},{"location":"getting_started/vectorizers/vectorizers.html#tokenizer","text":"The default tokenizer in the CountVectorizer works well for western languages but fails to tokenize some non-western languages, like Chinese. Fortunately, we can use the tokenizer variable in the CountVectorizer to use jieba , which is a package for Chinese text segmentation. Using it is straightforward: from sklearn.feature_extraction.text import CountVectorizer import jieba def tokenize_zh ( text ): words = jieba . lcut ( text ) return words vectorizer = CountVectorizer ( tokenizer = tokenize_zh ) Then, we can simply pass the vectorizer update our topic representations: topic_model . update_topics ( docs , vectorizer_model = vectorizer_model )","title":"tokenizer"},{"location":"getting_started/vectorizers/vectorizers.html#onlinecountvectorizer","text":"When using the online/incremental variant of BERTopic, we need a CountVectorizer than can incrementally update its representation. For that purpose, OnlineCountVectorizer was created that not only updates out-of-vocabulary words but also implements decay and cleaning functions to prevent the sparse bag-of-words matrix to become too large in size. It is a class that can be found in bertopic.vectorizers which extends sklearn.feature_extraction.text.CountVectorizer . In other words, you can use the exact same parameter in OnlineCountVectorizer as found in Scikit-Learn's CountVectorizer . We can use it as follows: from bertopic import BERTopic from bertopic.vectorizers import OnlineCountVectorizer # Train BERTopic with a custom OnlineCountVectorizer vectorizer_model = OnlineCountVectorizer () topic_model = BERTopic ( vectorizer_model = vectorizer_model )","title":"OnlineCountVectorizer"},{"location":"getting_started/vectorizers/vectorizers.html#parameters_1","text":"Other than parameters found in CountVectorizer , such as stop_words and ngram_range , we can two parameters in OnlineCountVectorizer to adjust the way old data is processed and kept.","title":"Parameters"},{"location":"getting_started/vectorizers/vectorizers.html#decay","text":"At each iteration, we sum the bag-of-words representation of the new documents with the bag-of-words representation of all documents processed thus far. In other words, the bag-of-words matrix keeps increasing with each iteration. However, especially in a streaming setting, older documents might become less and less relevant as time goes on. Therefore, a decay parameter was implemented that decays the bag-of-words's frequencies at each iteration before adding the document frequencies of new documents. The decay parameter is a value between 0 and 1 and indicates the percentage of frequencies the previous bag-of-words matrix should be reduced to. For example, a value of .1 will decrease the frequencies in the bag-of-words matrix with 10% at each iteration before adding the new bag-of-words matrix. This will make sure that recent data has more weight than previously iterations.","title":"decay"},{"location":"getting_started/vectorizers/vectorizers.html#delete_min_df","text":"In BERTopic, we might want to remove words from the topic representation that ppear infrequently. The min_df in the CountVectorizer works quite well for that. However, when have a streaming setting, the min_df does not work as well since a word's frequency might start below min_df but will end up higher than that over time. Setting that value high might not always be advised. As a result, the vocabulary of the resulting bag-of-words matrix can become quite large. Similarly, if we implement the decay parameter, then some values will actually decrease over time until they are below min_df . For these reasons, the delete_min_df parameter was implemented. The parameter takes positive integers and indicates, at each iteration, which words will be removed. If the value is set to 5, it will check after each iteration if the total frequency of a word is exceed by that value. If so, the word will be removed in its entirety from the bag-of-words matrix. This helps to keep the bag-of-words matrix of a manageble size. Note Although the delete_min_df parameter removes words from the bag-of-words matrix, it is not permament. If new documents come in where those previously deleted words are used frequently, they get added back to the matrix.","title":"delete_min_df"},{"location":"getting_started/visualization/visualization.html","text":"Visualizing BERTopic and its derivatives is important in understanding the model, how it works, but more importantly, where it works. Since topic modeling can be quite a subjective field it is difficult for users to validate their models. Looking at the topics and seeing if they make sense is an important factor in alleviating this issue. Visualize Topics \u00b6 After having trained our BERTopic model, we can iteratively go through hundreds of topics to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Then, we can use call .visualize_topics to create a 2D representation of your topics. The resulting graph is a plotly interactive graph which can be converted to HTML: topic_model . visualize_topics () You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including the size of the topic and its corresponding words. Visualize Documents \u00b6 Using the previous method, we can visualize the topics and get insight into their relationships. However, you might want a more fine-grained approach where we can visualize the documents inside the topics to see if they were assigned correctly or whether they make sense. To do so, we can use the topic_model.visualize_documents() function. This function recalculates the document embeddings and reduces them to 2-dimensional space for easier visualization purposes. This process can be quite expensive, so it is advised to adhere to the following pipeline: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Note The visualization above was generated with the additional parameter hide_document_hover=True which disables the option to hover over the individual points and see the content of the documents. This was done for demonstration purposes as saving all those documents in the visualization can be quite expensive and result in large files. However, it might be interesting to set hide_document_hover=False in order to hover over the points and see the content of the documents. Visualize Topic Hierarchy \u00b6 The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created. To visualize this hierarchy, run the following: topic_model . visualize_hierarchy () Note Do note that this is not the actual procedure of .reduce_topics() when nr_topics is set to auto since HDBSCAN is used to automatically extract topics. The visualization above closely resembles the actual procedure of .reduce_topics() when any number of nr_topics is selected. Hierarchical labels \u00b6 Although visualizing this hierarchy gives us information about the structure, it would be helpful to see what happens to the topic representations when merging topics. To do so, we first need to calculate the representations of the hierarchical topics: First, we train a basic BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) To visualize these results, we simply need to pass the resulting hierarchical_topics to our .visualize_hierarchy function: topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics together. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes. Text-based topic tree \u00b6 Although this gives a nice overview of the potential hierarchy, hovering over all black circles can be tiresome. Instead, we can use topic_model.get_topic_tree to create a text-based representation of this hierarchy. Although the general structure is more difficult to view, we can see better which topics could be logically merged: >>> tree = topic_model . get_topic_tree ( hierarchical_topics ) >>> print ( tree ) . \u2514\u2500 atheists_atheism_god_moral_atheist \u251c\u2500 atheists_atheism_god_atheist_argument \u2502 \u251c\u2500\u25a0\u2500\u2500 atheists_atheism_god_atheist_argument \u2500\u2500 Topic : 21 \u2502 \u2514\u2500\u25a0\u2500\u2500 br_god_exist_genetic_existence \u2500\u2500 Topic : 124 \u2514\u2500\u25a0\u2500\u2500 moral_morality_objective_immoral_morals \u2500\u2500 Topic : 29 Click here to view the full tree. . \u251c\u2500people_armenian_said_god_armenians \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u2502 \u251c\u2500jehovah_lord_mormon_mcconkie_god \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ra_satan_thou_god_lucifer \u2500\u2500 Topic: 94 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500jehovah_lord_mormon_mcconkie_unto \u2500\u2500 Topic: 78 \u2502 \u2502 \u2502 \u2514\u2500jesus_mary_god_hell_sin \u2502 \u2502 \u2502 \u251c\u2500jesus_hell_god_eternal_heaven \u2502 \u2502 \u2502 \u2502 \u251c\u2500hell_jesus_eternal_god_heaven \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jesus_tomb_disciples_resurrection_john \u2500\u2500 Topic: 69 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500hell_eternal_god_jesus_heaven \u2500\u2500 Topic: 53 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500aaron_baptism_sin_law_god \u2500\u2500 Topic: 89 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mary_sin_maria_priest_conception \u2500\u2500 Topic: 56 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500marriage_married_marry_ceremony_marriages \u2500\u2500 Topic: 110 \u2502 \u2514\u2500people_armenian_armenians_said_mr \u2502 \u251c\u2500people_armenian_armenians_said_israel \u2502 \u2502 \u251c\u2500god_homosexual_homosexuality_atheists_sex \u2502 \u2502 \u2502 \u251c\u2500homosexual_homosexuality_sex_gay_homosexuals \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500kinsey_sex_gay_men_sexual \u2500\u2500 Topic: 44 \u2502 \u2502 \u2502 \u2502 \u2514\u2500homosexuality_homosexual_sin_homosexuals_gay \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gay_homosexual_homosexuals_sexual_cramer \u2500\u2500 Topic: 50 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500homosexuality_homosexual_sin_paul_sex \u2500\u2500 Topic: 27 \u2502 \u2502 \u2502 \u2514\u2500god_atheists_atheism_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jim_context_challenges_articles_quote \u2500\u2500 Topic: 36 \u2502 \u2502 \u2502 \u2502 \u2514\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500islam_quran_islamic_rushdie_muslims \u2500\u2500 Topic: 31 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500judas_scripture_bible_books_greek \u2500\u2500 Topic: 33 \u2502 \u2502 \u2502 \u2514\u2500atheists_atheism_god_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500atheists_atheism_god_atheist_argument \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500atheists_atheism_god_atheist_argument \u2500\u2500 Topic: 21 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500br_god_exist_genetic_existence \u2500\u2500 Topic: 124 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500moral_morality_objective_immoral_morals \u2500\u2500 Topic: 29 \u2502 \u2502 \u2514\u2500armenian_armenians_people_israel_said \u2502 \u2502 \u251c\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500tax_rights_government_income_taxes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rights_right_slavery_slaves_residence \u2500\u2500 Topic: 106 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tax_government_taxes_income_libertarians \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500government_libertarians_libertarian_regulation_party \u2500\u2500 Topic: 58 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tax_taxes_income_billion_deficit \u2500\u2500 Topic: 41 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500gun_guns_militia_firearms_amendment \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500blacks_penalty_death_cruel_punishment \u2500\u2500 Topic: 55 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500gun_guns_militia_firearms_amendment \u2500\u2500 Topic: 7 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_jews_turkish \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500israel_israeli_jews_arab_jewish \u2500\u2500 Topic: 4 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500armenian_armenians_turkish_armenia_azerbaijan \u2500\u2500 Topic: 15 \u2502 \u2502 \u2514\u2500stephanopoulos_president_mr_myers_ms \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500serbs_muslims_stephanopoulos_mr_bosnia \u2500\u2500 Topic: 35 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500myers_stephanopoulos_president_ms_mr \u2500\u2500 Topic: 87 \u2502 \u2514\u2500batf_fbi_koresh_compound_gas \u2502 \u251c\u2500\u25a0\u2500\u2500reno_workers_janet_clinton_waco \u2500\u2500 Topic: 77 \u2502 \u2514\u2500batf_fbi_koresh_gas_compound \u2502 \u251c\u2500batf_koresh_fbi_warrant_compound \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500batf_warrant_raid_compound_fbi \u2500\u2500 Topic: 42 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500koresh_batf_fbi_children_compound \u2500\u2500 Topic: 61 \u2502 \u2514\u2500\u25a0\u2500\u2500fbi_gas_tear_bds_building \u2500\u2500 Topic: 23 \u2514\u2500use_like_just_dont_new \u251c\u2500game_team_year_games_like \u2502 \u251c\u2500game_team_games_25_year \u2502 \u2502 \u251c\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u2500window_printer_use_problem_mhz \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_wire_simms_wiring_battery \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_mhz_battery_cpu_heat \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_pds_simm_vram_lc \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500pds_nubus_lc_slot_card \u2500\u2500 Topic: 119 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500simms_simm_vram_meg_dram \u2500\u2500 Topic: 32 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mhz_battery_cpu_heat_speed \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500fan_cpu_heat_sink_fans \u2500\u2500 Topic: 92 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mhz_speed_cpu_fpu_clock \u2500\u2500 Topic: 22 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500monitor_turn_power_computer_electricity \u2500\u2500 Topic: 91 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500battery_batteries_concrete_duo_discharge \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500duo_battery_apple_230_problem \u2500\u2500 Topic: 121 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500battery_batteries_concrete_discharge_temperature \u2500\u2500 Topic: 75 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500leds_uv_blue_light_boards \u2500\u2500 Topic: 66 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500wire_wiring_ground_neutral_outlets \u2500\u2500 Topic: 120 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500scope_scopes_phone_dial_number \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500dial_number_phone_line_output \u2500\u2500 Topic: 93 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500scope_scopes_motorola_generator_oscilloscope \u2500\u2500 Topic: 113 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500celp_dsp_sampling_antenna_digital \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500antenna_antennas_receiver_cable_transmitter \u2500\u2500 Topic: 70 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500celp_dsp_sampling_speech_voice \u2500\u2500 Topic: 52 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_printer_xv_mouse_windows \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_xv_error_widget_problem \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500error_symbol_undefined_xterm_rx \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500symbol_error_undefined_doug_parse \u2500\u2500 Topic: 63 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500rx_remote_server_xdm_xterm \u2500\u2500 Topic: 45 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_xv_widget_application_expose \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_widget_expose_application_event \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gc_mydisplay_draw_gxxor_drawing \u2500\u2500 Topic: 103 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500window_widget_application_expose_event \u2500\u2500 Topic: 25 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500xv_den_polygon_points_algorithm \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500den_polygon_points_algorithm_polygons \u2500\u2500 Topic: 28 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500xv_24bit_image_bit_images \u2500\u2500 Topic: 57 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_mouse_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500scanner_logitech_grayscale_ocr_scanman \u2500\u2500 Topic: 108 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500printer_print_deskjet_hp_ink \u2500\u2500 Topic: 18 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500fonts_font_truetype_tt_atm \u2500\u2500 Topic: 49 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mouse_ghostscript_midi_driver_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500ghostscript_midi_postscript_files_file \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ghostscript_postscript_pageview_ghostview_dsc \u2500\u2500 Topic: 104 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500midi_sound_file_windows_driver \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500location_mar_file_host_rwrr \u2500\u2500 Topic: 83 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500midi_sound_driver_blaster_soundblaster \u2500\u2500 Topic: 98 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mouse_driver_mice_ball_problem \u2500\u2500 Topic: 68 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u25001st_sale_condition_comics_hulk \u2502 \u2502 \u2502 \u2502 \u251c\u2500sale_condition_offer_asking_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500condition_stereo_amp_speakers_asking \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500miles_car_amfm_toyota_cassette \u2500\u2500 Topic: 62 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500amp_speakers_condition_stereo_audio \u2500\u2500 Topic: 24 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500games_sale_pom_cds_shipping \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500pom_cds_sale_shipping_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500size_shipping_sale_condition_mattress \u2500\u2500 Topic: 100 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pom_cds_cd_sale_picture \u2500\u2500 Topic: 37 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500games_game_snes_sega_genesis \u2500\u2500 Topic: 40 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u251c\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500lens_tape_camera_backup_lenses \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500tape_backup_tapes_drive_4mm \u2500\u2500 Topic: 107 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500lens_camera_lenses_zoom_pouch \u2500\u2500 Topic: 114 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u25001st_hulk_comics_art_appears \u2500\u2500 Topic: 105 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500books_book_cover_trek_chemistry \u2500\u2500 Topic: 125 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tickets_hotel_ticket_voucher_package \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hotel_voucher_package_vacation_room \u2500\u2500 Topic: 74 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tickets_ticket_june_airlines_july \u2500\u2500 Topic: 84 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_season_hockey \u2502 \u2502 \u2502 \u251c\u2500game_hockey_team_25_550 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500espn_pt_pts_game_la \u2500\u2500 Topic: 17 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500team_25_game_hockey_550 \u2500\u2500 Topic: 2 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500year_game_hit_baseball_players \u2500\u2500 Topic: 0 \u2502 \u2502 \u2514\u2500bike_car_greek_insurance_msg \u2502 \u2502 \u251c\u2500car_bike_insurance_cars_engine \u2502 \u2502 \u2502 \u251c\u2500car_insurance_cars_radar_engine \u2502 \u2502 \u2502 \u2502 \u251c\u2500insurance_health_private_care_canada \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500insurance_health_private_care_canada \u2500\u2500 Topic: 99 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500insurance_car_accident_rates_sue \u2500\u2500 Topic: 82 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_radar_engine_detector \u2502 \u2502 \u2502 \u2502 \u251c\u2500car_radar_cars_detector_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500radar_detector_detectors_ka_alarm \u2500\u2500 Topic: 39 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_mustang_ford_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clutch_shift_shifting_transmission_gear \u2500\u2500 Topic: 88 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500car_cars_mustang_ford_v8 \u2500\u2500 Topic: 14 \u2502 \u2502 \u2502 \u2502 \u2514\u2500oil_diesel_odometer_diesels_car \u2502 \u2502 \u2502 \u2502 \u251c\u2500odometer_oil_sensor_car_drain \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500odometer_sensor_speedo_gauge_mileage \u2500\u2500 Topic: 96 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oil_drain_car_leaks_taillights \u2500\u2500 Topic: 102 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500diesel_diesels_emissions_fuel_oil \u2500\u2500 Topic: 79 \u2502 \u2502 \u2502 \u2514\u2500bike_riding_ride_bikes_motorcycle \u2502 \u2502 \u2502 \u251c\u2500bike_ride_riding_bikes_lane \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500bike_ride_riding_lane_car \u2500\u2500 Topic: 11 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500bike_bikes_miles_honda_motorcycle \u2500\u2500 Topic: 19 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500countersteering_bike_motorcycle_rear_shaft \u2500\u2500 Topic: 46 \u2502 \u2502 \u2514\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_kuwait_greece_turkish_greeks \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500greek_greece_turkish_greeks_cyprus \u2500\u2500 Topic: 71 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kuwait_iraq_iran_gulf_arabia \u2500\u2500 Topic: 76 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_dog_drugs_drug_food \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clinton_bush_quayle_reagan_panicking \u2500\u2500 Topic: 101 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cooper_trial_weaver_spence_witnesses \u2500\u2500 Topic: 90 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500dog_dogs_bike_trained_springer \u2500\u2500 Topic: 67 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_drugs_drug_food_chinese \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500msg_food_chinese_foods_taste \u2500\u2500 Topic: 30 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500drugs_drug_marijuana_cocaine_alcohol \u2500\u2500 Topic: 72 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_theory_universe_science_larsons \u2502 \u2502 \u2502 \u2502 \u251c\u2500water_nuclear_cooling_steam_dept \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rocketry_rockets_engines_nuclear_plutonium \u2500\u2500 Topic: 115 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_cooling_steam_dept_plants \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500water_dept_phd_environmental_atmospheric \u2500\u2500 Topic: 97 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500cooling_water_steam_towers_plants \u2500\u2500 Topic: 109 \u2502 \u2502 \u2502 \u2502 \u2514\u2500theory_universe_larsons_larson_science \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500theory_universe_larsons_larson_science \u2500\u2500 Topic: 54 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oort_cloud_grbs_gamma_burst \u2500\u2500 Topic: 80 \u2502 \u2502 \u2502 \u2514\u2500helmet_kirlian_photography_lock_wax \u2502 \u2502 \u2502 \u251c\u2500helmet_kirlian_photography_leaf_mask \u2502 \u2502 \u2502 \u2502 \u251c\u2500kirlian_photography_leaf_pictures_deleted \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500deleted_joke_stuff_maddi_nickname \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500joke_maddi_nickname_nicknames_frank \u2500\u2500 Topic: 43 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500deleted_stuff_bookstore_joke_motto \u2500\u2500 Topic: 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kirlian_photography_leaf_pictures_aura \u2500\u2500 Topic: 85 \u2502 \u2502 \u2502 \u2502 \u2514\u2500helmet_mask_liner_foam_cb \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500helmet_liner_foam_cb_helmets \u2500\u2500 Topic: 112 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mask_goalies_77_santore_tl \u2500\u2500 Topic: 123 \u2502 \u2502 \u2502 \u2514\u2500lock_wax_paint_plastic_ear \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500lock_cable_locks_bike_600 \u2500\u2500 Topic: 117 \u2502 \u2502 \u2502 \u2514\u2500wax_paint_ear_plastic_skin \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500wax_paint_plastic_scratches_solvent \u2500\u2500 Topic: 65 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ear_wax_skin_greasy_acne \u2500\u2500 Topic: 116 \u2502 \u2502 \u2514\u2500m4_mp_14_mw_mo \u2502 \u2502 \u251c\u2500m4_mp_14_mw_mo \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500m4_mp_14_mw_mo \u2500\u2500 Topic: 111 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500test_ensign_nameless_deane_deanebinahccbrandeisedu \u2500\u2500 Topic: 118 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ites_cheek_hello_hi_ken \u2500\u2500 Topic: 3 \u2502 \u2514\u2500space_medical_health_disease_cancer \u2502 \u251c\u2500medical_health_disease_cancer_patients \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cancer_centers_center_medical_research \u2500\u2500 Topic: 122 \u2502 \u2502 \u2514\u2500health_medical_disease_patients_hiv \u2502 \u2502 \u251c\u2500patients_medical_disease_candida_health \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2502 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 \u2502 \u2514\u2500space_launch_nasa_shuttle_orbit \u2502 \u251c\u2500space_moon_station_nasa_launch \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500sky_advertising_billboard_billboards_space \u2500\u2500 Topic: 59 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500space_station_moon_redesign_nasa \u2500\u2500 Topic: 16 \u2502 \u2514\u2500space_mission_hst_launch_orbit \u2502 \u251c\u2500space_launch_nasa_orbit_propulsion \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500space_launch_nasa_propulsion_astronaut \u2500\u2500 Topic: 47 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500orbit_km_jupiter_probe_earth \u2500\u2500 Topic: 86 \u2502 \u2514\u2500\u25a0\u2500\u2500hst_mission_shuttle_orbit_arrays \u2500\u2500 Topic: 60 \u2514\u2500drive_file_key_windows_use \u251c\u2500key_file_jpeg_encryption_image \u2502 \u251c\u2500key_encryption_clipper_chip_keys \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500key_clipper_encryption_chip_keys \u2500\u2500 Topic: 1 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500entry_file_ripem_entries_key \u2500\u2500 Topic: 73 \u2502 \u2514\u2500jpeg_image_file_gif_images \u2502 \u251c\u2500motif_graphics_ftp_available_3d \u2502 \u2502 \u251c\u2500motif_graphics_openwindows_ftp_available \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500openwindows_motif_xview_windows_mouse \u2500\u2500 Topic: 20 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500graphics_widget_ray_3d_available \u2500\u2500 Topic: 95 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u25003d_machines_version_comments_contact \u2500\u2500 Topic: 38 \u2502 \u2514\u2500jpeg_image_gif_images_format \u2502 \u251c\u2500\u25a0\u2500\u2500gopher_ftp_files_stuffit_images \u2500\u2500 Topic: 51 \u2502 \u2514\u2500\u25a0\u2500\u2500jpeg_image_gif_format_images \u2500\u2500 Topic: 13 \u2514\u2500drive_db_card_scsi_windows \u251c\u2500db_windows_dos_mov_os2 \u2502 \u251c\u2500\u25a0\u2500\u2500copy_protection_program_software_disk \u2500\u2500 Topic: 64 \u2502 \u2514\u2500\u25a0\u2500\u2500db_windows_dos_mov_os2 \u2500\u2500 Topic: 8 \u2514\u2500drive_card_scsi_drives_ide \u251c\u2500drive_scsi_drives_ide_disk \u2502 \u251c\u2500\u25a0\u2500\u2500drive_scsi_drives_ide_disk \u2500\u2500 Topic: 6 \u2502 \u2514\u2500\u25a0\u2500\u2500meg_sale_ram_drive_shipping \u2500\u2500 Topic: 12 \u2514\u2500card_modem_monitor_video_drivers \u251c\u2500\u25a0\u2500\u2500card_monitor_video_drivers_vga \u2500\u2500 Topic: 5 \u2514\u2500\u25a0\u2500\u2500modem_port_serial_irq_com \u2500\u2500 Topic: 10 Visualize Hierarchical Documents \u00b6 We can extend the previous method by calculating the topic representation at different levels of the hierarchy and plotting them on a 2D-plane. To do so, we first need to calculate the hierarchical topics: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) Then, we can visualize the hierarchical documents by either supplying it with our embeddings or by reducing their dimensionality ourselves: # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Note The visualization above was generated with the additional parameter hide_document_hover=True which disables the option to hover over the individual points and see the content of the documents. This makes the resulting visualization smaller and fit into your RAM. However, it might be interesting to set hide_document_hover=False in order to hover over the points and see the content of the documents. Visualize Terms \u00b6 We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other. To visualize this hierarchy, run the following: topic_model . visualize_barchart () Visualize Topic Similarity \u00b6 Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other. To visualize the heatmap, run the following: topic_model . visualize_heatmap () Note You can set n_clusters in visualize_heatmap to order the topics by their similarity. This will result in blocks being formed in the heatmap indicating which clusters of topics are similar to each other. This step is very much recommended as it will make reading the heatmap easier. Visualize Term Score Decline \u00b6 Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation. To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic. To visualize the c-TF-IDF score decline, run the following: topic_model . visualize_term_rank () To enable the log scale on the y-axis for a better view of individual topics, run the following: topic_model . visualize_term_rank ( log_scale = True ) This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here . Visualize Topics over Time \u00b6 After creating topics over time with Dynamic Topic Modeling, we can visualize these topics by leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call .visualize_topics_over_time with the newly created topics over time: import re import pandas as pd from bertopic import BERTopic # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () # Create topics over time model = BERTopic ( verbose = True ) topics , probs = model . fit_transform ( tweets ) topics_over_time = model . topics_over_time ( tweets , timestamps ) Then, we visualize some interesting topics: model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ]) Visualize Topics per Class \u00b6 You might want to extract and visualize the topic representation per class. For example, if you have specific groups of users that might approach topics differently, then extracting them would help understanding how these users talk about certain topics. In other words, this is simply creating a topic representation for certain classes that you might have in your data. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare data and classes data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] # Create topic model and calculate topics per class topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , classes = classes ) Then, we visualize the topic representation of major topics per class: topic_model . visualize_topics_per_class ( topics_per_class ) Visualize Probablities \u00b6 We can also calculate the probabilities of topics found in a document. In order to do so, we have to set calculate_probabilities to True as calculating them can be quite computationally expensive. Then, we use the variable probabilities that is returned from transform() or fit_transform() to understand how confident BERTopic is that certain topics can be found in a document: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probabilities = topic_model . fit_transform ( docs ) To visualize the distributions, run the following: topic_model . visualize_distribution ( probabilities [ 0 ]) Note The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Topic Visualization"},{"location":"getting_started/visualization/visualization.html#visualize-topics","text":"After having trained our BERTopic model, we can iteratively go through hundreds of topics to get a good understanding of the topics that were extract. However, that takes quite some time and lacks a global representation. Instead, we can visualize the topics that were generated in a way very similar to LDAvis . We embed our c-TF-IDF representation of the topics in 2D using Umap and then visualize the two dimensions using plotly such that we can create an interactive view. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) Then, we can use call .visualize_topics to create a 2D representation of your topics. The resulting graph is a plotly interactive graph which can be converted to HTML: topic_model . visualize_topics () You can use the slider to select the topic which then lights up red. If you hover over a topic, then general information is given about the topic, including the size of the topic and its corresponding words.","title":"Visualize Topics"},{"location":"getting_started/visualization/visualization.html#visualize-documents","text":"Using the previous method, we can visualize the topics and get insight into their relationships. However, you might want a more fine-grained approach where we can visualize the documents inside the topics to see if they were assigned correctly or whether they make sense. To do so, we can use the topic_model.visualize_documents() function. This function recalculates the document embeddings and reduces them to 2-dimensional space for easier visualization purposes. This process can be quite expensive, so it is advised to adhere to the following pipeline: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic topic_model = BERTopic () . fit ( docs , embeddings ) # Run the visualization with the original embeddings topic_model . visualize_documents ( docs , embeddings = embeddings ) # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_documents ( docs , reduced_embeddings = reduced_embeddings ) Note The visualization above was generated with the additional parameter hide_document_hover=True which disables the option to hover over the individual points and see the content of the documents. This was done for demonstration purposes as saving all those documents in the visualization can be quite expensive and result in large files. However, it might be interesting to set hide_document_hover=False in order to hover over the points and see the content of the documents.","title":"Visualize Documents"},{"location":"getting_started/visualization/visualization.html#visualize-topic-hierarchy","text":"The topics that were created can be hierarchically reduced. In order to understand the potential hierarchical structure of the topics, we can use scipy.cluster.hierarchy to create clusters and visualize how they relate to one another. This might help selecting an appropriate nr_topics when reducing the number of topics that you have created. To visualize this hierarchy, run the following: topic_model . visualize_hierarchy () Note Do note that this is not the actual procedure of .reduce_topics() when nr_topics is set to auto since HDBSCAN is used to automatically extract topics. The visualization above closely resembles the actual procedure of .reduce_topics() when any number of nr_topics is selected.","title":"Visualize Topic Hierarchy"},{"location":"getting_started/visualization/visualization.html#hierarchical-labels","text":"Although visualizing this hierarchy gives us information about the structure, it would be helpful to see what happens to the topic representations when merging topics. To do so, we first need to calculate the representations of the hierarchical topics: First, we train a basic BERTopic model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ \"data\" ] topic_model = BERTopic ( verbose = True ) topics , probs = topic_model . fit_transform ( docs ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) To visualize these results, we simply need to pass the resulting hierarchical_topics to our .visualize_hierarchy function: topic_model . visualize_hierarchy ( hierarchical_topics = hierarchical_topics ) If you hover over the black circles, you will see the topic representation at that level of the hierarchy. These representations help you understand the effect of merging certain topics together. Some might be logical to merge whilst others might not. Moreover, we can now see which sub-topics can be found within certain larger themes.","title":"Hierarchical labels"},{"location":"getting_started/visualization/visualization.html#text-based-topic-tree","text":"Although this gives a nice overview of the potential hierarchy, hovering over all black circles can be tiresome. Instead, we can use topic_model.get_topic_tree to create a text-based representation of this hierarchy. Although the general structure is more difficult to view, we can see better which topics could be logically merged: >>> tree = topic_model . get_topic_tree ( hierarchical_topics ) >>> print ( tree ) . \u2514\u2500 atheists_atheism_god_moral_atheist \u251c\u2500 atheists_atheism_god_atheist_argument \u2502 \u251c\u2500\u25a0\u2500\u2500 atheists_atheism_god_atheist_argument \u2500\u2500 Topic : 21 \u2502 \u2514\u2500\u25a0\u2500\u2500 br_god_exist_genetic_existence \u2500\u2500 Topic : 124 \u2514\u2500\u25a0\u2500\u2500 moral_morality_objective_immoral_morals \u2500\u2500 Topic : 29 Click here to view the full tree. . \u251c\u2500people_armenian_said_god_armenians \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u251c\u2500god_jesus_jehovah_lord_christ \u2502 \u2502 \u2502 \u251c\u2500jehovah_lord_mormon_mcconkie_god \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ra_satan_thou_god_lucifer \u2500\u2500 Topic: 94 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500jehovah_lord_mormon_mcconkie_unto \u2500\u2500 Topic: 78 \u2502 \u2502 \u2502 \u2514\u2500jesus_mary_god_hell_sin \u2502 \u2502 \u2502 \u251c\u2500jesus_hell_god_eternal_heaven \u2502 \u2502 \u2502 \u2502 \u251c\u2500hell_jesus_eternal_god_heaven \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jesus_tomb_disciples_resurrection_john \u2500\u2500 Topic: 69 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500hell_eternal_god_jesus_heaven \u2500\u2500 Topic: 53 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500aaron_baptism_sin_law_god \u2500\u2500 Topic: 89 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mary_sin_maria_priest_conception \u2500\u2500 Topic: 56 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500marriage_married_marry_ceremony_marriages \u2500\u2500 Topic: 110 \u2502 \u2514\u2500people_armenian_armenians_said_mr \u2502 \u251c\u2500people_armenian_armenians_said_israel \u2502 \u2502 \u251c\u2500god_homosexual_homosexuality_atheists_sex \u2502 \u2502 \u2502 \u251c\u2500homosexual_homosexuality_sex_gay_homosexuals \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500kinsey_sex_gay_men_sexual \u2500\u2500 Topic: 44 \u2502 \u2502 \u2502 \u2502 \u2514\u2500homosexuality_homosexual_sin_homosexuals_gay \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gay_homosexual_homosexuals_sexual_cramer \u2500\u2500 Topic: 50 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500homosexuality_homosexual_sin_paul_sex \u2500\u2500 Topic: 27 \u2502 \u2502 \u2502 \u2514\u2500god_atheists_atheism_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500jim_context_challenges_articles_quote \u2500\u2500 Topic: 36 \u2502 \u2502 \u2502 \u2502 \u2514\u2500islam_quran_judas_islamic_book \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500islam_quran_islamic_rushdie_muslims \u2500\u2500 Topic: 31 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500judas_scripture_bible_books_greek \u2500\u2500 Topic: 33 \u2502 \u2502 \u2502 \u2514\u2500atheists_atheism_god_moral_atheist \u2502 \u2502 \u2502 \u251c\u2500atheists_atheism_god_atheist_argument \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500atheists_atheism_god_atheist_argument \u2500\u2500 Topic: 21 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500br_god_exist_genetic_existence \u2500\u2500 Topic: 124 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500moral_morality_objective_immoral_morals \u2500\u2500 Topic: 29 \u2502 \u2502 \u2514\u2500armenian_armenians_people_israel_said \u2502 \u2502 \u251c\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500tax_rights_government_income_taxes \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rights_right_slavery_slaves_residence \u2500\u2500 Topic: 106 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tax_government_taxes_income_libertarians \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500government_libertarians_libertarian_regulation_party \u2500\u2500 Topic: 58 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tax_taxes_income_billion_deficit \u2500\u2500 Topic: 41 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_people_jews \u2502 \u2502 \u2502 \u251c\u2500gun_guns_militia_firearms_amendment \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500blacks_penalty_death_cruel_punishment \u2500\u2500 Topic: 55 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500gun_guns_militia_firearms_amendment \u2500\u2500 Topic: 7 \u2502 \u2502 \u2502 \u2514\u2500armenian_armenians_israel_jews_turkish \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500israel_israeli_jews_arab_jewish \u2500\u2500 Topic: 4 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500armenian_armenians_turkish_armenia_azerbaijan \u2500\u2500 Topic: 15 \u2502 \u2502 \u2514\u2500stephanopoulos_president_mr_myers_ms \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500serbs_muslims_stephanopoulos_mr_bosnia \u2500\u2500 Topic: 35 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500myers_stephanopoulos_president_ms_mr \u2500\u2500 Topic: 87 \u2502 \u2514\u2500batf_fbi_koresh_compound_gas \u2502 \u251c\u2500\u25a0\u2500\u2500reno_workers_janet_clinton_waco \u2500\u2500 Topic: 77 \u2502 \u2514\u2500batf_fbi_koresh_gas_compound \u2502 \u251c\u2500batf_koresh_fbi_warrant_compound \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500batf_warrant_raid_compound_fbi \u2500\u2500 Topic: 42 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500koresh_batf_fbi_children_compound \u2500\u2500 Topic: 61 \u2502 \u2514\u2500\u25a0\u2500\u2500fbi_gas_tear_bds_building \u2500\u2500 Topic: 23 \u2514\u2500use_like_just_dont_new \u251c\u2500game_team_year_games_like \u2502 \u251c\u2500game_team_games_25_year \u2502 \u2502 \u251c\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u2500window_printer_use_problem_mhz \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_wire_simms_wiring_battery \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_mhz_battery_cpu_heat \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500simms_pds_simm_vram_lc \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500pds_nubus_lc_slot_card \u2500\u2500 Topic: 119 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500simms_simm_vram_meg_dram \u2500\u2500 Topic: 32 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mhz_battery_cpu_heat_speed \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500mhz_cpu_speed_heat_fan \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500fan_cpu_heat_sink_fans \u2500\u2500 Topic: 92 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mhz_speed_cpu_fpu_clock \u2500\u2500 Topic: 22 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500monitor_turn_power_computer_electricity \u2500\u2500 Topic: 91 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500battery_batteries_concrete_duo_discharge \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500duo_battery_apple_230_problem \u2500\u2500 Topic: 121 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500battery_batteries_concrete_discharge_temperature \u2500\u2500 Topic: 75 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500wire_wiring_ground_neutral_outlets \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500leds_uv_blue_light_boards \u2500\u2500 Topic: 66 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500wire_wiring_ground_neutral_outlets \u2500\u2500 Topic: 120 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500scope_scopes_phone_dial_number \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500dial_number_phone_line_output \u2500\u2500 Topic: 93 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500scope_scopes_motorola_generator_oscilloscope \u2500\u2500 Topic: 113 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500celp_dsp_sampling_antenna_digital \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500antenna_antennas_receiver_cable_transmitter \u2500\u2500 Topic: 70 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500celp_dsp_sampling_speech_voice \u2500\u2500 Topic: 52 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_printer_xv_mouse_windows \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_xv_error_widget_problem \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500error_symbol_undefined_xterm_rx \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500symbol_error_undefined_doug_parse \u2500\u2500 Topic: 63 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500rx_remote_server_xdm_xterm \u2500\u2500 Topic: 45 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500window_xv_widget_application_expose \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500window_widget_expose_application_event \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500gc_mydisplay_draw_gxxor_drawing \u2500\u2500 Topic: 103 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500window_widget_application_expose_event \u2500\u2500 Topic: 25 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500xv_den_polygon_points_algorithm \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500den_polygon_points_algorithm_polygons \u2500\u2500 Topic: 28 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500xv_24bit_image_bit_images \u2500\u2500 Topic: 57 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_mouse_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500scanner_logitech_grayscale_ocr_scanman \u2500\u2500 Topic: 108 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500printer_fonts_print_font_deskjet \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500printer_print_deskjet_hp_ink \u2500\u2500 Topic: 18 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500fonts_font_truetype_tt_atm \u2500\u2500 Topic: 49 \u2502 \u2502 \u2502 \u2502 \u2514\u2500mouse_ghostscript_midi_driver_postscript \u2502 \u2502 \u2502 \u2502 \u251c\u2500ghostscript_midi_postscript_files_file \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500ghostscript_postscript_pageview_ghostview_dsc \u2500\u2500 Topic: 104 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500midi_sound_file_windows_driver \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500location_mar_file_host_rwrr \u2500\u2500 Topic: 83 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500midi_sound_driver_blaster_soundblaster \u2500\u2500 Topic: 98 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mouse_driver_mice_ball_problem \u2500\u2500 Topic: 68 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_25_season \u2502 \u2502 \u2502 \u251c\u25001st_sale_condition_comics_hulk \u2502 \u2502 \u2502 \u2502 \u251c\u2500sale_condition_offer_asking_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500condition_stereo_amp_speakers_asking \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500miles_car_amfm_toyota_cassette \u2500\u2500 Topic: 62 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500amp_speakers_condition_stereo_audio \u2500\u2500 Topic: 24 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500games_sale_pom_cds_shipping \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500pom_cds_sale_shipping_cd \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500size_shipping_sale_condition_mattress \u2500\u2500 Topic: 100 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pom_cds_cd_sale_picture \u2500\u2500 Topic: 37 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500games_game_snes_sega_genesis \u2500\u2500 Topic: 40 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u251c\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500lens_tape_camera_backup_lenses \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500tape_backup_tapes_drive_4mm \u2500\u2500 Topic: 107 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500lens_camera_lenses_zoom_pouch \u2500\u2500 Topic: 114 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u25001st_hulk_comics_art_appears \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u25001st_hulk_comics_art_appears \u2500\u2500 Topic: 105 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500books_book_cover_trek_chemistry \u2500\u2500 Topic: 125 \u2502 \u2502 \u2502 \u2502 \u2514\u2500tickets_hotel_ticket_voucher_package \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hotel_voucher_package_vacation_room \u2500\u2500 Topic: 74 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500tickets_ticket_june_airlines_july \u2500\u2500 Topic: 84 \u2502 \u2502 \u2502 \u2514\u2500game_team_games_season_hockey \u2502 \u2502 \u2502 \u251c\u2500game_hockey_team_25_550 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500espn_pt_pts_game_la \u2500\u2500 Topic: 17 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500team_25_game_hockey_550 \u2500\u2500 Topic: 2 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500year_game_hit_baseball_players \u2500\u2500 Topic: 0 \u2502 \u2502 \u2514\u2500bike_car_greek_insurance_msg \u2502 \u2502 \u251c\u2500car_bike_insurance_cars_engine \u2502 \u2502 \u2502 \u251c\u2500car_insurance_cars_radar_engine \u2502 \u2502 \u2502 \u2502 \u251c\u2500insurance_health_private_care_canada \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500insurance_health_private_care_canada \u2500\u2500 Topic: 99 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500insurance_car_accident_rates_sue \u2500\u2500 Topic: 82 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_radar_engine_detector \u2502 \u2502 \u2502 \u2502 \u251c\u2500car_radar_cars_detector_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500radar_detector_detectors_ka_alarm \u2500\u2500 Topic: 39 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500car_cars_mustang_ford_engine \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clutch_shift_shifting_transmission_gear \u2500\u2500 Topic: 88 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500car_cars_mustang_ford_v8 \u2500\u2500 Topic: 14 \u2502 \u2502 \u2502 \u2502 \u2514\u2500oil_diesel_odometer_diesels_car \u2502 \u2502 \u2502 \u2502 \u251c\u2500odometer_oil_sensor_car_drain \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500odometer_sensor_speedo_gauge_mileage \u2500\u2500 Topic: 96 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oil_drain_car_leaks_taillights \u2500\u2500 Topic: 102 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500diesel_diesels_emissions_fuel_oil \u2500\u2500 Topic: 79 \u2502 \u2502 \u2502 \u2514\u2500bike_riding_ride_bikes_motorcycle \u2502 \u2502 \u2502 \u251c\u2500bike_ride_riding_bikes_lane \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500bike_ride_riding_lane_car \u2500\u2500 Topic: 11 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500bike_bikes_miles_honda_motorcycle \u2500\u2500 Topic: 19 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500countersteering_bike_motorcycle_rear_shaft \u2500\u2500 Topic: 46 \u2502 \u2502 \u2514\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_water \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_msg_kuwait_greece_dog \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500greek_kuwait_greece_turkish_greeks \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500greek_greece_turkish_greeks_cyprus \u2500\u2500 Topic: 71 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kuwait_iraq_iran_gulf_arabia \u2500\u2500 Topic: 76 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_dog_drugs_drug_food \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500clinton_bush_quayle_reagan_panicking \u2500\u2500 Topic: 101 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500dog_dogs_cooper_trial_weaver \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cooper_trial_weaver_spence_witnesses \u2500\u2500 Topic: 90 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500dog_dogs_bike_trained_springer \u2500\u2500 Topic: 67 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500msg_drugs_drug_food_chinese \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500msg_food_chinese_foods_taste \u2500\u2500 Topic: 30 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500drugs_drug_marijuana_cocaine_alcohol \u2500\u2500 Topic: 72 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_theory_universe_science_larsons \u2502 \u2502 \u2502 \u2502 \u251c\u2500water_nuclear_cooling_steam_dept \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500rocketry_rockets_engines_nuclear_plutonium \u2500\u2500 Topic: 115 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500water_cooling_steam_dept_plants \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500water_dept_phd_environmental_atmospheric \u2500\u2500 Topic: 97 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500cooling_water_steam_towers_plants \u2500\u2500 Topic: 109 \u2502 \u2502 \u2502 \u2502 \u2514\u2500theory_universe_larsons_larson_science \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500theory_universe_larsons_larson_science \u2500\u2500 Topic: 54 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500oort_cloud_grbs_gamma_burst \u2500\u2500 Topic: 80 \u2502 \u2502 \u2502 \u2514\u2500helmet_kirlian_photography_lock_wax \u2502 \u2502 \u2502 \u251c\u2500helmet_kirlian_photography_leaf_mask \u2502 \u2502 \u2502 \u2502 \u251c\u2500kirlian_photography_leaf_pictures_deleted \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500deleted_joke_stuff_maddi_nickname \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500joke_maddi_nickname_nicknames_frank \u2500\u2500 Topic: 43 \u2502 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500deleted_stuff_bookstore_joke_motto \u2500\u2500 Topic: 81 \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500kirlian_photography_leaf_pictures_aura \u2500\u2500 Topic: 85 \u2502 \u2502 \u2502 \u2502 \u2514\u2500helmet_mask_liner_foam_cb \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500helmet_liner_foam_cb_helmets \u2500\u2500 Topic: 112 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500mask_goalies_77_santore_tl \u2500\u2500 Topic: 123 \u2502 \u2502 \u2502 \u2514\u2500lock_wax_paint_plastic_ear \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500lock_cable_locks_bike_600 \u2500\u2500 Topic: 117 \u2502 \u2502 \u2502 \u2514\u2500wax_paint_ear_plastic_skin \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500wax_paint_plastic_scratches_solvent \u2500\u2500 Topic: 65 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ear_wax_skin_greasy_acne \u2500\u2500 Topic: 116 \u2502 \u2502 \u2514\u2500m4_mp_14_mw_mo \u2502 \u2502 \u251c\u2500m4_mp_14_mw_mo \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500m4_mp_14_mw_mo \u2500\u2500 Topic: 111 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500test_ensign_nameless_deane_deanebinahccbrandeisedu \u2500\u2500 Topic: 118 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500ites_cheek_hello_hi_ken \u2500\u2500 Topic: 3 \u2502 \u2514\u2500space_medical_health_disease_cancer \u2502 \u251c\u2500medical_health_disease_cancer_patients \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500cancer_centers_center_medical_research \u2500\u2500 Topic: 122 \u2502 \u2502 \u2514\u2500health_medical_disease_patients_hiv \u2502 \u2502 \u251c\u2500patients_medical_disease_candida_health \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500candida_yeast_infection_gonorrhea_infections \u2500\u2500 Topic: 48 \u2502 \u2502 \u2502 \u2514\u2500patients_disease_cancer_medical_doctor \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500hiv_medical_cancer_patients_doctor \u2500\u2500 Topic: 34 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500pain_drug_patients_disease_diet \u2500\u2500 Topic: 26 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500health_newsgroup_tobacco_vote_votes \u2500\u2500 Topic: 9 \u2502 \u2514\u2500space_launch_nasa_shuttle_orbit \u2502 \u251c\u2500space_moon_station_nasa_launch \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500sky_advertising_billboard_billboards_space \u2500\u2500 Topic: 59 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500space_station_moon_redesign_nasa \u2500\u2500 Topic: 16 \u2502 \u2514\u2500space_mission_hst_launch_orbit \u2502 \u251c\u2500space_launch_nasa_orbit_propulsion \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500space_launch_nasa_propulsion_astronaut \u2500\u2500 Topic: 47 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500orbit_km_jupiter_probe_earth \u2500\u2500 Topic: 86 \u2502 \u2514\u2500\u25a0\u2500\u2500hst_mission_shuttle_orbit_arrays \u2500\u2500 Topic: 60 \u2514\u2500drive_file_key_windows_use \u251c\u2500key_file_jpeg_encryption_image \u2502 \u251c\u2500key_encryption_clipper_chip_keys \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500key_clipper_encryption_chip_keys \u2500\u2500 Topic: 1 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500entry_file_ripem_entries_key \u2500\u2500 Topic: 73 \u2502 \u2514\u2500jpeg_image_file_gif_images \u2502 \u251c\u2500motif_graphics_ftp_available_3d \u2502 \u2502 \u251c\u2500motif_graphics_openwindows_ftp_available \u2502 \u2502 \u2502 \u251c\u2500\u25a0\u2500\u2500openwindows_motif_xview_windows_mouse \u2500\u2500 Topic: 20 \u2502 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u2500graphics_widget_ray_3d_available \u2500\u2500 Topic: 95 \u2502 \u2502 \u2514\u2500\u25a0\u2500\u25003d_machines_version_comments_contact \u2500\u2500 Topic: 38 \u2502 \u2514\u2500jpeg_image_gif_images_format \u2502 \u251c\u2500\u25a0\u2500\u2500gopher_ftp_files_stuffit_images \u2500\u2500 Topic: 51 \u2502 \u2514\u2500\u25a0\u2500\u2500jpeg_image_gif_format_images \u2500\u2500 Topic: 13 \u2514\u2500drive_db_card_scsi_windows \u251c\u2500db_windows_dos_mov_os2 \u2502 \u251c\u2500\u25a0\u2500\u2500copy_protection_program_software_disk \u2500\u2500 Topic: 64 \u2502 \u2514\u2500\u25a0\u2500\u2500db_windows_dos_mov_os2 \u2500\u2500 Topic: 8 \u2514\u2500drive_card_scsi_drives_ide \u251c\u2500drive_scsi_drives_ide_disk \u2502 \u251c\u2500\u25a0\u2500\u2500drive_scsi_drives_ide_disk \u2500\u2500 Topic: 6 \u2502 \u2514\u2500\u25a0\u2500\u2500meg_sale_ram_drive_shipping \u2500\u2500 Topic: 12 \u2514\u2500card_modem_monitor_video_drivers \u251c\u2500\u25a0\u2500\u2500card_monitor_video_drivers_vga \u2500\u2500 Topic: 5 \u2514\u2500\u25a0\u2500\u2500modem_port_serial_irq_com \u2500\u2500 Topic: 10","title":"Text-based topic tree"},{"location":"getting_started/visualization/visualization.html#visualize-hierarchical-documents","text":"We can extend the previous method by calculating the topic representation at different levels of the hierarchy and plotting them on a 2D-plane. To do so, we first need to calculate the hierarchical topics: from sklearn.datasets import fetch_20newsgroups from sentence_transformers import SentenceTransformer from bertopic import BERTopic from umap import UMAP # Prepare embeddings docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] sentence_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) embeddings = sentence_model . encode ( docs , show_progress_bar = False ) # Train BERTopic and extract hierarchical topics topic_model = BERTopic () . fit ( docs , embeddings ) hierarchical_topics = topic_model . hierarchical_topics ( docs ) Then, we can visualize the hierarchical documents by either supplying it with our embeddings or by reducing their dimensionality ourselves: # Run the visualization with the original embeddings topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , embeddings = embeddings ) # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively: reduced_embeddings = UMAP ( n_neighbors = 10 , n_components = 2 , min_dist = 0.0 , metric = 'cosine' ) . fit_transform ( embeddings ) topic_model . visualize_hierarchical_documents ( docs , hierarchical_topics , reduced_embeddings = reduced_embeddings ) Note The visualization above was generated with the additional parameter hide_document_hover=True which disables the option to hover over the individual points and see the content of the documents. This makes the resulting visualization smaller and fit into your RAM. However, it might be interesting to set hide_document_hover=False in order to hover over the points and see the content of the documents.","title":"Visualize Hierarchical Documents"},{"location":"getting_started/visualization/visualization.html#visualize-terms","text":"We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other. To visualize this hierarchy, run the following: topic_model . visualize_barchart ()","title":"Visualize Terms"},{"location":"getting_started/visualization/visualization.html#visualize-topic-similarity","text":"Having generated topic embeddings, through both c-TF-IDF and embeddings, we can create a similarity matrix by simply applying cosine similarities through those topic embeddings. The result will be a matrix indicating how similar certain topics are to each other. To visualize the heatmap, run the following: topic_model . visualize_heatmap () Note You can set n_clusters in visualize_heatmap to order the topics by their similarity. This will result in blocks being formed in the heatmap indicating which clusters of topics are similar to each other. This step is very much recommended as it will make reading the heatmap easier.","title":"Visualize Topic Similarity"},{"location":"getting_started/visualization/visualization.html#visualize-term-score-decline","text":"Topics are represented by a number of words starting with the best representative word. Each word is represented by a c-TF-IDF score. The higher the score, the more representative a word to the topic is. Since the topic words are sorted by their c-TF-IDF score, the scores slowly decline with each word that is added. At some point adding words to the topic representation only marginally increases the total c-TF-IDF score and would not be beneficial for its representation. To visualize this effect, we can plot the c-TF-IDF scores for each topic by the term rank of each word. In other words, the position of the words (term rank), where the words with the highest c-TF-IDF score will have a rank of 1, will be put on the x-axis. Whereas the y-axis will be populated by the c-TF-IDF scores. The result is a visualization that shows you the decline of c-TF-IDF score when adding words to the topic representation. It allows you, using the elbow method, the select the best number of words in a topic. To visualize the c-TF-IDF score decline, run the following: topic_model . visualize_term_rank () To enable the log scale on the y-axis for a better view of individual topics, run the following: topic_model . visualize_term_rank ( log_scale = True ) This visualization was heavily inspired by the \"Term Probability Decline\" visualization found in an analysis by the amazing tmtoolkit . Reference to that specific analysis can be found here .","title":"Visualize Term Score Decline"},{"location":"getting_started/visualization/visualization.html#visualize-topics-over-time","text":"After creating topics over time with Dynamic Topic Modeling, we can visualize these topics by leveraging the interactive abilities of Plotly. Plotly allows us to show the frequency of topics over time whilst giving the option of hovering over the points to show the time-specific topic representations. Simply call .visualize_topics_over_time with the newly created topics over time: import re import pandas as pd from bertopic import BERTopic # Prepare data trump = pd . read_csv ( 'https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6' ) trump . text = trump . apply ( lambda row : re . sub ( r \"http\\S+\" , \"\" , row . text ) . lower (), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( filter ( lambda x : x [ 0 ] != \"@\" , row . text . split ())), 1 ) trump . text = trump . apply ( lambda row : \" \" . join ( re . sub ( \"[^a-zA-Z]+\" , \" \" , row . text ) . split ()), 1 ) trump = trump . loc [( trump . isRetweet == \"f\" ) & ( trump . text != \"\" ), :] timestamps = trump . date . to_list () tweets = trump . text . to_list () # Create topics over time model = BERTopic ( verbose = True ) topics , probs = model . fit_transform ( tweets ) topics_over_time = model . topics_over_time ( tweets , timestamps ) Then, we visualize some interesting topics: model . visualize_topics_over_time ( topics_over_time , topics = [ 9 , 10 , 72 , 83 , 87 , 91 ])","title":"Visualize Topics over Time"},{"location":"getting_started/visualization/visualization.html#visualize-topics-per-class","text":"You might want to extract and visualize the topic representation per class. For example, if you have specific groups of users that might approach topics differently, then extracting them would help understanding how these users talk about certain topics. In other words, this is simply creating a topic representation for certain classes that you might have in your data. First, we need to train our model: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups # Prepare data and classes data = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' )) docs = data [ \"data\" ] classes = [ data [ \"target_names\" ][ i ] for i in data [ \"target\" ]] # Create topic model and calculate topics per class topic_model = BERTopic () topics , probs = topic_model . fit_transform ( docs ) topics_per_class = topic_model . topics_per_class ( docs , classes = classes ) Then, we visualize the topic representation of major topics per class: topic_model . visualize_topics_per_class ( topics_per_class )","title":"Visualize Topics per Class"},{"location":"getting_started/visualization/visualization.html#visualize-probablities","text":"We can also calculate the probabilities of topics found in a document. In order to do so, we have to set calculate_probabilities to True as calculating them can be quite computationally expensive. Then, we use the variable probabilities that is returned from transform() or fit_transform() to understand how confident BERTopic is that certain topics can be found in a document: from bertopic import BERTopic from sklearn.datasets import fetch_20newsgroups docs = fetch_20newsgroups ( subset = 'all' , remove = ( 'headers' , 'footers' , 'quotes' ))[ 'data' ] topic_model = BERTopic ( calculate_probabilities = True ) topics , probabilities = topic_model . fit_transform ( docs ) To visualize the distributions, run the following: topic_model . visualize_distribution ( probabilities [ 0 ]) Note The distribution of the probabilities does not give an indication to the distribution of the frequencies of topics across a document. It merely shows how confident BERTopic is that certain topics can be found in a document.","title":"Visualize Probablities"}]}